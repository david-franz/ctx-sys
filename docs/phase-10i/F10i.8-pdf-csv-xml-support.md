# F10i.8 PDF, CSV & XML Document Support

**Phase**: 10i - Code Quality & Infrastructure
**Priority**: High
**Dependencies**: None

## Goal

Add first-class indexing support for PDF, CSV, and XML file types in the document indexing pipeline, extending the existing 6 supported formats (markdown, YAML, JSON, TOML, HTML, plain text) to 9.

## Current State

The document indexer at `src/documents/document-indexer.ts` dispatches by file extension via `EXTENSION_MAP` (line 57-68):

```typescript
const EXTENSION_MAP: Record<string, DocumentType> = {
  '.md': 'markdown', '.mdx': 'markdown',
  '.yaml': 'yaml', '.yml': 'yaml',
  '.json': 'json',
  '.toml': 'toml',
  '.html': 'html', '.htm': 'html',
  '.txt': 'text', '.log': 'text',
};
```

Each type has a dedicated `index<Type>()` method that:
1. Reads the file content
2. Parses it into structured sections/entities
3. Creates entities (type `document` for the file, type `section` for chunks)
4. Establishes `CONTAINS` relationships
5. Runs `DocumentLinker` to find code references and create `DOCUMENTS` relationships

The pipeline supports:
- Hash-based change detection (skip unchanged files)
- Size-aware chunking (split large sections, merge small ones, add overlap)
- Requirement extraction from document content
- Code reference resolution against the entity store

## Implementation Plan

### PDF Support

#### Approach

Use `pdf-parse` (or `pdf2json`) to extract text content from PDFs, then process through the existing chunking and entity creation pipeline. PDFs are treated as flat text with optional page-level sectioning.

**Dependency**: `pdf-parse` — lightweight, no native dependencies, ~200KB

#### Implementation

**File**: `src/documents/pdf-parser.ts`

```typescript
import pdf from 'pdf-parse';
import { readFileSync } from 'fs';

export interface PdfPage {
  pageNumber: number;
  text: string;
}

export interface PdfDocument {
  title: string;
  author?: string;
  pages: PdfPage[];
  totalPages: number;
  fullText: string;
}

export async function parsePdf(filePath: string): Promise<PdfDocument> {
  const buffer = readFileSync(filePath);
  const data = await pdf(buffer);

  // pdf-parse returns full text; split by form feeds for page boundaries
  const pageTexts = data.text.split('\f').filter(t => t.trim().length > 0);

  return {
    title: data.info?.Title ?? path.basename(filePath, '.pdf'),
    author: data.info?.Author,
    pages: pageTexts.map((text, i) => ({ pageNumber: i + 1, text: text.trim() })),
    totalPages: data.numpages,
    fullText: data.text,
  };
}
```

**File**: `src/documents/document-indexer.ts` — add `indexPdf()`:

```typescript
private async indexPdf(filePath: string, content: Buffer, hash: string): Promise<Entity[]> {
  const { parsePdf } = await import('./pdf-parser.js');
  const pdf = await parsePdf(filePath);
  const entities: Entity[] = [];

  // Create top-level document entity
  const docEntity = await this.entityStore.upsert({
    type: 'document',
    name: pdf.title,
    qualifiedName: filePath,
    content: pdf.fullText,
    summary: `PDF document: ${pdf.title} (${pdf.totalPages} pages)`,
    metadata: { hash, docType: 'pdf', author: pdf.author, pages: pdf.totalPages },
    filePath,
  });
  entities.push(docEntity);

  // Create section entities per page (or per chunk if pages are large)
  for (const page of pdf.pages) {
    if (page.text.length < 50) continue;  // Skip near-empty pages

    const chunks = this.chunkText(page.text, {
      maxSize: 3000,
      overlap: 200,
      prefix: `Page ${page.pageNumber}`,
    });

    for (const chunk of chunks) {
      const sectionEntity = await this.entityStore.upsert({
        type: 'section',
        name: `${pdf.title} - Page ${page.pageNumber}`,
        qualifiedName: `${filePath}::page-${page.pageNumber}${chunks.length > 1 ? `-chunk-${chunk.index}` : ''}`,
        content: chunk.text,
        summary: chunk.text.slice(0, 200),
        metadata: {
          hash,
          docType: 'pdf',
          pageNumber: page.pageNumber,
          level: 1,
        },
        filePath,
        startLine: page.pageNumber,  // Use page number as line proxy
      });
      entities.push(sectionEntity);

      // CONTAINS relationship: document -> section
      await this.addRelationship(docEntity.id, sectionEntity.id, 'CONTAINS');
    }
  }

  // Run document linker to find code references
  await this.linkCodeReferences(docEntity, pdf.fullText);

  return entities;
}
```

#### PDF-specific considerations

- **Binary file handling**: The indexer currently reads files as UTF-8 text. For PDF, read as `Buffer` and pass to `pdf-parse`. Update the file reading logic to detect binary extensions and read accordingly.
- **Large PDFs**: Apply the existing page-size chunking. Cap at a configurable max page count (default: 200) to prevent memory issues on massive PDFs.
- **Scanned PDFs**: `pdf-parse` extracts embedded text only. Scanned PDFs without text layers will produce empty content — detect this and log a warning. OCR support is out of scope.

---

### CSV Support

#### Approach

Parse CSV files into structured data, creating entities for the dataset (document), columns (metadata), and optionally rows/groups. CSVs are useful for configuration data, data dictionaries, and lookup tables that AI agents reference.

**No external dependency needed** — use a lightweight built-in parser since CSV parsing is straightforward for well-formed files. For robustness with quoted fields and edge cases, use `src/documents/csv-parser.ts` with a simple state-machine parser.

#### Implementation

**File**: `src/documents/csv-parser.ts`

```typescript
export interface CsvDocument {
  headers: string[];
  rows: string[][];
  rowCount: number;
}

export function parseCsv(content: string, options?: {
  delimiter?: string;
  maxRows?: number;
}): CsvDocument {
  const delimiter = options?.delimiter ?? ',';
  const lines = content.split('\n');
  const headers = parseRow(lines[0], delimiter);
  const maxRows = options?.maxRows ?? 10000;

  const rows: string[][] = [];
  for (let i = 1; i < lines.length && rows.length < maxRows; i++) {
    const line = lines[i].trim();
    if (line.length === 0) continue;
    rows.push(parseRow(line, delimiter));
  }

  return { headers, rows, rowCount: rows.length };
}

function parseRow(line: string, delimiter: string): string[] {
  // Simple CSV parser handling quoted fields
  const fields: string[] = [];
  let current = '';
  let inQuotes = false;

  for (let i = 0; i < line.length; i++) {
    const ch = line[i];
    if (ch === '"') {
      if (inQuotes && line[i + 1] === '"') {
        current += '"';
        i++;
      } else {
        inQuotes = !inQuotes;
      }
    } else if (ch === delimiter && !inQuotes) {
      fields.push(current.trim());
      current = '';
    } else {
      current += ch;
    }
  }
  fields.push(current.trim());
  return fields;
}
```

**File**: `src/documents/document-indexer.ts` — add `indexCsv()`:

```typescript
private async indexCsv(filePath: string, content: string, hash: string): Promise<Entity[]> {
  const { parseCsv } = await import('./csv-parser.js');
  const csv = parseCsv(content, { maxRows: 10000 });
  const entities: Entity[] = [];
  const name = path.basename(filePath, path.extname(filePath));

  // Create document entity with schema summary
  const schemaDesc = csv.headers.map((h, i) => {
    // Infer column type from first non-empty value
    const sample = csv.rows.find(r => r[i]?.length > 0)?.[i] ?? '';
    const type = inferColumnType(sample);
    return `${h} (${type})`;
  }).join(', ');

  const docEntity = await this.entityStore.upsert({
    type: 'document',
    name,
    qualifiedName: filePath,
    content: `Columns: ${schemaDesc}\n\nSample rows:\n${this.formatSampleRows(csv, 10)}`,
    summary: `CSV dataset: ${name} — ${csv.rowCount} rows, ${csv.headers.length} columns (${csv.headers.slice(0, 5).join(', ')}${csv.headers.length > 5 ? '...' : ''})`,
    metadata: {
      hash,
      docType: 'csv',
      columns: csv.headers,
      rowCount: csv.rowCount,
      columnCount: csv.headers.length,
    },
    filePath,
  });
  entities.push(docEntity);

  // For large CSVs, create section entities for logical groups
  // Group by first column if it looks categorical, otherwise chunk by row ranges
  if (csv.rowCount > 50) {
    const chunks = this.chunkCsvRows(csv, { maxRowsPerChunk: 100 });
    for (const chunk of chunks) {
      const sectionEntity = await this.entityStore.upsert({
        type: 'section',
        name: `${name} - Rows ${chunk.startRow}-${chunk.endRow}`,
        qualifiedName: `${filePath}::rows-${chunk.startRow}-${chunk.endRow}`,
        content: this.formatCsvChunk(csv.headers, chunk.rows),
        summary: `Rows ${chunk.startRow}-${chunk.endRow} of ${name}`,
        metadata: { hash, docType: 'csv', startRow: chunk.startRow, endRow: chunk.endRow },
        filePath,
      });
      entities.push(sectionEntity);
      await this.addRelationship(docEntity.id, sectionEntity.id, 'CONTAINS');
    }
  }

  // Create variable entities for each column (useful for data dictionary queries)
  for (const header of csv.headers) {
    const colEntity = await this.entityStore.upsert({
      type: 'variable',
      name: header,
      qualifiedName: `${filePath}::column::${header}`,
      content: `Column "${header}" in ${name}`,
      metadata: { hash, docType: 'csv-column', dataset: name },
      filePath,
    });
    entities.push(colEntity);
    await this.addRelationship(docEntity.id, colEntity.id, 'CONTAINS');
  }

  return entities;
}

private formatSampleRows(csv: CsvDocument, count: number): string {
  const rows = csv.rows.slice(0, count);
  return rows.map(row =>
    csv.headers.map((h, i) => `${h}: ${row[i] ?? ''}`).join(' | ')
  ).join('\n');
}

private formatCsvChunk(headers: string[], rows: string[][]): string {
  const headerLine = headers.join(',');
  const dataLines = rows.map(r => r.join(',')).join('\n');
  return `${headerLine}\n${dataLines}`;
}
```

#### CSV-specific considerations

- **Delimiter detection**: Auto-detect common delimiters (comma, tab, semicolon, pipe) by analyzing the first few lines. Check which delimiter produces consistent column counts.
- **Large CSVs**: Cap at 10,000 rows by default. Store the full schema and sample rows in the document entity, with chunked sections for deeper content.
- **TSV support**: Treat `.tsv` files as CSV with tab delimiter.

---

### XML Support

#### Approach

Parse XML files using a lightweight streaming parser, creating entities for the document root and significant elements. Focus on common XML use cases in codebases: configuration files (pom.xml, web.xml, .csproj), data files, and API schemas (WSDL, XSD).

**No external dependency needed** — use a simple regex/state-machine XML parser for entity extraction. We don't need full DOM — just element names, attributes, text content, and hierarchy.

#### Implementation

**File**: `src/documents/xml-parser.ts`

```typescript
export interface XmlElement {
  tag: string;
  attributes: Record<string, string>;
  textContent: string;
  children: XmlElement[];
  path: string;  // XPath-like: /root/parent/child
  line: number;
}

export interface XmlDocument {
  rootTag: string;
  declaration?: Record<string, string>;  // <?xml version="1.0" ?>
  namespaces: Record<string, string>;
  elements: XmlElement[];  // Flattened for indexing
  fullText: string;
}

export function parseXml(content: string): XmlDocument {
  // Extract declaration
  const declMatch = content.match(/<\?xml\s+([^?]*)\?>/);
  const declaration = declMatch ? parseAttributes(declMatch[1]) : undefined;

  // Strip comments and processing instructions
  const cleaned = content
    .replace(/<!--[\s\S]*?-->/g, '')
    .replace(/<\?[\s\S]*?\?>/g, '');

  // Parse element tree and flatten
  const root = parseElement(cleaned, '', 1);
  const elements = flattenElements(root);
  const namespaces = extractNamespaces(root);

  return {
    rootTag: root?.tag ?? 'unknown',
    declaration,
    namespaces,
    elements,
    fullText: content,
  };
}
```

**File**: `src/documents/document-indexer.ts` — add `indexXml()`:

```typescript
private async indexXml(filePath: string, content: string, hash: string): Promise<Entity[]> {
  const { parseXml } = await import('./xml-parser.js');
  const xml = parseXml(content);
  const entities: Entity[] = [];
  const name = path.basename(filePath);

  // Detect XML subtype for specialized handling
  const xmlType = this.detectXmlType(filePath, xml);

  // Create document entity
  const docEntity = await this.entityStore.upsert({
    type: 'document',
    name,
    qualifiedName: filePath,
    content: content.length > 50000 ? content.slice(0, 50000) : content,
    summary: `XML document: ${name} (root: <${xml.rootTag}>, ${xml.elements.length} elements)${xmlType ? ` [${xmlType}]` : ''}`,
    metadata: {
      hash,
      docType: 'xml',
      xmlType,
      rootTag: xml.rootTag,
      namespaces: xml.namespaces,
      elementCount: xml.elements.length,
    },
    filePath,
  });
  entities.push(docEntity);

  // Create section entities for significant top-level elements
  const topLevelElements = xml.elements.filter(e => e.path.split('/').length <= 3);
  for (const elem of topLevelElements) {
    if (elem.textContent.length < 20 && elem.children.length === 0) continue;

    const elemContent = this.formatXmlElement(elem);
    if (elemContent.length < 30) continue;

    const sectionEntity = await this.entityStore.upsert({
      type: 'section',
      name: `${name} - <${elem.tag}>`,
      qualifiedName: `${filePath}::${elem.path}`,
      content: elemContent,
      summary: `<${elem.tag}> element in ${name}`,
      metadata: {
        hash,
        docType: 'xml',
        tag: elem.tag,
        attributes: elem.attributes,
        xpath: elem.path,
      },
      filePath,
      startLine: elem.line,
    });
    entities.push(sectionEntity);
    await this.addRelationship(docEntity.id, sectionEntity.id, 'CONTAINS');
  }

  // Specialized handling by XML type
  if (xmlType === 'maven-pom') {
    entities.push(...await this.indexMavenPom(filePath, xml, docEntity, hash));
  } else if (xmlType === 'csproj') {
    entities.push(...await this.indexCsproj(filePath, xml, docEntity, hash));
  }

  // Run document linker
  await this.linkCodeReferences(docEntity, content);

  return entities;
}

private detectXmlType(filePath: string, xml: XmlDocument): string | undefined {
  const basename = path.basename(filePath).toLowerCase();
  if (basename === 'pom.xml' || xml.rootTag === 'project' && xml.namespaces[''] === 'http://maven.apache.org/POM/4.0.0') return 'maven-pom';
  if (basename.endsWith('.csproj') || xml.rootTag === 'Project' && xml.namespaces['']?.includes('microsoft.com')) return 'csproj';
  if (basename === 'web.xml') return 'web-xml';
  if (basename.endsWith('.xsd') || xml.rootTag === 'schema') return 'xsd';
  if (basename.endsWith('.wsdl') || xml.rootTag === 'definitions') return 'wsdl';
  if (basename.endsWith('.svg') || xml.rootTag === 'svg') return 'svg';
  return undefined;
}

private async indexMavenPom(
  filePath: string, xml: XmlDocument, docEntity: Entity, hash: string
): Promise<Entity[]> {
  const entities: Entity[] = [];

  // Extract dependencies as technology entities
  const deps = xml.elements.filter(e => e.path.endsWith('/dependency'));
  for (const dep of deps) {
    const groupId = dep.children.find(c => c.tag === 'groupId')?.textContent ?? '';
    const artifactId = dep.children.find(c => c.tag === 'artifactId')?.textContent ?? '';
    const version = dep.children.find(c => c.tag === 'version')?.textContent ?? '';

    if (!artifactId) continue;

    const techEntity = await this.entityStore.upsert({
      type: 'technology',
      name: artifactId,
      qualifiedName: `${filePath}::dep::${groupId}:${artifactId}`,
      content: `Maven dependency: ${groupId}:${artifactId}:${version}`,
      metadata: { hash, groupId, artifactId, version, source: 'pom.xml' },
      filePath,
    });
    entities.push(techEntity);
    await this.addRelationship(docEntity.id, techEntity.id, 'DEPENDS_ON');
  }

  return entities;
}
```

#### XML-specific considerations

- **Namespace handling**: Strip namespaces for element matching but preserve them in metadata for accurate XPath queries.
- **Large XML files**: Cap content storage at 50KB. For very large XML (>1MB), only index the structural skeleton (element names, attributes) without full text content.
- **SVG files**: Detect and skip SVG files (they're XML but not useful for code context).
- **Binary-like XML**: Skip files that appear to be binary serialization (e.g., base64 content).

---

### Integration into the Pipeline

#### Update EXTENSION_MAP

**File**: `src/documents/document-indexer.ts`

```typescript
const EXTENSION_MAP: Record<string, DocumentType> = {
  // Existing
  '.md': 'markdown', '.mdx': 'markdown',
  '.yaml': 'yaml', '.yml': 'yaml',
  '.json': 'json',
  '.toml': 'toml',
  '.html': 'html', '.htm': 'html',
  '.txt': 'text', '.log': 'text',
  // New
  '.pdf': 'pdf',
  '.csv': 'csv', '.tsv': 'csv',
  '.xml': 'xml', '.xsd': 'xml', '.wsdl': 'xml',
  '.csproj': 'xml', '.fsproj': 'xml', '.vbproj': 'xml',
  '.pom': 'xml',
};
```

#### Update DocumentType

**File**: `src/documents/types.ts`

```typescript
export type DocumentType =
  | 'markdown' | 'yaml' | 'json' | 'toml' | 'html' | 'text'
  | 'pdf' | 'csv' | 'xml';  // New types
```

#### Update MCP tool description

The `index_document` tool's `type` parameter description should list the new types:

```typescript
type: {
  type: 'string',
  description: 'Document type (markdown, yaml, json, toml, html, text, pdf, csv, xml). Auto-detected from extension if not specified.',
}
```

#### Update CLI help text

The `index-document` CLI command should list supported formats including the new ones.

## File Changes

| File | Change |
|------|--------|
| `src/documents/pdf-parser.ts` | **New** — PDF text extraction |
| `src/documents/csv-parser.ts` | **New** — CSV parsing with quoted field support |
| `src/documents/xml-parser.ts` | **New** — XML parsing and element extraction |
| `src/documents/document-indexer.ts` | **Update** — add `indexPdf()`, `indexCsv()`, `indexXml()`, update EXTENSION_MAP |
| `src/documents/types.ts` | **Update** — add new DocumentType values |
| `src/mcp/tool-registry.ts` | **Update** — update `index_document` type description |
| `src/cli/index-doc-cmd.ts` | **Update** — update help text |
| `package.json` | **Update** — add `pdf-parse` dependency |

## New Dependency

| Package | Version | Size | Purpose |
|---------|---------|------|---------|
| `pdf-parse` | ^1.1.1 | ~200KB | PDF text extraction (no native deps) |

## Testing

**File**: `tests/phase-10i/pdf-parser.test.ts`
- Parse a sample PDF and verify text extraction
- Handle empty PDF, password-protected PDF (should fail gracefully)
- Handle large PDF (page count limit)

**File**: `tests/phase-10i/csv-parser.test.ts`
- Parse standard CSV, TSV
- Handle quoted fields with commas, escaped quotes
- Handle empty rows, missing columns
- Delimiter auto-detection

**File**: `tests/phase-10i/xml-parser.test.ts`
- Parse well-formed XML
- Handle namespaces, attributes, CDATA
- Detect XML subtypes (pom.xml, .csproj)
- Handle malformed XML gracefully

**File**: `tests/phase-10i/document-indexer-formats.test.ts`
- End-to-end: index a PDF and verify entities + relationships
- End-to-end: index a CSV and verify column entities
- End-to-end: index a pom.xml and verify dependency entities

## Success Criteria

- PDF files are indexed with page-level sections and code reference linking
- CSV files are indexed with column metadata and row chunking
- XML files are indexed with element-level sections and specialized handling for Maven POM and .csproj
- All three new types work through both CLI (`ctx-sys index-document`) and MCP (`index_document` tool)
- Hash-based change detection works for all three types
- Existing document types are unaffected

## Tasks

- [ ] Create `src/documents/pdf-parser.ts`
- [ ] Create `src/documents/csv-parser.ts`
- [ ] Create `src/documents/xml-parser.ts`
- [ ] Add `indexPdf()` to DocumentIndexer
- [ ] Add `indexCsv()` to DocumentIndexer
- [ ] Add `indexXml()` with Maven POM / .csproj specialization
- [ ] Update EXTENSION_MAP and DocumentType
- [ ] Add `pdf-parse` dependency
- [ ] Update MCP tool and CLI help text
- [ ] Write parser unit tests
- [ ] Write integration tests
- [ ] `tsc --noEmit` passes
