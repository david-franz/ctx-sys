# F10i.8 PDF, CSV, XML & C/C++/C# Support

**Phase**: 10i - Code Quality & Infrastructure
**Priority**: High
**Dependencies**: None

## Goal

Two complementary additions:

1. **Document formats** — Add first-class indexing support for PDF, CSV, and XML file types in the document indexing pipeline, extending the existing 6 supported formats (markdown, YAML, JSON, TOML, HTML, plain text) to 9.
2. **Language extractors** — Add dedicated AST extractors for C, C++, and C#. C/C++ already have tree-sitter grammars and extension mappings but fall through to GenericExtractor. C# has no support at all.

## Current State

The document indexer at `src/documents/document-indexer.ts` dispatches by file extension via `EXTENSION_MAP` (line 57-68):

```typescript
const EXTENSION_MAP: Record<string, DocumentType> = {
  '.md': 'markdown', '.mdx': 'markdown',
  '.yaml': 'yaml', '.yml': 'yaml',
  '.json': 'json',
  '.toml': 'toml',
  '.html': 'html', '.htm': 'html',
  '.txt': 'text', '.log': 'text',
};
```

Each type has a dedicated `index<Type>()` method that:
1. Reads the file content
2. Parses it into structured sections/entities
3. Creates entities (type `document` for the file, type `section` for chunks)
4. Establishes `CONTAINS` relationships
5. Runs `DocumentLinker` to find code references and create `DOCUMENTS` relationships

The pipeline supports:
- Hash-based change detection (skip unchanged files)
- Size-aware chunking (split large sections, merge small ones, add overlap)
- Requirement extraction from document content
- Code reference resolution against the entity store

## Implementation Plan

### PDF Support

#### Approach

Use `pdf-parse` (or `pdf2json`) to extract text content from PDFs, then process through the existing chunking and entity creation pipeline. PDFs are treated as flat text with optional page-level sectioning.

**Dependency**: `pdf-parse` — lightweight, no native dependencies, ~200KB

#### Implementation

**File**: `src/documents/pdf-parser.ts`

```typescript
import pdf from 'pdf-parse';
import { readFileSync } from 'fs';

export interface PdfPage {
  pageNumber: number;
  text: string;
}

export interface PdfDocument {
  title: string;
  author?: string;
  pages: PdfPage[];
  totalPages: number;
  fullText: string;
}

export async function parsePdf(filePath: string): Promise<PdfDocument> {
  const buffer = readFileSync(filePath);
  const data = await pdf(buffer);

  // pdf-parse returns full text; split by form feeds for page boundaries
  const pageTexts = data.text.split('\f').filter(t => t.trim().length > 0);

  return {
    title: data.info?.Title ?? path.basename(filePath, '.pdf'),
    author: data.info?.Author,
    pages: pageTexts.map((text, i) => ({ pageNumber: i + 1, text: text.trim() })),
    totalPages: data.numpages,
    fullText: data.text,
  };
}
```

**File**: `src/documents/document-indexer.ts` — add `indexPdf()`:

```typescript
private async indexPdf(filePath: string, content: Buffer, hash: string): Promise<Entity[]> {
  const { parsePdf } = await import('./pdf-parser.js');
  const pdf = await parsePdf(filePath);
  const entities: Entity[] = [];

  // Create top-level document entity
  const docEntity = await this.entityStore.upsert({
    type: 'document',
    name: pdf.title,
    qualifiedName: filePath,
    content: pdf.fullText,
    summary: `PDF document: ${pdf.title} (${pdf.totalPages} pages)`,
    metadata: { hash, docType: 'pdf', author: pdf.author, pages: pdf.totalPages },
    filePath,
  });
  entities.push(docEntity);

  // Create section entities per page (or per chunk if pages are large)
  for (const page of pdf.pages) {
    if (page.text.length < 50) continue;  // Skip near-empty pages

    const chunks = this.chunkText(page.text, {
      maxSize: 3000,
      overlap: 200,
      prefix: `Page ${page.pageNumber}`,
    });

    for (const chunk of chunks) {
      const sectionEntity = await this.entityStore.upsert({
        type: 'section',
        name: `${pdf.title} - Page ${page.pageNumber}`,
        qualifiedName: `${filePath}::page-${page.pageNumber}${chunks.length > 1 ? `-chunk-${chunk.index}` : ''}`,
        content: chunk.text,
        summary: chunk.text.slice(0, 200),
        metadata: {
          hash,
          docType: 'pdf',
          pageNumber: page.pageNumber,
          level: 1,
        },
        filePath,
        startLine: page.pageNumber,  // Use page number as line proxy
      });
      entities.push(sectionEntity);

      // CONTAINS relationship: document -> section
      await this.addRelationship(docEntity.id, sectionEntity.id, 'CONTAINS');
    }
  }

  // Run document linker to find code references
  await this.linkCodeReferences(docEntity, pdf.fullText);

  return entities;
}
```

#### PDF-specific considerations

- **Binary file handling**: The indexer currently reads files as UTF-8 text. For PDF, read as `Buffer` and pass to `pdf-parse`. Update the file reading logic to detect binary extensions and read accordingly.
- **Large PDFs**: Apply the existing page-size chunking. Cap at a configurable max page count (default: 200) to prevent memory issues on massive PDFs.
- **Scanned PDFs**: `pdf-parse` extracts embedded text only. Scanned PDFs without text layers will produce empty content — detect this and log a warning. OCR support is out of scope.

---

### CSV Support

#### Approach

Parse CSV files into structured data, creating entities for the dataset (document), columns (metadata), and optionally rows/groups. CSVs are useful for configuration data, data dictionaries, and lookup tables that AI agents reference.

**No external dependency needed** — use a lightweight built-in parser since CSV parsing is straightforward for well-formed files. For robustness with quoted fields and edge cases, use `src/documents/csv-parser.ts` with a simple state-machine parser.

#### Implementation

**File**: `src/documents/csv-parser.ts`

```typescript
export interface CsvDocument {
  headers: string[];
  rows: string[][];
  rowCount: number;
}

export function parseCsv(content: string, options?: {
  delimiter?: string;
  maxRows?: number;
}): CsvDocument {
  const delimiter = options?.delimiter ?? ',';
  const lines = content.split('\n');
  const headers = parseRow(lines[0], delimiter);
  const maxRows = options?.maxRows ?? 10000;

  const rows: string[][] = [];
  for (let i = 1; i < lines.length && rows.length < maxRows; i++) {
    const line = lines[i].trim();
    if (line.length === 0) continue;
    rows.push(parseRow(line, delimiter));
  }

  return { headers, rows, rowCount: rows.length };
}

function parseRow(line: string, delimiter: string): string[] {
  // Simple CSV parser handling quoted fields
  const fields: string[] = [];
  let current = '';
  let inQuotes = false;

  for (let i = 0; i < line.length; i++) {
    const ch = line[i];
    if (ch === '"') {
      if (inQuotes && line[i + 1] === '"') {
        current += '"';
        i++;
      } else {
        inQuotes = !inQuotes;
      }
    } else if (ch === delimiter && !inQuotes) {
      fields.push(current.trim());
      current = '';
    } else {
      current += ch;
    }
  }
  fields.push(current.trim());
  return fields;
}
```

**File**: `src/documents/document-indexer.ts` — add `indexCsv()`:

```typescript
private async indexCsv(filePath: string, content: string, hash: string): Promise<Entity[]> {
  const { parseCsv } = await import('./csv-parser.js');
  const csv = parseCsv(content, { maxRows: 10000 });
  const entities: Entity[] = [];
  const name = path.basename(filePath, path.extname(filePath));

  // Create document entity with schema summary
  const schemaDesc = csv.headers.map((h, i) => {
    // Infer column type from first non-empty value
    const sample = csv.rows.find(r => r[i]?.length > 0)?.[i] ?? '';
    const type = inferColumnType(sample);
    return `${h} (${type})`;
  }).join(', ');

  const docEntity = await this.entityStore.upsert({
    type: 'document',
    name,
    qualifiedName: filePath,
    content: `Columns: ${schemaDesc}\n\nSample rows:\n${this.formatSampleRows(csv, 10)}`,
    summary: `CSV dataset: ${name} — ${csv.rowCount} rows, ${csv.headers.length} columns (${csv.headers.slice(0, 5).join(', ')}${csv.headers.length > 5 ? '...' : ''})`,
    metadata: {
      hash,
      docType: 'csv',
      columns: csv.headers,
      rowCount: csv.rowCount,
      columnCount: csv.headers.length,
    },
    filePath,
  });
  entities.push(docEntity);

  // For large CSVs, create section entities for logical groups
  // Group by first column if it looks categorical, otherwise chunk by row ranges
  if (csv.rowCount > 50) {
    const chunks = this.chunkCsvRows(csv, { maxRowsPerChunk: 100 });
    for (const chunk of chunks) {
      const sectionEntity = await this.entityStore.upsert({
        type: 'section',
        name: `${name} - Rows ${chunk.startRow}-${chunk.endRow}`,
        qualifiedName: `${filePath}::rows-${chunk.startRow}-${chunk.endRow}`,
        content: this.formatCsvChunk(csv.headers, chunk.rows),
        summary: `Rows ${chunk.startRow}-${chunk.endRow} of ${name}`,
        metadata: { hash, docType: 'csv', startRow: chunk.startRow, endRow: chunk.endRow },
        filePath,
      });
      entities.push(sectionEntity);
      await this.addRelationship(docEntity.id, sectionEntity.id, 'CONTAINS');
    }
  }

  // Create variable entities for each column (useful for data dictionary queries)
  for (const header of csv.headers) {
    const colEntity = await this.entityStore.upsert({
      type: 'variable',
      name: header,
      qualifiedName: `${filePath}::column::${header}`,
      content: `Column "${header}" in ${name}`,
      metadata: { hash, docType: 'csv-column', dataset: name },
      filePath,
    });
    entities.push(colEntity);
    await this.addRelationship(docEntity.id, colEntity.id, 'CONTAINS');
  }

  return entities;
}

private formatSampleRows(csv: CsvDocument, count: number): string {
  const rows = csv.rows.slice(0, count);
  return rows.map(row =>
    csv.headers.map((h, i) => `${h}: ${row[i] ?? ''}`).join(' | ')
  ).join('\n');
}

private formatCsvChunk(headers: string[], rows: string[][]): string {
  const headerLine = headers.join(',');
  const dataLines = rows.map(r => r.join(',')).join('\n');
  return `${headerLine}\n${dataLines}`;
}
```

#### CSV-specific considerations

- **Delimiter detection**: Auto-detect common delimiters (comma, tab, semicolon, pipe) by analyzing the first few lines. Check which delimiter produces consistent column counts.
- **Large CSVs**: Cap at 10,000 rows by default. Store the full schema and sample rows in the document entity, with chunked sections for deeper content.
- **TSV support**: Treat `.tsv` files as CSV with tab delimiter.

---

### XML Support

#### Approach

Parse XML files using a lightweight streaming parser, creating entities for the document root and significant elements. Focus on common XML use cases in codebases: configuration files (pom.xml, web.xml, .csproj), data files, and API schemas (WSDL, XSD).

**No external dependency needed** — use a simple regex/state-machine XML parser for entity extraction. We don't need full DOM — just element names, attributes, text content, and hierarchy.

#### Implementation

**File**: `src/documents/xml-parser.ts`

```typescript
export interface XmlElement {
  tag: string;
  attributes: Record<string, string>;
  textContent: string;
  children: XmlElement[];
  path: string;  // XPath-like: /root/parent/child
  line: number;
}

export interface XmlDocument {
  rootTag: string;
  declaration?: Record<string, string>;  // <?xml version="1.0" ?>
  namespaces: Record<string, string>;
  elements: XmlElement[];  // Flattened for indexing
  fullText: string;
}

export function parseXml(content: string): XmlDocument {
  // Extract declaration
  const declMatch = content.match(/<\?xml\s+([^?]*)\?>/);
  const declaration = declMatch ? parseAttributes(declMatch[1]) : undefined;

  // Strip comments and processing instructions
  const cleaned = content
    .replace(/<!--[\s\S]*?-->/g, '')
    .replace(/<\?[\s\S]*?\?>/g, '');

  // Parse element tree and flatten
  const root = parseElement(cleaned, '', 1);
  const elements = flattenElements(root);
  const namespaces = extractNamespaces(root);

  return {
    rootTag: root?.tag ?? 'unknown',
    declaration,
    namespaces,
    elements,
    fullText: content,
  };
}
```

**File**: `src/documents/document-indexer.ts` — add `indexXml()`:

```typescript
private async indexXml(filePath: string, content: string, hash: string): Promise<Entity[]> {
  const { parseXml } = await import('./xml-parser.js');
  const xml = parseXml(content);
  const entities: Entity[] = [];
  const name = path.basename(filePath);

  // Detect XML subtype for specialized handling
  const xmlType = this.detectXmlType(filePath, xml);

  // Create document entity
  const docEntity = await this.entityStore.upsert({
    type: 'document',
    name,
    qualifiedName: filePath,
    content: content.length > 50000 ? content.slice(0, 50000) : content,
    summary: `XML document: ${name} (root: <${xml.rootTag}>, ${xml.elements.length} elements)${xmlType ? ` [${xmlType}]` : ''}`,
    metadata: {
      hash,
      docType: 'xml',
      xmlType,
      rootTag: xml.rootTag,
      namespaces: xml.namespaces,
      elementCount: xml.elements.length,
    },
    filePath,
  });
  entities.push(docEntity);

  // Create section entities for significant top-level elements
  const topLevelElements = xml.elements.filter(e => e.path.split('/').length <= 3);
  for (const elem of topLevelElements) {
    if (elem.textContent.length < 20 && elem.children.length === 0) continue;

    const elemContent = this.formatXmlElement(elem);
    if (elemContent.length < 30) continue;

    const sectionEntity = await this.entityStore.upsert({
      type: 'section',
      name: `${name} - <${elem.tag}>`,
      qualifiedName: `${filePath}::${elem.path}`,
      content: elemContent,
      summary: `<${elem.tag}> element in ${name}`,
      metadata: {
        hash,
        docType: 'xml',
        tag: elem.tag,
        attributes: elem.attributes,
        xpath: elem.path,
      },
      filePath,
      startLine: elem.line,
    });
    entities.push(sectionEntity);
    await this.addRelationship(docEntity.id, sectionEntity.id, 'CONTAINS');
  }

  // Specialized handling by XML type
  if (xmlType === 'maven-pom') {
    entities.push(...await this.indexMavenPom(filePath, xml, docEntity, hash));
  } else if (xmlType === 'csproj') {
    entities.push(...await this.indexCsproj(filePath, xml, docEntity, hash));
  }

  // Run document linker
  await this.linkCodeReferences(docEntity, content);

  return entities;
}

private detectXmlType(filePath: string, xml: XmlDocument): string | undefined {
  const basename = path.basename(filePath).toLowerCase();
  if (basename === 'pom.xml' || xml.rootTag === 'project' && xml.namespaces[''] === 'http://maven.apache.org/POM/4.0.0') return 'maven-pom';
  if (basename.endsWith('.csproj') || xml.rootTag === 'Project' && xml.namespaces['']?.includes('microsoft.com')) return 'csproj';
  if (basename === 'web.xml') return 'web-xml';
  if (basename.endsWith('.xsd') || xml.rootTag === 'schema') return 'xsd';
  if (basename.endsWith('.wsdl') || xml.rootTag === 'definitions') return 'wsdl';
  if (basename.endsWith('.svg') || xml.rootTag === 'svg') return 'svg';
  return undefined;
}

private async indexMavenPom(
  filePath: string, xml: XmlDocument, docEntity: Entity, hash: string
): Promise<Entity[]> {
  const entities: Entity[] = [];

  // Extract dependencies as technology entities
  const deps = xml.elements.filter(e => e.path.endsWith('/dependency'));
  for (const dep of deps) {
    const groupId = dep.children.find(c => c.tag === 'groupId')?.textContent ?? '';
    const artifactId = dep.children.find(c => c.tag === 'artifactId')?.textContent ?? '';
    const version = dep.children.find(c => c.tag === 'version')?.textContent ?? '';

    if (!artifactId) continue;

    const techEntity = await this.entityStore.upsert({
      type: 'technology',
      name: artifactId,
      qualifiedName: `${filePath}::dep::${groupId}:${artifactId}`,
      content: `Maven dependency: ${groupId}:${artifactId}:${version}`,
      metadata: { hash, groupId, artifactId, version, source: 'pom.xml' },
      filePath,
    });
    entities.push(techEntity);
    await this.addRelationship(docEntity.id, techEntity.id, 'DEPENDS_ON');
  }

  return entities;
}
```

#### XML-specific considerations

- **Namespace handling**: Strip namespaces for element matching but preserve them in metadata for accurate XPath queries.
- **Large XML files**: Cap content storage at 50KB. For very large XML (>1MB), only index the structural skeleton (element names, attributes) without full text content.
- **SVG files**: Detect and skip SVG files (they're XML but not useful for code context).
- **Binary-like XML**: Skip files that appear to be binary serialization (e.g., base64 content).

---

### C/C++ AST Extractor

#### Current State

C and C++ already have tree-sitter grammar support (`tree-sitter-cpp.wasm`) and extension mappings (`.c`, `.h`, `.cpp`, `.hpp`, `.cc`, `.cxx`), but the parser's `getExtractor()` dispatches them to `GenericExtractor`. The generic extractor does minimal pattern-matching (looks for node types containing "function", "class", etc.) but misses C/C++ specifics: structs, enums, namespaces, templates, `#include` directives, and visibility sections.

#### Approach

Create a dedicated `CppExtractor` extending `BaseExtractor` that handles the tree-sitter-cpp grammar's node types. C and C++ share the same grammar, so one extractor covers both. The extractor distinguishes C-style code (no classes, no namespaces) from C++ automatically based on what nodes are present.

#### Implementation

**File**: `src/ast/extractors/cpp.ts`

Key node types to handle (from tree-sitter-cpp grammar):

| Node Type | Symbol Type | Notes |
|-----------|-------------|-------|
| `function_definition` | `function` | Free functions, including `main()` |
| `declaration` (with function declarator) | `function` | Forward declarations / prototypes |
| `class_specifier` | `class` | C++ classes with methods, fields |
| `struct_specifier` | `class` | C structs and C++ structs (treated as class) |
| `enum_specifier` | `enum` | C/C++ enums, including `enum class` |
| `namespace_definition` | `namespace` | C++ namespaces, track for qualified names |
| `field_declaration` (inside class/struct) | `property` | Member variables |
| `function_definition` (inside class/struct) | `method` | Member functions |
| `type_definition` | `type` | `typedef` aliases |
| `using_declaration` / `type_alias_declaration` | `type` | C++ `using X = Y` |
| `template_declaration` | — | Wrapper; extract inner declaration, note template params |
| `preproc_include` | — | `#include` directives for import extraction |

```typescript
export class CppExtractor extends BaseExtractor {
  extractSymbols(node: SyntaxNode, filePath?: string): Symbol[] {
    const symbols: Symbol[] = [];
    this.visitNode(node, [], symbols, filePath);
    return symbols;
  }

  private visitNode(
    node: SyntaxNode, context: string[], symbols: Symbol[], filePath?: string
  ): void {
    switch (node.type) {
      case 'function_definition':
        // Check if inside class/struct body → method, else → function
        symbols.push(this.extractFunction(node, context, filePath));
        return; // Don't recurse into function body
      case 'class_specifier':
      case 'struct_specifier':
        symbols.push(this.extractClassOrStruct(node, context, filePath));
        return;
      case 'enum_specifier':
        symbols.push(this.extractEnum(node, context, filePath));
        return;
      case 'namespace_definition':
        this.extractNamespace(node, context, symbols, filePath);
        return;
      case 'template_declaration':
        // Unwrap: extract inner declaration with template params noted
        this.visitNode(node.namedChildren.at(-1)!, context, symbols, filePath);
        return;
    }
    // Recurse into children
    for (const child of node.namedChildren) {
      this.visitNode(child, context, symbols, filePath);
    }
  }

  extractImports(node: SyntaxNode): ImportStatement[] {
    // Find all #include directives
    const includes: ImportStatement[] = [];
    const visit = (n: SyntaxNode) => {
      if (n.type === 'preproc_include') {
        const pathNode = this.findChild(n, 'string_literal')
            ?? this.findChild(n, 'system_lib_string');
        if (pathNode) {
          const raw = this.getNodeText(pathNode);
          const source = raw.replace(/^[<"]|[>"]$/g, '');
          includes.push({
            source,
            specifiers: [],
            isDefault: false,
            startLine: n.startPosition.row + 1,
            endLine: n.endPosition.row + 1,
          });
        }
      }
      for (const child of n.namedChildren) visit(child);
    };
    visit(node);
    return includes;
  }

  extractExports(node: SyntaxNode): string[] {
    // C/C++ doesn't have explicit exports; all top-level symbols are visible
    return [];
  }
}
```

**Visibility handling**: C++ classes have `access_specifier` nodes (`public:`, `private:`, `protected:`) that apply to subsequent members. Track the current visibility as we traverse class body children.

**Template handling**: `template_declaration` wraps the actual declaration. Extract template params from `template_parameter_list` and note them in the symbol's metadata or signature.

**Qualified names**: Build hierarchically using the namespace + class context stack, e.g. `file.cpp::MyNamespace::MyClass::myMethod`.

---

### C# AST Extractor

#### Current State

C# has no support in the parser at all — no grammar, no extension mapping, no extractor.

#### Approach

Add `tree-sitter-c-sharp.wasm` grammar support and create a dedicated `CSharpExtractor`. C# is structurally similar to Java/TypeScript, with explicit visibility modifiers, interfaces, generics, properties with get/set, and `using` directives for imports.

#### Implementation

**Grammar**: The `@vscode/tree-sitter-wasm` package includes `tree-sitter-c_sharp.wasm`. Add it to `GRAMMAR_MAP` and `EXTENSION_MAP`.

**File**: `src/ast/types.ts` — Add `'csharp'` to `SupportedLanguage`:

```typescript
export type SupportedLanguage =
  | 'typescript' | 'javascript' | 'python'
  | 'go' | 'rust' | 'java'
  | 'c' | 'cpp' | 'csharp';
```

**File**: `src/ast/parser.ts` — Add grammar and extension mappings:

```typescript
// In GRAMMAR_MAP:
csharp: 'tree-sitter-c_sharp.wasm',

// In EXTENSION_MAP:
'.cs': 'csharp',
```

**File**: `src/ast/extractors/csharp.ts`

Key node types to handle (from tree-sitter-c-sharp grammar):

| Node Type | Symbol Type | Notes |
|-----------|-------------|-------|
| `class_declaration` | `class` | With base list, generic params |
| `struct_declaration` | `class` | Treated as class |
| `interface_declaration` | `interface` | With methods, properties |
| `method_declaration` | `method` | Inside class/struct/interface |
| `constructor_declaration` | `method` | Constructor |
| `property_declaration` | `property` | With get/set accessors |
| `field_declaration` | `property` | Member fields |
| `enum_declaration` | `enum` | Enum type |
| `namespace_declaration` | `namespace` | Track for qualified names |
| `record_declaration` | `class` | C# records (9.0+) |
| `delegate_declaration` | `type` | Delegate types |
| `using_directive` | — | Import extraction |

```typescript
export class CSharpExtractor extends BaseExtractor {
  extractSymbols(node: SyntaxNode, filePath?: string): Symbol[] {
    const symbols: Symbol[] = [];
    this.visitNode(node, [], symbols, filePath);
    return symbols;
  }

  private visitNode(
    node: SyntaxNode, context: string[], symbols: Symbol[], filePath?: string
  ): void {
    switch (node.type) {
      case 'class_declaration':
      case 'struct_declaration':
      case 'record_declaration':
        symbols.push(this.extractClass(node, context, filePath));
        return;
      case 'interface_declaration':
        symbols.push(this.extractInterface(node, context, filePath));
        return;
      case 'method_declaration':
      case 'constructor_declaration':
        symbols.push(this.extractMethod(node, context, filePath));
        return;
      case 'enum_declaration':
        symbols.push(this.extractEnum(node, context, filePath));
        return;
      case 'namespace_declaration':
        this.extractNamespace(node, context, symbols, filePath);
        return;
      case 'property_declaration':
        symbols.push(this.extractProperty(node, context, filePath));
        return;
    }
    for (const child of node.namedChildren) {
      this.visitNode(child, context, symbols, filePath);
    }
  }

  extractImports(node: SyntaxNode): ImportStatement[] {
    const imports: ImportStatement[] = [];
    const visit = (n: SyntaxNode) => {
      if (n.type === 'using_directive') {
        const nameNode = this.findChild(n, 'qualified_name')
            ?? this.findChild(n, 'identifier');
        if (nameNode) {
          imports.push({
            source: this.getNodeText(nameNode),
            specifiers: [],
            isDefault: false,
            startLine: n.startPosition.row + 1,
            endLine: n.endPosition.row + 1,
          });
        }
      }
      for (const child of n.namedChildren) visit(child);
    };
    visit(node);
    return imports;
  }

  extractExports(node: SyntaxNode): string[] {
    // C# uses `public` modifier rather than explicit exports
    // Collect all public top-level types
    const exports: string[] = [];
    for (const child of node.namedChildren) {
      if (['class_declaration', 'struct_declaration', 'interface_declaration',
           'enum_declaration', 'record_declaration'].includes(child.type)) {
        const mods = this.getNodeText(child).split(/\s+/);
        if (mods.includes('public')) {
          const nameNode = this.findChild(child, 'identifier');
          if (nameNode) exports.push(this.getNodeText(nameNode));
        }
      }
    }
    return exports;
  }
}
```

**Visibility**: C# declarations have modifier lists (`public`, `private`, `protected`, `internal`, `static`, `abstract`, `virtual`, `override`, `async`). Extract from `modifier` children preceding the declaration.

**Properties**: C# properties have `accessor_list` with `get_accessor_declaration` and `set_accessor_declaration`. Note whether the property is read-only, write-only, or read-write.

**Generics**: Handle `type_parameter_list` on classes, interfaces, and methods. Include in signature.

**Attributes**: C# `[Attribute]` syntax is similar to decorators. Extract from `attribute_list` nodes.

---

### Register New Extractors

**File**: `src/ast/parser.ts` — Update `getExtractor()`:

```typescript
private getExtractor(language: SupportedLanguage): LanguageExtractor {
  switch (language) {
    case 'typescript':
    case 'javascript':
      return new TypeScriptExtractor();
    case 'python':
      return new PythonExtractor();
    case 'c':
    case 'cpp':
      return new CppExtractor();
    case 'csharp':
      return new CSharpExtractor();
    default:
      return new GenericExtractor();
  }
}
```

**File**: `src/ast/extractors/index.ts` — Add barrel exports:

```typescript
export { CppExtractor } from './cpp';
export { CSharpExtractor } from './csharp';
```

---

### Integration into the Pipeline

#### Update EXTENSION_MAP

**File**: `src/documents/document-indexer.ts`

```typescript
const EXTENSION_MAP: Record<string, DocumentType> = {
  // Existing
  '.md': 'markdown', '.mdx': 'markdown',
  '.yaml': 'yaml', '.yml': 'yaml',
  '.json': 'json',
  '.toml': 'toml',
  '.html': 'html', '.htm': 'html',
  '.txt': 'text', '.log': 'text',
  // New
  '.pdf': 'pdf',
  '.csv': 'csv', '.tsv': 'csv',
  '.xml': 'xml', '.xsd': 'xml', '.wsdl': 'xml',
  '.csproj': 'xml', '.fsproj': 'xml', '.vbproj': 'xml',
  '.pom': 'xml',
};
```

#### Update DocumentType

**File**: `src/documents/types.ts`

```typescript
export type DocumentType =
  | 'markdown' | 'yaml' | 'json' | 'toml' | 'html' | 'text'
  | 'pdf' | 'csv' | 'xml';  // New types
```

#### Update MCP tool description

The `index_document` tool's `type` parameter description should list the new types:

```typescript
type: {
  type: 'string',
  description: 'Document type (markdown, yaml, json, toml, html, text, pdf, csv, xml). Auto-detected from extension if not specified.',
}
```

#### Update CLI help text

The `index-document` CLI command should list supported formats including the new ones.

## File Changes

| File | Change |
|------|--------|
| `src/documents/pdf-parser.ts` | **New** — PDF text extraction |
| `src/documents/csv-parser.ts` | **New** — CSV parsing with quoted field support |
| `src/documents/xml-parser.ts` | **New** — XML parsing and element extraction |
| `src/documents/document-indexer.ts` | **Update** — add `indexPdf()`, `indexCsv()`, `indexXml()`, update EXTENSION_MAP |
| `src/documents/types.ts` | **Update** — add new DocumentType values |
| `src/ast/extractors/cpp.ts` | **New** — C/C++ AST extractor |
| `src/ast/extractors/csharp.ts` | **New** — C# AST extractor |
| `src/ast/extractors/index.ts` | **Update** — export new extractors |
| `src/ast/parser.ts` | **Update** — register C/C++/C# extractors, add C# grammar + extensions |
| `src/ast/types.ts` | **Update** — add `'csharp'` to SupportedLanguage |
| `src/mcp/tool-registry.ts` | **Update** — update `index_document` type description |
| `src/cli/index-doc-cmd.ts` | **Update** — update help text |
| `package.json` | **Update** — add `pdf-parse` dependency |

## New Dependency

| Package | Version | Size | Purpose |
|---------|---------|------|---------|
| `pdf-parse` | ^1.1.1 | ~200KB | PDF text extraction (no native deps) |

## Testing

**File**: `tests/phase-10i/pdf-parser.test.ts`
- Parse a sample PDF and verify text extraction
- Handle empty PDF, password-protected PDF (should fail gracefully)
- Handle large PDF (page count limit)

**File**: `tests/phase-10i/csv-parser.test.ts`
- Parse standard CSV, TSV
- Handle quoted fields with commas, escaped quotes
- Handle empty rows, missing columns
- Delimiter auto-detection

**File**: `tests/phase-10i/xml-parser.test.ts`
- Parse well-formed XML
- Handle namespaces, attributes, CDATA
- Detect XML subtypes (pom.xml, .csproj)
- Handle malformed XML gracefully

**File**: `tests/phase-10i/cpp-extractor.test.ts`
- Extract functions, classes, structs, enums from C++ source
- Handle namespaces and nested classes for qualified names
- Extract `#include` directives as imports
- Handle visibility sections (`public:`, `private:`, `protected:`)
- Handle templates (extract inner declaration)
- Extract C-only code (no classes/namespaces)

**File**: `tests/phase-10i/csharp-extractor.test.ts`
- Extract classes, interfaces, structs, enums, records from C# source
- Handle namespaces for qualified names
- Extract `using` directives as imports
- Handle visibility modifiers (`public`, `private`, `protected`, `internal`)
- Extract properties with get/set accessors
- Extract constructors, static methods, async methods

**File**: `tests/phase-10i/document-indexer-formats.test.ts`
- End-to-end: index a PDF and verify entities + relationships
- End-to-end: index a CSV and verify column entities
- End-to-end: index a pom.xml and verify dependency entities

## Success Criteria

- PDF files are indexed with page-level sections and code reference linking
- CSV files are indexed with column metadata and row chunking
- XML files are indexed with element-level sections and specialized handling for Maven POM and .csproj
- All three new document types work through both CLI (`ctx-sys index-document`) and MCP (`index_document` tool)
- Hash-based change detection works for all three document types
- C/C++ files produce rich symbol extraction (classes, structs, functions, enums, namespaces, templates) instead of falling through to GenericExtractor
- C# files are fully supported with grammar loading, extension mapping, and dedicated extraction (classes, interfaces, methods, properties, enums, using directives)
- Existing document types and language extractors are unaffected

## Tasks

### Document format support
- [ ] Create `src/documents/pdf-parser.ts`
- [ ] Create `src/documents/csv-parser.ts`
- [ ] Create `src/documents/xml-parser.ts`
- [ ] Add `indexPdf()` to DocumentIndexer
- [ ] Add `indexCsv()` to DocumentIndexer
- [ ] Add `indexXml()` with Maven POM / .csproj specialization
- [ ] Update document EXTENSION_MAP and DocumentType
- [ ] Add `pdf-parse` dependency
- [ ] Update MCP tool and CLI help text
- [ ] Write document parser unit tests
- [ ] Write document integration tests

### Language extractor support
- [ ] Create `src/ast/extractors/cpp.ts` (C/C++ extractor)
- [ ] Create `src/ast/extractors/csharp.ts` (C# extractor)
- [ ] Add `'csharp'` to SupportedLanguage, GRAMMAR_MAP, EXTENSION_MAP
- [ ] Register CppExtractor and CSharpExtractor in `getExtractor()`
- [ ] Export new extractors from `src/ast/extractors/index.ts`
- [ ] Verify `tree-sitter-c_sharp.wasm` is available from `@vscode/tree-sitter-wasm`
- [ ] Write C/C++ extractor tests
- [ ] Write C# extractor tests

### Verification
- [ ] `tsc --noEmit` passes
- [ ] All new and existing tests pass
