# F8.2 Hot/Cold Memory API

**Phase**: 8 - Agent Patterns
**Priority**: High
**Dependencies**: F3.1 Message Storage, F3.2 Session Management, F6.2 Multi-Strategy Search

## Goal

Implement explicit hot/cold memory tiering with access-pattern promotion for efficient context management.

## Overview

Not all context is equally relevant at any moment:
- **Hot memory**: Immediately relevant, kept in-prompt (last few turns, current task)
- **Cold storage**: Historical, retrieved on-demand (old conversations, archived facts)

The current session lifecycle (active→archived→summarized) is time-based. This feature adds:
1. Explicit API for memory tier control
2. Access-pattern tracking (frequently recalled cold items get promoted)
3. Memory status reporting
4. Intelligent spilling and recall

## Data Model

```typescript
interface MemoryTier {
  tier: 'hot' | 'warm' | 'cold';
  sizeLimit: number;          // Max items or tokens
  currentSize: number;
}

interface MemoryItem {
  id: string;
  sessionId: string;
  content: string;
  type: 'message' | 'fact' | 'decision' | 'entity' | 'context';
  tier: 'hot' | 'warm' | 'cold';
  
  // Access tracking
  accessCount: number;
  lastAccessedAt: Date;
  createdAt: Date;
  
  // For promotion/demotion
  relevanceScore: number;
  metadata: Record<string, unknown>;
}

interface MemoryStatus {
  sessionId: string;
  hot: {
    items: number;
    tokens: number;
    limit: number;
    utilizationPercent: number;
  };
  warm: {
    items: number;
    tokens: number;
  };
  cold: {
    items: number;
    tokens: number;
  };
  suggestions: MemorySuggestion[];
}

interface MemorySuggestion {
  type: 'spill' | 'recall' | 'prune';
  reason: string;
  itemIds?: string[];
}

interface RecallResult {
  items: MemoryItem[];
  promoted: string[];           // Items moved from cold to hot
  relevanceScores: Map<string, number>;
}
```

## Database Schema

```sql
-- Memory items with tier tracking
CREATE TABLE memory_items (
  id TEXT PRIMARY KEY,
  session_id TEXT NOT NULL,
  content TEXT NOT NULL,
  type TEXT NOT NULL,
  tier TEXT NOT NULL DEFAULT 'hot',
  
  -- Access tracking
  access_count INTEGER DEFAULT 0,
  last_accessed_at TEXT,
  created_at TEXT NOT NULL,
  
  -- Scoring
  relevance_score REAL DEFAULT 0.5,
  token_count INTEGER,
  
  -- Metadata
  metadata_json TEXT,
  embedding BLOB,              -- For semantic recall
  
  FOREIGN KEY (session_id) REFERENCES sessions(id)
);

CREATE INDEX idx_memory_session_tier ON memory_items(session_id, tier);
CREATE INDEX idx_memory_access ON memory_items(session_id, last_accessed_at DESC);
CREATE INDEX idx_memory_relevance ON memory_items(session_id, relevance_score DESC);
```

## Implementation

### File: `src/agent/memory-tier.ts`

```typescript
import { Database } from '../db/database';
import { EmbeddingManager } from '../embeddings/manager';
import { estimateTokens } from '../utils/tokens';

export class MemoryTierManager {
  private config: MemoryConfig;
  
  constructor(
    private db: Database,
    private embeddingManager: EmbeddingManager,
    config: Partial<MemoryConfig> = {}
  ) {
    this.config = { ...defaultMemoryConfig, ...config };
  }

  /**
   * Add item to hot memory
   */
  async addToHot(
    sessionId: string,
    content: string,
    type: MemoryItem['type'],
    metadata: Record<string, unknown> = {}
  ): Promise<MemoryItem> {
    // Check if spill needed
    const status = await this.getStatus(sessionId);
    if (status.hot.tokens + estimateTokens(content) > status.hot.limit) {
      await this.spillToWarm(sessionId);
    }

    const item: MemoryItem = {
      id: generateId('mem'),
      sessionId,
      content,
      type,
      tier: 'hot',
      accessCount: 0,
      lastAccessedAt: new Date(),
      createdAt: new Date(),
      relevanceScore: 1.0,  // New items start with high relevance
      metadata
    };

    const embedding = await this.embeddingManager.embed(content);
    const tokenCount = estimateTokens(content);

    await this.db.run(`
      INSERT INTO memory_items (
        id, session_id, content, type, tier,
        access_count, last_accessed_at, created_at,
        relevance_score, token_count, metadata_json, embedding
      ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    `, [
      item.id, item.sessionId, item.content, item.type, item.tier,
      item.accessCount, item.lastAccessedAt.toISOString(), item.createdAt.toISOString(),
      item.relevanceScore, tokenCount, JSON.stringify(item.metadata),
      Buffer.from(new Float32Array(embedding).buffer)
    ]);

    return item;
  }

  /**
   * Get current hot memory contents
   */
  async getHot(sessionId: string): Promise<MemoryItem[]> {
    const rows = await this.db.all(`
      SELECT * FROM memory_items
      WHERE session_id = ? AND tier = 'hot'
      ORDER BY created_at DESC
    `, [sessionId]);

    return rows.map(this.rowToItem);
  }

  /**
   * Force spill from hot to warm/cold
   */
  async spillToWarm(
    sessionId: string,
    options: SpillOptions = {}
  ): Promise<SpillResult> {
    const itemsToSpill = options.itemIds
      ? await this.getItemsByIds(options.itemIds)
      : await this.selectItemsForSpill(sessionId, options.count || 4);

    const spilled: string[] = [];
    
    for (const item of itemsToSpill) {
      // Determine target tier based on access patterns
      const targetTier = item.accessCount > this.config.warmAccessThreshold ? 'warm' : 'cold';
      
      await this.db.run(`
        UPDATE memory_items SET tier = ? WHERE id = ?
      `, [targetTier, item.id]);
      
      spilled.push(item.id);
    }

    return {
      spilledCount: spilled.length,
      spilledIds: spilled,
      targetTier: 'warm'
    };
  }

  /**
   * Recall items from cold storage based on query
   */
  async recall(
    sessionId: string,
    query: string,
    options: RecallOptions = {}
  ): Promise<RecallResult> {
    const limit = options.limit || 3;
    const queryEmbedding = await this.embeddingManager.embed(query);
    
    // Search cold/warm storage by semantic similarity
    const rows = await this.db.all(`
      SELECT *, vec_distance_cosine(embedding, ?) as distance
      FROM memory_items
      WHERE session_id = ? AND tier IN ('warm', 'cold')
      ORDER BY distance ASC
      LIMIT ?
    `, [
      Buffer.from(new Float32Array(queryEmbedding).buffer),
      sessionId,
      limit
    ]);

    const items = rows.map(this.rowToItem);
    const promoted: string[] = [];
    const relevanceScores = new Map<string, number>();

    // Track access and potentially promote
    for (const item of items) {
      const relevance = 1 - (rows.find(r => r.id === item.id)?.distance || 0);
      relevanceScores.set(item.id, relevance);
      
      // Update access tracking
      await this.db.run(`
        UPDATE memory_items 
        SET access_count = access_count + 1,
            last_accessed_at = ?,
            relevance_score = (relevance_score + ?) / 2
        WHERE id = ?
      `, [new Date().toISOString(), relevance, item.id]);

      // Promote if high relevance and auto-promote enabled
      if (options.autoPromote !== false && relevance > this.config.promoteThreshold) {
        await this.promoteToHot(item.id);
        promoted.push(item.id);
      }
    }

    return { items, promoted, relevanceScores };
  }

  /**
   * Promote item from cold/warm to hot
   */
  async promoteToHot(itemId: string): Promise<void> {
    // Check hot memory capacity first
    const item = await this.getItem(itemId);
    if (!item) return;

    const status = await this.getStatus(item.sessionId);
    if (status.hot.tokens + estimateTokens(item.content) > status.hot.limit) {
      await this.spillToWarm(item.sessionId, { count: 2 });
    }

    await this.db.run(`
      UPDATE memory_items SET tier = 'hot', relevance_score = 1.0 WHERE id = ?
    `, [itemId]);
  }

  /**
   * Get memory status for a session
   */
  async getStatus(sessionId: string): Promise<MemoryStatus> {
    const stats = await this.db.all(`
      SELECT tier, COUNT(*) as count, SUM(token_count) as tokens
      FROM memory_items
      WHERE session_id = ?
      GROUP BY tier
    `, [sessionId]);

    const tierStats = {
      hot: { items: 0, tokens: 0 },
      warm: { items: 0, tokens: 0 },
      cold: { items: 0, tokens: 0 }
    };

    for (const row of stats) {
      tierStats[row.tier as keyof typeof tierStats] = {
        items: row.count,
        tokens: row.tokens || 0
      };
    }

    const suggestions = this.generateSuggestions(tierStats, this.config);

    return {
      sessionId,
      hot: {
        ...tierStats.hot,
        limit: this.config.hotTokenLimit,
        utilizationPercent: (tierStats.hot.tokens / this.config.hotTokenLimit) * 100
      },
      warm: tierStats.warm,
      cold: tierStats.cold,
      suggestions
    };
  }

  /**
   * Select items for spilling based on relevance and recency
   */
  private async selectItemsForSpill(
    sessionId: string,
    count: number
  ): Promise<MemoryItem[]> {
    // Select oldest, lowest-relevance items from hot
    const rows = await this.db.all(`
      SELECT * FROM memory_items
      WHERE session_id = ? AND tier = 'hot'
      ORDER BY relevance_score ASC, created_at ASC
      LIMIT ?
    `, [sessionId, count]);

    return rows.map(this.rowToItem);
  }

  private generateSuggestions(
    stats: Record<string, { items: number; tokens: number }>,
    config: MemoryConfig
  ): MemorySuggestion[] {
    const suggestions: MemorySuggestion[] = [];

    // Suggest spill if hot is near capacity
    if (stats.hot.tokens > config.hotTokenLimit * 0.9) {
      suggestions.push({
        type: 'spill',
        reason: 'Hot memory near capacity (>90%)'
      });
    }

    // Suggest pruning cold if very large
    if (stats.cold.items > config.maxColdItems) {
      suggestions.push({
        type: 'prune',
        reason: `Cold storage exceeds ${config.maxColdItems} items`
      });
    }

    return suggestions;
  }

  private async getItem(id: string): Promise<MemoryItem | null> {
    const row = await this.db.get(`SELECT * FROM memory_items WHERE id = ?`, [id]);
    return row ? this.rowToItem(row) : null;
  }

  private async getItemsByIds(ids: string[]): Promise<MemoryItem[]> {
    const placeholders = ids.map(() => '?').join(',');
    const rows = await this.db.all(`
      SELECT * FROM memory_items WHERE id IN (${placeholders})
    `, ids);
    return rows.map(this.rowToItem);
  }

  private rowToItem(row: any): MemoryItem {
    return {
      id: row.id,
      sessionId: row.session_id,
      content: row.content,
      type: row.type,
      tier: row.tier,
      accessCount: row.access_count,
      lastAccessedAt: new Date(row.last_accessed_at),
      createdAt: new Date(row.created_at),
      relevanceScore: row.relevance_score,
      metadata: JSON.parse(row.metadata_json || '{}')
    };
  }
}

interface MemoryConfig {
  hotTokenLimit: number;
  warmAccessThreshold: number;  // Access count to stay in warm vs cold
  promoteThreshold: number;     // Relevance score to auto-promote
  maxColdItems: number;
}

const defaultMemoryConfig: MemoryConfig = {
  hotTokenLimit: 4000,
  warmAccessThreshold: 3,
  promoteThreshold: 0.85,
  maxColdItems: 1000
};

interface SpillOptions {
  itemIds?: string[];
  count?: number;
}

interface SpillResult {
  spilledCount: number;
  spilledIds: string[];
  targetTier: 'warm' | 'cold';
}

interface RecallOptions {
  limit?: number;
  autoPromote?: boolean;
  types?: MemoryItem['type'][];
}
```

## MCP Tools

```typescript
{
  name: 'memory_add',
  description: 'Add item to hot memory',
  inputSchema: {
    type: 'object',
    properties: {
      sessionId: { type: 'string' },
      content: { type: 'string' },
      type: { type: 'string', enum: ['message', 'fact', 'decision', 'entity', 'context'] }
    },
    required: ['sessionId', 'content', 'type']
  }
},
{
  name: 'memory_recall',
  description: 'Recall relevant items from cold storage',
  inputSchema: {
    type: 'object',
    properties: {
      sessionId: { type: 'string' },
      query: { type: 'string', description: 'What to search for' },
      limit: { type: 'number', default: 3 },
      autoPromote: { type: 'boolean', default: true }
    },
    required: ['sessionId', 'query']
  }
},
{
  name: 'memory_spill',
  description: 'Force spill hot memory to cold storage',
  inputSchema: {
    type: 'object',
    properties: {
      sessionId: { type: 'string' },
      count: { type: 'number', description: 'Number of items to spill' }
    },
    required: ['sessionId']
  }
},
{
  name: 'memory_status',
  description: 'Get memory tier status and suggestions',
  inputSchema: {
    type: 'object',
    properties: {
      sessionId: { type: 'string' }
    },
    required: ['sessionId']
  }
}
```

## Examples

### Example 1: Conversation with Memory Management

```
Turn 1-6: Discussion about project proposal
  → Hot: [Turn 1, 2, 3, 4, 5, 6] (1200 tokens)

Turn 7: Hot memory approaching limit
  → Auto-spill: Turns 1-2 → Cold
  → Hot: [Turn 3, 4, 5, 6, 7] (1000 tokens)

Turn 30: "What was the budget we discussed earlier?"
  → Recall query: "budget discussed"
  → Found in Cold: Turn 2 mentions "$50K budget"
  → Turn 2 promoted to Hot (relevance: 0.92)
  → Response: "The budget was $50K"
```

### Example 2: Memory Status Check

```json
{
  "sessionId": "session-123",
  "hot": {
    "items": 8,
    "tokens": 3200,
    "limit": 4000,
    "utilizationPercent": 80
  },
  "warm": {
    "items": 12,
    "tokens": 2400
  },
  "cold": {
    "items": 45,
    "tokens": 8500
  },
  "suggestions": [
    {
      "type": "spill",
      "reason": "Hot memory at 80% capacity"
    }
  ]
}
```

## Configuration

```yaml
# ctx-sys.yaml
agent:
  memory:
    hotTokenLimit: 4000
    warmAccessThreshold: 3
    promoteThreshold: 0.85
    maxColdItems: 1000
    autoSpillEnabled: true
    autoPromoteEnabled: true
```

## Testing

```typescript
describe('MemoryTierManager', () => {
  it('should auto-spill when hot memory exceeds limit', async () => {
    // Fill hot memory
    for (let i = 0; i < 20; i++) {
      await manager.addToHot('session-1', `Message ${i} with some content`, 'message');
    }
    
    const status = await manager.getStatus('session-1');
    expect(status.hot.tokens).toBeLessThanOrEqual(status.hot.limit);
    expect(status.cold.items).toBeGreaterThan(0);
  });

  it('should recall from cold storage', async () => {
    await manager.addToHot('session-1', 'The budget is $50K', 'fact');
    await manager.spillToWarm('session-1');
    
    const result = await manager.recall('session-1', 'what was the budget?');
    expect(result.items[0].content).toContain('$50K');
  });

  it('should promote frequently accessed items', async () => {
    await manager.addToHot('session-1', 'Important fact', 'fact');
    await manager.spillToWarm('session-1');
    
    // Access multiple times
    for (let i = 0; i < 5; i++) {
      await manager.recall('session-1', 'important fact');
    }
    
    const hot = await manager.getHot('session-1');
    expect(hot.some(i => i.content === 'Important fact')).toBe(true);
  });
});
```

## Metrics to Track

| Metric | Description |
|--------|-------------|
| `memory_hot_utilization` | Hot memory usage percentage |
| `memory_spill_count` | Number of spill operations |
| `memory_recall_count` | Number of recall operations |
| `memory_promotion_count` | Items promoted to hot |
| `memory_recall_latency_ms` | Time to recall from cold |
| `memory_hit_rate` | % of recalls that found relevant items |
