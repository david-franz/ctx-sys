# F10b.7 Fix summarize_session LLM Integration

**Phase**: 10b - MCP Tool Fixes
**Priority**: Low
**Dependencies**: None

## Problem

`summarize_session` produces a naive string concatenation instead of an LLM-powered summary. The current output is:

```
"Session with 2 user messages and 1 assistant responses. Topics discussed: How does indexin..., The indexer uses t..., ..."
```

### Root Cause

`CoreService.summarizeSession()` (lines 389-405) uses a template string instead of calling the LLM summarization provider.

## Implementation Plan

### Step 1: Use LLM When Available, Fallback to Template

**File**: `src/services/core-service.ts`

```typescript
async summarizeSession(projectId: string, sessionId: string): Promise<string> {
  const messageStore = this.getMessageStore(projectId);
  const messages = await messageStore.getBySession(sessionId);

  if (messages.length === 0) return 'Empty session.';

  // Build transcript
  const transcript = messages
    .map(m => `[${m.role}]: ${m.content}`)
    .join('\n\n');

  // Try LLM summarization
  let summary: string;
  try {
    const { LLMSummarizationManager } = await import('../summarization/llm-manager');
    const manager = new LLMSummarizationManager();
    const provider = await manager.getProvider();

    if (provider) {
      summary = await provider.summarize(
        `Summarize this conversation session concisely:\n\n${transcript.slice(0, 4000)}`
      );
    } else {
      summary = this.buildTemplateSummary(messages);
    }
  } catch {
    summary = this.buildTemplateSummary(messages);
  }

  // Mark session as summarized
  const sessionManager = this.getSessionManager(projectId);
  sessionManager.markSummarized(sessionId, summary);

  return summary;
}

private buildTemplateSummary(messages: Message[]): string {
  const userMessages = messages.filter(m => m.role === 'user').length;
  const assistantMessages = messages.filter(m => m.role === 'assistant').length;
  return `Session with ${userMessages} user messages and ${assistantMessages} assistant responses. ` +
    `Topics discussed: ${messages.slice(0, 3).map(m => m.content.slice(0, 50)).join(', ')}...`;
}
```

## Testing

```typescript
describe('summarize_session', () => {
  it('should produce a summary string', async () => {
    const result = await mcp.call('summarize_session', { session: testSessionId });
    expect(result.summary).toBeTruthy();
    expect(typeof result.summary).toBe('string');
  });

  it('should fall back to template when no LLM available', async () => {
    // With no Ollama running, should still produce a summary
    const result = await mcp.call('summarize_session', { session: testSessionId });
    expect(result.summary).toContain('Session with');
  });
});
```

## Success Criteria

- Uses LLM when available (Ollama, OpenAI, Anthropic)
- Gracefully falls back to template summary when no LLM provider
- Session is marked as summarized in database
