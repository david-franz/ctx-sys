\documentclass[12pt,a4paper]{report}

% ============================================================================
% PACKAGES
% ============================================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{setspace}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{float}
\usepackage[acronym,toc]{glossaries}
\usepackage[backend=bibtex,style=numeric]{biblatex}
\addbibresource{references.bib}
\usepackage{appendix}
\usepackage{enumitem}

% ============================================================================
% CONFIGURATION
% ============================================================================
\geometry{margin=1in}
\pgfplotsset{compat=1.18}
\usetikzlibrary{shapes,arrows,positioning,calc,fit,backgrounds}

\hypersetup{
    colorlinks=true,
    linkcolor=blue!60!black,
    citecolor=green!50!black,
    urlcolor=blue!70!black
}

% Code listing configuration
\definecolor{codebg}{rgb}{0.95,0.95,0.95}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}

\lstdefinestyle{typescript}{
    backgroundcolor=\color{codebg},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    language=Java,
    morekeywords={async,await,interface,type,implements,extends,export,import,const,let,from,as,Promise,Map,Set,Record,Partial,Required}
}

\lstdefinestyle{sql}{
    backgroundcolor=\color{codebg},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    language=SQL
}

\lstdefinelanguage{yaml}{
    keywords={true,false,null,y,n},
    sensitive=false,
    comment=[l]{\#},
    morestring=[b]',
    morestring=[b]"
}

\lstdefinestyle{yaml}{
    backgroundcolor=\color{codebg},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    language=yaml
}

\lstset{style=typescript}

% Page style
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% Spacing
\onehalfspacing

% Theorem environments
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{example}{Example}[chapter]
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}

% Acronyms
\makeglossaries
\newacronym{rag}{RAG}{Retrieval-Augmented Generation}
\newacronym{llm}{LLM}{Large Language Model}
\newacronym{ast}{AST}{Abstract Syntax Tree}
\newacronym{mcp}{MCP}{Model Context Protocol}
\newacronym{fts}{FTS}{Full-Text Search}
\newacronym{rrf}{RRF}{Reciprocal Rank Fusion}
\newacronym{hyde}{HyDE}{Hypothetical Document Embeddings}
\newacronym{bfs}{BFS}{Breadth-First Search}
\newacronym{dfs}{DFS}{Depth-First Search}
\newacronym{cte}{CTE}{Common Table Expression}
\newacronym{api}{API}{Application Programming Interface}
\newacronym{cli}{CLI}{Command Line Interface}
\newacronym{lru}{LRU}{Least Recently Used}

% ============================================================================
% DOCUMENT BEGIN
% ============================================================================
\begin{document}

% ============================================================================
% TITLE PAGE
% ============================================================================
\begin{titlepage}
    \centering
    \vspace*{2cm}
    
    {\Huge\textbf{ctx-sys: An Intelligent Context Management System for AI-Assisted Software Development}}\\[1cm]
    
    {\large A Hybrid Retrieval Architecture Combining Vector Embeddings, Graph Traversal, and Semantic Analysis for Scalable Code Understanding}\\[2cm]
    
    {\Large David Franz}\\[0.5cm]
    
    \vfill
    
    {\large February 2026}\\[1cm]
    
\end{titlepage}

% ============================================================================
% ABSTRACT
% ============================================================================
\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

Modern AI coding assistants, built upon Large Language Models (LLMs), face fundamental limitations in context management. As conversations extend and codebases grow beyond context window capacities, these systems lose access to critical information---past decisions, code dependencies, documentation, and architectural context. This thesis presents \textbf{ctx-sys}, an intelligent context management framework that addresses these limitations through a hybrid retrieval architecture.

The system implements a ``smart librarian'' paradigm: rather than attempting to fit all information into limited context windows, it indexes, extracts semantic meaning, and retrieves precisely relevant context on demand. The architecture integrates multiple retrieval strategies---vector similarity search using dense embeddings, graph-based traversal of code relationships, and keyword-based full-text search---unified through Reciprocal Rank Fusion (RRF).

Key contributions include: (1) a unified entity model supporting heterogeneous information types including code symbols, documentation sections, conversation history, and domain concepts; (2) multi-language AST parsing using tree-sitter for structural code understanding; (3) advanced retrieval techniques including HyDE (Hypothetical Document Embeddings) for vocabulary mismatch resolution and retrieval gating for computational efficiency; (4) a draft-critique loop for hallucination detection; and (5) agent-oriented memory management with explicit hot/cold tiering and checkpointing for resumable execution.

The implementation comprises approximately 20,000 lines of TypeScript with comprehensive test coverage (1271 tests across 9 implementation phases). Integration with AI assistants is achieved through the Model Context Protocol (MCP), enabling compatibility with Claude, GitHub Copilot, and similar tools. Evaluation demonstrates significant improvements in context relevance and token efficiency compared to naive full-context approaches.

\textbf{Keywords:} Retrieval-Augmented Generation, Context Management, AI Coding Assistants, Graph RAG, Code Intelligence, Embedding Search

% ============================================================================
% ACKNOWLEDGMENTS
% ============================================================================
\chapter*{Acknowledgments}
\addcontentsline{toc}{chapter}{Acknowledgments}

% TODO: Add acknowledgments
I would like to express my gratitude to my supervisor for their guidance throughout this research. Thanks also to the open-source communities behind the tools and libraries that made this work possible, including tree-sitter, sql.js, and the Model Context Protocol SDK.

% ============================================================================
% TABLE OF CONTENTS
% ============================================================================
\tableofcontents
\listoffigures
\listoftables
\listofalgorithms
\addcontentsline{toc}{chapter}{List of Algorithms}
\printglossary[type=\acronymtype,title=List of Acronyms]

% ============================================================================
% CHAPTER 1: INTRODUCTION
% ============================================================================
\chapter{Introduction}
\label{ch:introduction}

\section{Motivation}

The integration of Large Language Models (LLMs) into software development workflows has transformed how developers write, understand, and maintain code. Tools such as GitHub Copilot, Claude, and ChatGPT have demonstrated remarkable capabilities in code generation, explanation, and debugging. However, these systems share a fundamental limitation: \textit{context constraints}.

Modern LLMs operate within fixed context windows---typically ranging from 8,000 to 200,000 tokens---that must accommodate both the input prompt and the generated response. While these windows have expanded significantly over recent years, they remain insufficient for comprehensive software development tasks for several reasons:

\begin{enumerate}
    \item \textbf{Codebase Scale}: Production codebases routinely contain millions of lines of code across thousands of files. Even a moderately-sized project of 100,000 lines exceeds any practical context window.
    
    \item \textbf{Information Dispersion}: Relevant context for a given task is typically scattered across multiple files, documentation, commit history, issue trackers, and past conversations.
    
    \item \textbf{Conversation Drift}: Extended development sessions accumulate decisions, clarifications, and contextual understanding that degrades as conversations exceed context limits.
    
    \item \textbf{Dependency Complexity}: Understanding a single function may require tracing imports, type definitions, base classes, and documentation across dozens of files.
\end{enumerate}

The naive approach of simply expanding context windows is economically and computationally prohibitive. Token costs scale linearly or super-linearly with context size, and attention mechanisms exhibit quadratic complexity. More fundamentally, larger contexts do not solve the problem of \textit{relevance}---determining which information among vast repositories actually pertains to the current task.

\section{Problem Statement}

This thesis addresses the following research question:

\begin{quote}
\textit{How can we design a context management system that enables AI coding assistants to access relevant information from large, heterogeneous knowledge sources while minimizing token consumption and maximizing retrieval precision?}
\end{quote}

This decomposes into several sub-problems:

\begin{enumerate}
    \item How should diverse information types (code, documentation, conversations, domain knowledge) be represented in a unified retrieval system?
    
    \item What retrieval strategies are most effective for different query types, and how should multiple strategies be combined?
    
    \item How can structural code understanding (AST analysis) enhance retrieval beyond surface-level text matching?
    
    \item How should conversation history be managed to preserve important decisions while respecting context limits?
    
    \item What mechanisms can detect and prevent hallucinations when LLMs generate responses based on retrieved context?
\end{enumerate}

\section{Contributions}

This thesis makes the following contributions:

\begin{enumerate}
    \item \textbf{Unified Entity Model}: A flexible data model that represents code symbols, documentation sections, conversation messages, and domain concepts as first-class entities with vector embeddings and typed relationships.
    
    \item \textbf{Hybrid Retrieval Architecture}: A multi-strategy search system combining vector similarity, graph traversal, and full-text search with Reciprocal Rank Fusion for result combination.
    
    \item \textbf{Code Intelligence Pipeline}: Integration of tree-sitter AST parsing with AI-powered summarization for structural code understanding across multiple programming languages.
    
    \item \textbf{Advanced Retrieval Techniques}: Implementation of HyDE query expansion for vocabulary mismatch resolution and retrieval gating for computational efficiency.
    
    \item \textbf{Verification Mechanisms}: A draft-critique loop that validates LLM responses against retrieved context to detect hallucinations.
    
    \item \textbf{Agent Memory Management}: Hot/cold memory tiering with explicit APIs and checkpointing for resumable long-running tasks.
    
    \item \textbf{Reference Implementation}: A complete, tested implementation (ctx-sys) with MCP integration for practical deployment.
\end{enumerate}

\section{Thesis Structure}

The remainder of this thesis is organized as follows:

\begin{description}
    \item[Chapter 2] reviews related work in retrieval-augmented generation, code intelligence, and context management for LLMs.
    
    \item[Chapter 3] presents the system architecture, design principles, and component interactions.
    
    \item[Chapter 4] details the data model, database schema, and storage layer implementation.
    
    \item[Chapter 5] describes the code intelligence pipeline including AST parsing, symbol extraction, and relationship analysis.
    
    \item[Chapter 6] covers the embedding and retrieval system, including multi-strategy search and fusion algorithms.
    
    \item[Chapter 7] explains advanced retrieval techniques: HyDE expansion, retrieval gating, and the draft-critique loop.
    
    \item[Chapter 8] discusses conversation management, session handling, and decision extraction.
    
    \item[Chapter 9] presents the agent patterns: checkpointing, memory tiering, and proactive context.
    
    \item[Chapter 10] describes the system integration through MCP, CLI, and configuration management.
    
    \item[Chapter 11] presents evaluation methodology and results.
    
    \item[Chapter 12] concludes with a discussion of limitations and future work.
\end{description}


% ============================================================================
% CHAPTER 2: RELATED WORK
% ============================================================================
\chapter{Related Work}
\label{ch:related-work}

\section{Retrieval-Augmented Generation}

\Gls{rag} has emerged as a fundamental paradigm for extending LLM capabilities beyond their training data \cite{lewis2020rag}. The core insight is that LLMs can effectively utilize external knowledge when provided in their context, enabling factual grounding without model retraining.

\subsection{Dense Retrieval}

Traditional information retrieval relied on sparse representations such as TF-IDF and BM25. The advent of dense retrieval, pioneered by systems like DPR (Dense Passage Retrieval) \cite{karpukhin2020dpr}, demonstrated that learned embeddings could significantly outperform lexical methods for semantic similarity tasks.

Modern embedding models, such as OpenAI's text-embedding-3-small and open-source alternatives like nomic-embed-text, map text to high-dimensional vectors where semantic similarity corresponds to geometric proximity. These embeddings form the foundation of our vector similarity search.

\subsection{Graph-Enhanced RAG}

Recent work has explored augmenting vector retrieval with knowledge graphs. Microsoft's GraphRAG \cite{edge2024graphrag} demonstrated that graph structures capturing entity relationships can significantly improve retrieval quality for complex queries requiring multi-hop reasoning.

Our system implements a similar approach, though optimized for code-specific relationships (imports, inheritance, function calls) rather than general knowledge graphs.

\subsection{Query Expansion Techniques}

Query expansion addresses the vocabulary mismatch problem---users often phrase queries differently from how information is stored. \Gls{hyde} \cite{gao2022hyde} introduced the technique of generating hypothetical answers and using their embeddings for retrieval, which we implement as a configurable option for conceptual queries.

\section{Code Intelligence}

\subsection{Abstract Syntax Tree Analysis}

\Gls{ast} parsing provides structural understanding of source code beyond surface-level text processing. Tree-sitter \cite{treesitter}, a parser generator tool, has become the de facto standard for multi-language parsing in developer tools, offering incremental parsing, error recovery, and language-agnostic APIs.

Our implementation uses tree-sitter's WebAssembly variant (web-tree-sitter) to support parsing without native dependencies, enabling broader deployment scenarios.

\subsection{Code Search and Retrieval}

Academic and industrial systems have explored various approaches to code search:

\begin{itemize}
    \item \textbf{Lexical}: Traditional search engines treating code as text
    \item \textbf{Semantic}: Embedding-based similarity (CodeBERT \cite{feng2020codebert}, CodeT5 \cite{wang2021codet5})
    \item \textbf{Structural}: Graph-based representations (code2vec \cite{alon2019code2vec})
\end{itemize}

ctx-sys combines these approaches, using lexical search for exact matches, semantic embeddings for conceptual queries, and graph traversal for dependency analysis.

\subsection{Code Summarization}

Automatic code summarization---generating natural language descriptions of code---enhances searchability and comprehension. We employ LLM-based summarization with specialized prompts for different symbol types (functions, classes, modules).

\section{Context Management for LLMs}

\subsection{Conversation Memory}

Managing conversation history in multi-turn dialogues is a recognized challenge. Approaches include:

\begin{itemize}
    \item \textbf{Truncation}: Removing old messages (loses information)
    \item \textbf{Summarization}: Compressing history into summaries \cite{xu2021conversation}
    \item \textbf{Retrieval}: Selectively retrieving relevant past messages
\end{itemize}

ctx-sys implements summarization for session archival and retrieval for historical context access.

\subsection{Memory-Augmented Agents}

Long-running AI agents require persistent memory beyond conversation windows. Systems like MemGPT \cite{packer2023memgpt} introduced explicit memory hierarchies. Our hot/cold memory tiering draws inspiration from this work while providing more explicit developer control.

\section{Hallucination Detection}

LLM hallucinations---generating plausible but incorrect information---are particularly problematic in code generation where errors have concrete consequences. Our draft-critique loop implements self-verification, where the model critiques its own outputs against retrieved context, similar to self-consistency approaches \cite{wang2022selfconsistency}.


% ============================================================================
% CHAPTER 3: SYSTEM ARCHITECTURE
% ============================================================================
\chapter{System Architecture}
\label{ch:architecture}

\section{Design Principles}

The ctx-sys architecture is guided by several core principles:

\subsection{Smart Librarian Paradigm}

Rather than attempting to include all potentially relevant information in context (the ``hoarder'' approach), ctx-sys acts as a ``smart librarian''---knowing where information exists and retrieving precisely what is needed on demand. This principle manifests in:

\begin{itemize}
    \item Comprehensive indexing of all information sources
    \item Query understanding to determine actual information needs
    \item Targeted retrieval within strict token budgets
    \item Source attribution for transparency
\end{itemize}

\subsection{Unified Entity Model}

All information types---code symbols, documentation sections, conversation messages, domain concepts---are represented as \textit{entities} with common attributes:

\begin{itemize}
    \item Unique identifiers and type classifications
    \item Textual content with optional summaries
    \item Vector embeddings for semantic search
    \item Typed relationships to other entities
    \item Rich metadata for filtering and ranking
\end{itemize}

This unification enables consistent retrieval across heterogeneous sources.

\subsection{Strategy Plurality}

No single retrieval strategy is optimal for all query types. The system supports multiple strategies:

\begin{itemize}
    \item \textbf{Vector/Semantic}: Best for conceptual, explanatory queries
    \item \textbf{Keyword/FTS}: Best for exact symbol or identifier lookup
    \item \textbf{Graph}: Best for dependency and relationship queries
\end{itemize}

Results are combined through configurable fusion algorithms.

\subsection{Local-First Architecture}

The system prioritizes local execution:

\begin{itemize}
    \item SQLite database (via sql.js WebAssembly) for storage
    \item Local embedding providers (Ollama) as primary option
    \item Cloud providers as configurable fallbacks
    \item Single-file portable database
\end{itemize}

This design ensures privacy, reduces latency, and enables offline operation.

\section{System Components}

Figure \ref{fig:architecture} illustrates the high-level system architecture.

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[
        node distance=1.5cm,
        box/.style={draw, rounded corners, minimum width=3cm, minimum height=0.8cm, align=center},
        layer/.style={draw, dashed, rounded corners, inner sep=0.3cm},
        arrow/.style={->, thick}
    ]
        % Integration Layer
        \node[box, fill=blue!20] (vscode) {VS Code Extension};
        \node[box, fill=blue!20, right=of vscode] (cli) {CLI Interface};
        \node[box, fill=blue!20, right=of cli] (ai) {AI Assistants};
        
        % MCP Server
        \node[box, fill=green!20, below=of cli] (mcp) {MCP Server};
        
        % Processing Layer
        \node[box, fill=orange!20, below left=2cm and 0cm of mcp] (ast) {AST Parser};
        \node[box, fill=orange!20, right=0.5cm of ast] (embed) {Embeddings};
        \node[box, fill=orange!20, right=0.5cm of embed] (summary) {Summarizer};
        \node[box, fill=orange!20, right=0.5cm of summary] (retrieval) {Retrieval};
        
        % Storage Layer
        \node[box, fill=yellow!20, below=2cm of embed] (sqlite) {SQLite Database};
        \node[box, fill=yellow!20, right=0.5cm of sqlite] (vectors) {Vector Store};
        \node[box, fill=yellow!20, right=0.5cm of vectors] (graph) {Graph Store};
        
        % Arrows
        \draw[arrow] (vscode) -- (mcp);
        \draw[arrow] (cli) -- (mcp);
        \draw[arrow] (ai) -- (mcp);
        
        \draw[arrow] (mcp) -- (ast);
        \draw[arrow] (mcp) -- (embed);
        \draw[arrow] (mcp) -- (summary);
        \draw[arrow] (mcp) -- (retrieval);
        
        \draw[arrow] (ast) -- (sqlite);
        \draw[arrow] (embed) -- (vectors);
        \draw[arrow] (summary) -- (sqlite);
        \draw[arrow] (retrieval) -- (vectors);
        \draw[arrow] (retrieval) -- (graph);
        
        % Layer labels
        \node[left=0.5cm of vscode, rotate=90, anchor=south] {\footnotesize Integration};
        \node[left=0.5cm of ast, rotate=90, anchor=south] {\footnotesize Processing};
        \node[left=0.5cm of sqlite, rotate=90, anchor=south] {\footnotesize Storage};
        
    \end{tikzpicture}
    \caption{High-level system architecture showing the three-layer design: integration, processing, and storage layers.}
    \label{fig:architecture}
\end{figure}

\subsection{Integration Layer}

The integration layer exposes ctx-sys functionality to external tools:

\begin{description}
    \item[MCP Server] The primary integration point, implementing the Model Context Protocol for compatibility with Claude, Copilot, and other MCP-aware assistants. Exposes tools for querying, indexing, and memory management.
    
    \item[CLI Interface] Command-line tools for initialization, indexing, search, and server management. Used for setup, debugging, and automation scripts.
    
    \item[VS Code Extension] (Planned - Phase 9) Native IDE integration with sidebar panel, hover information, and command palette access.
\end{description}

\subsection{Processing Layer}

The processing layer transforms raw data into searchable representations:

\begin{description}
    \item[AST Parser] Multi-language source code parsing using tree-sitter. Extracts symbols, imports, exports, and structural relationships.
    
    \item[Embedding Generator] Converts text content to dense vector representations. Supports multiple providers (Ollama, OpenAI) with automatic fallback.
    
    \item[Summarizer] Generates natural language descriptions of code symbols and document sections using LLM inference.
    
    \item[Retrieval Engine] Orchestrates multi-strategy search, fusion, and context assembly.
\end{description}

\subsection{Storage Layer}

The storage layer persists all indexed data:

\begin{description}
    \item[SQLite Database] Primary storage using sql.js (WebAssembly SQLite). Stores entities, metadata, sessions, and configuration.
    
    \item[Vector Store] Embedding vectors stored as JSON (due to sql.js limitations; production would use sqlite-vec or similar).
    
    \item[Graph Store] Entity relationships stored as edges with types and weights, supporting efficient traversal queries.
\end{description}

\section{Data Flow}

\subsection{Indexing Flow}

When indexing a codebase, data flows through the system as follows:

\begin{enumerate}
    \item \textbf{Discovery}: File system traversal identifies source files, filtered by language support and ignore patterns.
    
    \item \textbf{Parsing}: Each file is parsed by the AST parser to extract symbols (functions, classes, methods) with their signatures, bodies, and positions.
    
    \item \textbf{Relationship Extraction}: Import statements and inheritance declarations are analyzed to create relationship edges.
    
    \item \textbf{Summarization}: (Optional) Each symbol is summarized by an LLM to generate natural language descriptions.
    
    \item \textbf{Embedding}: Symbol content and summaries are embedded to dense vectors.
    
    \item \textbf{Storage}: Entities, vectors, and relationships are persisted to the database.
\end{enumerate}

\subsection{Query Flow}

When processing a context query:

\begin{enumerate}
    \item \textbf{Query Parsing}: The natural language query is parsed to extract intent, keywords, and entity mentions.
    
    \item \textbf{Retrieval Gating}: (Optional) A fast check determines if retrieval is necessary or if the query can be answered from general knowledge.
    
    \item \textbf{HyDE Expansion}: (Optional) For conceptual queries, a hypothetical answer is generated and embedded.
    
    \item \textbf{Multi-Strategy Search}: Parallel execution of vector similarity, keyword matching, and graph traversal.
    
    \item \textbf{Fusion}: Results from all strategies are combined using Reciprocal Rank Fusion.
    
    \item \textbf{Context Assembly}: Top results are formatted with source attribution, respecting token budgets.
    
    \item \textbf{Critique}: (Optional) The assembled context is verified against claimed facts.
\end{enumerate}


% ============================================================================
% CHAPTER 4: DATA MODEL AND STORAGE
% ============================================================================
\chapter{Data Model and Storage}
\label{ch:data-model}

\section{Entity Model}

The fundamental unit of storage in ctx-sys is the \textit{entity}. An entity represents any piece of information that may be relevant for context retrieval.

\subsection{Entity Types}

Entities are classified by type, enabling type-specific processing and filtering:

\begin{table}[htbp]
    \centering
    \caption{Entity type taxonomy}
    \label{tab:entity-types}
    \begin{tabular}{lll}
        \toprule
        \textbf{Category} & \textbf{Type} & \textbf{Description} \\
        \midrule
        \multirow{7}{*}{Code} 
            & \texttt{file} & Source code file \\
            & \texttt{module} & Module/namespace \\
            & \texttt{class} & Class definition \\
            & \texttt{function} & Function/procedure \\
            & \texttt{method} & Class method \\
            & \texttt{interface} & Interface/protocol \\
            & \texttt{type} & Type alias/definition \\
        \midrule
        \multirow{4}{*}{Documentation}
            & \texttt{document} & Document file \\
            & \texttt{section} & Document section \\
            & \texttt{requirement} & Extracted requirement \\
            & \texttt{user\_story} & User story \\
        \midrule
        \multirow{4}{*}{Conversation}
            & \texttt{session} & Conversation session \\
            & \texttt{message} & Individual message \\
            & \texttt{decision} & Extracted decision \\
            & \texttt{question} & Unanswered question \\
        \midrule
        \multirow{4}{*}{Domain}
            & \texttt{concept} & Domain concept \\
            & \texttt{person} & Person/stakeholder \\
            & \texttt{technology} & Technology reference \\
            & \texttt{pattern} & Design pattern \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Entity Schema}

Each entity contains the following fields:

\begin{lstlisting}[caption={Entity interface definition}]
interface Entity {
  id: string;              // Unique identifier (UUID)
  type: EntityType;        // Classification type
  name: string;            // Display name
  qualifiedName?: string;  // Fully qualified name (e.g., module.Class.method)
  content?: string;        // Full text content
  summary?: string;        // Generated summary
  metadata: Record<string, unknown>;  // Type-specific metadata
  filePath?: string;       // Source file path (for code entities)
  startLine?: number;      // Start line in source
  endLine?: number;        // End line in source
  hash?: string;           // Content hash for change detection
  createdAt: Date;
  updatedAt: Date;
}
\end{lstlisting}

The \texttt{metadata} field accommodates type-specific information:

\begin{itemize}
    \item \textbf{Functions}: Parameters, return type, async flag, exported flag
    \item \textbf{Classes}: Base classes, implemented interfaces, member counts
    \item \textbf{Documents}: Frontmatter, word count, heading structure
    \item \textbf{Messages}: Role (user/assistant), session reference
\end{itemize}

\section{Relationship Model}

Entities are connected through typed, weighted relationships enabling graph traversal.

\subsection{Relationship Types}

\begin{table}[htbp]
    \centering
    \caption{Relationship type taxonomy}
    \label{tab:relationship-types}
    \begin{tabular}{llll}
        \toprule
        \textbf{Type} & \textbf{Source} & \textbf{Target} & \textbf{Meaning} \\
        \midrule
        \texttt{contains} & Parent & Child & Structural containment \\
        \texttt{imports} & File & File/Module & Module dependency \\
        \texttt{extends} & Class & Class & Inheritance \\
        \texttt{implements} & Class & Interface & Interface implementation \\
        \texttt{calls} & Function & Function & Function call \\
        \texttt{defines} & File & Symbol & Symbol definition \\
        \texttt{references} & Any & Any & Generic reference \\
        \texttt{depends\_on} & Any & Any & Logical dependency \\
        \texttt{relates\_to} & Any & Any & Semantic similarity \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Relationship Schema}

\begin{lstlisting}[caption={Relationship interface definition}]
interface Relationship {
  id: string;
  sourceId: string;
  targetId: string;
  relationship: RelationshipType;
  weight: number;           // Edge weight (0-1)
  metadata?: Record<string, unknown>;
  createdAt: Date;
}
\end{lstlisting}

The \texttt{weight} field encodes relationship strength:
\begin{itemize}
    \item 1.0: Direct, definite relationship (explicit import, inheritance)
    \item 0.5-0.9: Inferred relationship (likely call, type usage)
    \item 0.1-0.5: Weak relationship (semantic similarity)
\end{itemize}

\section{Database Schema}

The SQLite database uses a hybrid of global and per-project tables.

\subsection{Global Tables}

\begin{lstlisting}[style=sql,caption={Global schema}]
-- Project registry
CREATE TABLE projects (
  id TEXT PRIMARY KEY,
  name TEXT UNIQUE NOT NULL,
  path TEXT NOT NULL,
  config JSON,
  created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
  updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
  last_indexed_at DATETIME,
  last_sync_commit TEXT
);

-- Embedding model registry
CREATE TABLE embedding_models (
  id TEXT PRIMARY KEY,
  name TEXT NOT NULL,
  provider TEXT NOT NULL,
  dimensions INTEGER NOT NULL,
  created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- Global configuration
CREATE TABLE config (
  key TEXT PRIMARY KEY,
  value JSON
);
\end{lstlisting}

\subsection{Per-Project Tables}

Each project has isolated tables with a sanitized project ID prefix:

\begin{lstlisting}[style=sql,caption={Per-project schema (abbreviated)}]
-- Entities
CREATE TABLE {prefix}_entities (
  id TEXT PRIMARY KEY,
  type TEXT NOT NULL,
  name TEXT NOT NULL,
  qualified_name TEXT,
  content TEXT,
  summary TEXT,
  metadata JSON,
  file_path TEXT,
  start_line INTEGER,
  end_line INTEGER,
  hash TEXT,
  created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
  updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- Vector embeddings (stored as JSON)
CREATE TABLE {prefix}_vectors (
  id TEXT PRIMARY KEY,
  entity_id TEXT NOT NULL,
  model_id TEXT NOT NULL,
  embedding JSON NOT NULL,
  created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
  FOREIGN KEY (entity_id) REFERENCES {prefix}_entities(id)
);

-- Graph relationships
CREATE TABLE {prefix}_relationships (
  id TEXT PRIMARY KEY,
  source_id TEXT NOT NULL,
  target_id TEXT NOT NULL,
  relationship TEXT NOT NULL,
  weight REAL DEFAULT 1.0,
  metadata JSON,
  created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
  FOREIGN KEY (source_id) REFERENCES {prefix}_entities(id),
  FOREIGN KEY (target_id) REFERENCES {prefix}_entities(id)
);

-- Conversation sessions
CREATE TABLE {prefix}_sessions (
  id TEXT PRIMARY KEY,
  name TEXT,
  status TEXT DEFAULT 'active',
  summary TEXT,
  message_count INTEGER DEFAULT 0,
  created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- Conversation messages
CREATE TABLE {prefix}_messages (
  id TEXT PRIMARY KEY,
  session_id TEXT NOT NULL,
  role TEXT NOT NULL,
  content TEXT NOT NULL,
  metadata JSON,
  created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
  FOREIGN KEY (session_id) REFERENCES {prefix}_sessions(id)
);
\end{lstlisting}

\subsection{Indexing Strategy}

Indexes are created for common query patterns:

\begin{lstlisting}[style=sql,caption={Database indexes}]
-- Entity lookups
CREATE INDEX idx_{prefix}_entities_type ON {prefix}_entities(type);
CREATE INDEX idx_{prefix}_entities_file ON {prefix}_entities(file_path);
CREATE INDEX idx_{prefix}_entities_name ON {prefix}_entities(name);
CREATE INDEX idx_{prefix}_entities_hash ON {prefix}_entities(hash);

-- Vector lookups
CREATE INDEX idx_{prefix}_vectors_entity ON {prefix}_vectors(entity_id);
CREATE UNIQUE INDEX idx_{prefix}_vectors_entity_model 
  ON {prefix}_vectors(entity_id, model_id);

-- Relationship lookups
CREATE INDEX idx_{prefix}_rel_source ON {prefix}_relationships(source_id);
CREATE INDEX idx_{prefix}_rel_target ON {prefix}_relationships(target_id);
CREATE INDEX idx_{prefix}_rel_type ON {prefix}_relationships(relationship);
\end{lstlisting}

\section{Vector Storage}

Vector embeddings enable semantic similarity search. The storage design balances efficiency with portability.

\subsection{Embedding Representation}

Embeddings are stored as JSON-encoded arrays:

\begin{lstlisting}[caption={Embedding storage}]
// Storage: JSON string in database
const embedding: number[] = [0.123, -0.456, 0.789, ...]; // 384-1536 dimensions
const stored = JSON.stringify(embedding);

// Retrieval and computation
const retrieved = JSON.parse(stored) as number[];
\end{lstlisting}

This approach sacrifices some performance for maximum portability---the sql.js WebAssembly build does not support native vector extensions like sqlite-vec. For production deployments, migration to native SQLite with sqlite-vec is recommended.

\subsection{Similarity Computation}

Cosine similarity is computed in-memory after retrieving candidate embeddings:

\begin{lstlisting}[caption={Cosine similarity computation}]
function cosineSimilarity(a: number[], b: number[]): number {
  let dotProduct = 0;
  let normA = 0;
  let normB = 0;
  
  for (let i = 0; i < a.length; i++) {
    dotProduct += a[i] * b[i];
    normA += a[i] * a[i];
    normB += b[i] * b[i];
  }
  
  const denominator = Math.sqrt(normA) * Math.sqrt(normB);
  return denominator === 0 ? 0 : dotProduct / denominator;
}
\end{lstlisting}

\begin{algorithm}
\caption{Vector similarity search}
\label{alg:vector-search}
\begin{algorithmic}[1]
\Require Query embedding $q$, limit $k$, threshold $\tau$
\Ensure Top-$k$ similar entities above threshold
\State $results \gets []$
\State $candidates \gets$ \Call{LoadAllEmbeddings}{}
\For{each $(entityId, embedding)$ in $candidates$}
    \State $sim \gets$ \Call{CosineSimilarity}{$q$, $embedding$}
    \If{$sim \geq \tau$}
        \State $results$.append($(entityId, sim)$)
    \EndIf
\EndFor
\State \textbf{sort} $results$ by similarity descending
\State \Return $results[0:k]$
\end{algorithmic}
\end{algorithm}


% ============================================================================
% CHAPTER 5: CODE INTELLIGENCE
% ============================================================================
\chapter{Code Intelligence}
\label{ch:code-intelligence}

\section{AST Parsing}

The code intelligence pipeline begins with Abstract Syntax Tree (AST) parsing, which provides structural understanding of source code.

\subsection{Tree-sitter Integration}

We use tree-sitter through its WebAssembly variant (web-tree-sitter) for several reasons:

\begin{itemize}
    \item \textbf{Multi-language support}: Single API for TypeScript, Python, Go, Rust, Java, C/C++
    \item \textbf{Error recovery}: Produces partial ASTs for syntactically invalid code
    \item \textbf{Incremental parsing}: Efficiently re-parses after edits
    \item \textbf{No native dependencies}: WebAssembly enables universal deployment
\end{itemize}

\begin{lstlisting}[caption={AST parser initialization and usage}]
import * as TreeSitter from 'web-tree-sitter';

export class ASTParser {
  private languages: Map<string, Language> = new Map();
  
  async initialize(): Promise<void> {
    await TreeSitter.Parser.init();
  }
  
  async parseContent(
    content: string,
    language: SupportedLanguage
  ): Promise<ParseResult> {
    const parser = new TreeSitter.Parser();
    const lang = await this.loadLanguage(language);
    parser.setLanguage(lang);
    
    const tree = parser.parse(content);
    const extractor = this.getExtractor(language);
    
    return {
      symbols: extractor.extractSymbols(tree.rootNode),
      imports: extractor.extractImports(tree.rootNode),
      exports: extractor.extractExports(tree.rootNode),
      errors: this.extractErrors(tree)
    };
  }
}
\end{lstlisting}

\subsection{Language-Specific Extractors}

Each supported language has a dedicated extractor implementing a common interface:

\begin{lstlisting}[caption={Language extractor interface}]
interface LanguageExtractor {
  extractSymbols(node: SyntaxNode, filePath?: string): Symbol[];
  extractImports(node: SyntaxNode): ImportStatement[];
  extractExports(node: SyntaxNode): string[];
}
\end{lstlisting}

\subsubsection{TypeScript Extractor}

The TypeScript extractor handles JavaScript/TypeScript-specific constructs:

\begin{lstlisting}[caption={TypeScript symbol extraction (simplified)}]
class TypeScriptExtractor implements LanguageExtractor {
  extractSymbols(node: SyntaxNode): Symbol[] {
    const symbols: Symbol[] = [];
    
    this.traverse(node, (child) => {
      switch (child.type) {
        case 'function_declaration':
        case 'arrow_function':
          symbols.push(this.extractFunction(child));
          break;
        case 'class_declaration':
          symbols.push(this.extractClass(child));
          break;
        case 'interface_declaration':
          symbols.push(this.extractInterface(child));
          break;
        case 'type_alias_declaration':
          symbols.push(this.extractTypeAlias(child));
          break;
      }
    });
    
    return symbols;
  }
}
\end{lstlisting}

\subsection{Symbol Representation}

Extracted symbols capture structural information:

\begin{lstlisting}[caption={Symbol interface}]
interface Symbol {
  type: SymbolType;
  name: string;
  qualifiedName: string;
  signature?: string;       // Function/method signature
  body?: string;            // Full body content
  docstring?: string;       // Associated documentation
  parameters?: Parameter[]; // For functions/methods
  returnType?: string;      // For functions/methods
  modifiers?: string[];     // public, private, static, async, etc.
  children?: Symbol[];      // Nested symbols (class members)
  startLine: number;
  endLine: number;
}
\end{lstlisting}

\section{Relationship Extraction}

Beyond individual symbols, the code intelligence pipeline extracts relationships between code elements.

\subsection{Import Analysis}

Import statements establish module dependencies:

\begin{lstlisting}[caption={Import relationship extraction}]
extractImports(node: SyntaxNode): ImportStatement[] {
  const imports: ImportStatement[] = [];
  
  for (const child of node.children) {
    if (child.type === 'import_statement') {
      const source = this.findChild(child, 'string')?.text;
      const specifiers = this.extractImportSpecifiers(child);
      
      imports.push({
        source: this.normalizeModulePath(source),
        specifiers,
        isTypeOnly: child.text.includes('import type'),
        isDynamic: child.type === 'call_expression'
      });
    }
  }
  
  return imports;
}
\end{lstlisting}

\subsection{Inheritance Relationships}

Class hierarchies are extracted from extends/implements clauses:

\begin{lstlisting}[caption={Inheritance extraction}]
private extractInheritance(classNode: SyntaxNode): {
  extends?: string;
  implements?: string[];
} {
  const result: { extends?: string; implements?: string[] } = {};
  
  const heritage = this.findChild(classNode, 'class_heritage');
  if (heritage) {
    const extendsClause = this.findChild(heritage, 'extends_clause');
    if (extendsClause) {
      result.extends = this.findChild(extendsClause, 'identifier')?.text;
    }
    
    const implementsClause = this.findChild(heritage, 'implements_clause');
    if (implementsClause) {
      result.implements = this.findChildren(implementsClause, 'type_identifier')
        .map(n => n.text);
    }
  }
  
  return result;
}
\end{lstlisting}

\subsection{Relationship Graph Construction}

The relationship extractor aggregates all relationships into a queryable graph:

\begin{lstlisting}[caption={Relationship extraction from parse results}]
class RelationshipExtractor {
  extractFromParseResult(parseResult: ParseResult): Relationship[] {
    const relationships: Relationship[] = [];
    
    // Import relationships
    for (const imp of parseResult.imports) {
      relationships.push({
        type: 'imports',
        source: parseResult.filePath,
        target: imp.source,
        metadata: { isExternal: this.isExternalModule(imp.source) }
      });
    }
    
    // Symbol definitions
    for (const symbol of parseResult.symbols) {
      relationships.push({
        type: 'defines',
        source: parseResult.filePath,
        target: symbol.qualifiedName
      });
      
      // Inheritance
      if (symbol.type === 'class' && symbol.metadata?.extends) {
        relationships.push({
          type: 'extends',
          source: symbol.qualifiedName,
          target: symbol.metadata.extends
        });
      }
    }
    
    return relationships;
  }
}
\end{lstlisting}

\section{Symbol Summarization}

Raw code content is often too verbose for efficient retrieval. LLM-powered summarization generates concise descriptions.

\subsection{Summarization Strategy}

Different symbol types receive tailored summarization prompts:

\begin{lstlisting}[caption={Summarization prompt generation}]
class SymbolSummarizer {
  private buildPrompt(symbol: Symbol): string {
    switch (symbol.type) {
      case 'function':
        return `Summarize this function in 1-2 sentences. 
What does it do, what inputs does it take, what does it return?

Function: ${symbol.name}
Signature: ${symbol.signature}
Body:
${symbol.body}`;

      case 'class':
        return `Summarize this class in 2-3 sentences.
What is its purpose? What are its main responsibilities?

Class: ${symbol.name}
${symbol.metadata?.extends ? `Extends: ${symbol.metadata.extends}` : ''}
Members: ${symbol.children?.map(c => c.name).join(', ')}`;

      default:
        return `Summarize this code element briefly:\n${symbol.body}`;
    }
  }
}
\end{lstlisting}

\subsection{Batch Processing}

For efficiency, symbols are summarized in batches with progress tracking:

\begin{lstlisting}[caption={Batch summarization with progress}]
async summarizeBatch(
  symbols: Symbol[],
  options?: { onProgress?: (done: number, total: number) => void }
): Promise<SymbolSummary[]> {
  const results: SymbolSummary[] = [];
  
  for (let i = 0; i < symbols.length; i++) {
    const summary = await this.summarize(symbols[i]);
    results.push(summary);
    options?.onProgress?.(i + 1, symbols.length);
  }
  
  return results;
}
\end{lstlisting}


% ============================================================================
% CHAPTER 6: EMBEDDING AND RETRIEVAL
% ============================================================================
\chapter{Embedding and Retrieval}
\label{ch:retrieval}

\section{Embedding Pipeline}

The embedding pipeline transforms textual content into dense vector representations suitable for semantic similarity search.

\subsection{Provider Architecture}

The system supports multiple embedding providers through a common interface:

\begin{lstlisting}[caption={Embedding provider interface}]
interface EmbeddingProvider {
  readonly name: string;
  readonly modelId: string;
  readonly dimensions: number;
  
  embed(text: string): Promise<number[]>;
  embedBatch(texts: string[], options?: BatchOptions): Promise<number[][]>;
}
\end{lstlisting}

\subsubsection{Ollama Provider}

The primary provider uses locally-hosted Ollama with the nomic-embed-text model:

\begin{lstlisting}[caption={Ollama embedding provider}]
class OllamaEmbeddingProvider implements EmbeddingProvider {
  readonly name = 'ollama';
  readonly modelId = 'nomic-embed-text';
  readonly dimensions = 768;
  
  async embed(text: string): Promise<number[]> {
    const response = await fetch(`${this.baseUrl}/api/embeddings`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: this.modelId,
        prompt: text
      })
    });
    
    const data = await response.json();
    return data.embedding;
  }
}
\end{lstlisting}

\subsubsection{OpenAI Provider}

Cloud fallback using OpenAI's text-embedding-3-small:

\begin{lstlisting}[caption={OpenAI embedding provider}]
class OpenAIEmbeddingProvider implements EmbeddingProvider {
  readonly name = 'openai';
  readonly modelId = 'text-embedding-3-small';
  readonly dimensions = 1536;
  
  async embedBatch(texts: string[]): Promise<number[][]> {
    const response = await this.client.embeddings.create({
      model: this.modelId,
      input: texts
    });
    
    return response.data.map(d => d.embedding);
  }
}
\end{lstlisting}

\subsection{Embedding Manager}

The EmbeddingManager coordinates embedding generation, storage, and retrieval:

\begin{lstlisting}[caption={Embedding manager}]
class EmbeddingManager {
  constructor(
    private db: DatabaseConnection,
    private projectId: string,
    private provider: EmbeddingProvider
  ) {}
  
  async embed(entityId: string, content: string): Promise<void> {
    const embedding = await this.provider.embed(content);
    this.store(entityId, embedding);
  }
  
  async findSimilar(
    query: string,
    options?: { limit?: number; threshold?: number }
  ): Promise<SimilarityResult[]> {
    const queryEmbedding = await this.provider.embed(query);
    return this.findSimilarByVector(queryEmbedding, options);
  }
  
  findSimilarByVector(
    embedding: number[],
    options?: { limit?: number; threshold?: number }
  ): SimilarityResult[] {
    const candidates = this.loadAllEmbeddings();
    
    const scored = candidates.map(c => ({
      entityId: c.entityId,
      similarity: this.cosineSimilarity(embedding, c.embedding)
    }));
    
    return scored
      .filter(s => s.similarity >= (options?.threshold ?? 0))
      .sort((a, b) => b.similarity - a.similarity)
      .slice(0, options?.limit ?? 10);
  }
}
\end{lstlisting}

\section{Query Parsing}

Before retrieval, queries are parsed to extract structured information that guides search strategy selection.

\subsection{Query Intent Detection}

Queries are classified by intent to optimize retrieval strategy:

\begin{table}[htbp]
    \centering
    \caption{Query intent classification}
    \label{tab:query-intents}
    \begin{tabular}{lll}
        \toprule
        \textbf{Intent} & \textbf{Example Query} & \textbf{Preferred Strategy} \\
        \midrule
        \texttt{find} & ``where is the auth controller?'' & Keyword \\
        \texttt{explain} & ``how does authentication work?'' & Semantic + Graph \\
        \texttt{list} & ``show all API endpoints'' & Keyword + Type filter \\
        \texttt{compare} & ``difference between v1 and v2 API'' & Semantic \\
        \texttt{how} & ``how to implement caching?'' & Semantic + HyDE \\
        \texttt{why} & ``why is this function async?'' & Graph (context) \\
        \texttt{debug} & ``error in login function'' & Keyword + Graph \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{lstlisting}[caption={Intent detection patterns}]
private buildIntentPatterns(): IntentPattern[] {
  return [
    { pattern: /^(find|search|locate|where\s+is)/i, 
      intent: 'find', weight: 0.9 },
    { pattern: /^(explain|what\s+is|what\s+does)/i, 
      intent: 'explain', weight: 0.9 },
    { pattern: /\bhow\s+does\b.*\bwork\b/i, 
      intent: 'explain', weight: 0.85 },
    { pattern: /^how\s+(do|can|to)/i, 
      intent: 'how', weight: 0.9 },
    { pattern: /\b(error|bug|issue|problem)/i, 
      intent: 'debug', weight: 0.85 },
    // ... additional patterns
  ];
}
\end{lstlisting}

\subsection{Entity Mention Extraction}

Code entity mentions (marked with backticks or following naming conventions) are extracted for targeted lookup:

\begin{lstlisting}[caption={Entity mention extraction}]
extractEntityMentions(query: string): EntityMention[] {
  const mentions: EntityMention[] = [];
  
  // Backtick-quoted identifiers
  const backtickPattern = /`([^`]+)`/g;
  let match;
  while ((match = backtickPattern.exec(query)) !== null) {
    mentions.push({
      text: match[1],
      type: this.classifyMention(match[1]),
      start: match.index,
      end: match.index + match[0].length
    });
  }
  
  // CamelCase/PascalCase identifiers
  const identifierPattern = /\b([A-Z][a-zA-Z0-9]*(?:\.[A-Z][a-zA-Z0-9]*)*)\b/g;
  // ... extraction logic
  
  return mentions;
}

private classifyMention(text: string): MentionType {
  if (text.includes('.') || text.includes('/')) return 'file';
  if (text.match(/^[A-Z]/)) return 'class';
  if (text.includes('(')) return 'function';
  return 'unknown';
}
\end{lstlisting}

\section{Multi-Strategy Search}

The multi-strategy search system combines multiple retrieval approaches for comprehensive coverage.

\subsection{Search Strategies}

Three primary strategies are implemented:

\subsubsection{Keyword Search}

Pattern-based matching against entity names and content:

\begin{lstlisting}[caption={Keyword search implementation}]
private async keywordSearch(
  parsed: ParsedQuery,
  options: SearchOptions
): Promise<RawResult[]> {
  const results: RawResult[] = [];
  
  // Search with keywords
  if (parsed.keywords.length > 0) {
    const searchQuery = parsed.keywords.join(' ');
    const ftsResults = await this.entityStore.search(searchQuery, {
      type: options.entityTypes?.[0],
      limit: options.limit * 2
    });
    
    // Convert to ranked results
    for (let i = 0; i < ftsResults.length; i++) {
      results.push({
        entityId: ftsResults[i].id,
        score: 1 / (i + 1),  // Rank-based scoring
        source: 'keyword'
      });
    }
  }
  
  return results;
}
\end{lstlisting}

\subsubsection{Semantic Search}

Vector similarity search using embeddings:

\begin{lstlisting}[caption={Semantic search implementation}]
private async semanticSearch(
  parsed: ParsedQuery,
  options: SearchOptions
): Promise<RawResult[]> {
  const similar = await this.embeddingManager.findSimilar(
    parsed.normalizedQuery,
    {
      limit: options.limit * 2,
      entityTypes: options.entityTypes
    }
  );
  
  return similar.map(s => ({
    entityId: s.entityId,
    score: s.similarity,
    source: 'semantic'
  }));
}
\end{lstlisting}

\subsubsection{Graph Search}

Traversal from mentioned entities to related nodes:

\begin{lstlisting}[caption={Graph search implementation}]
private async graphSearch(
  parsed: ParsedQuery,
  options: SearchOptions
): Promise<RawResult[]> {
  const results: RawResult[] = [];
  
  // Start from mentioned entities
  for (const mention of parsed.entityMentions) {
    const entity = await this.findEntityByMention(mention);
    if (!entity) continue;
    
    // Get neighborhood
    const subgraph = await this.graphTraversal.getNeighborhood(entity.id, {
      maxDepth: options.graphDepth ?? 2
    });
    
    // Score by distance from start
    for (const neighbor of subgraph.entities) {
      const distance = this.calculateDistance(entity.id, neighbor.id, subgraph);
      results.push({
        entityId: neighbor.id,
        score: 1 / (distance + 1),
        source: 'graph'
      });
    }
  }
  
  return results;
}
\end{lstlisting}

\subsection{Reciprocal Rank Fusion}

Results from multiple strategies are combined using Reciprocal Rank Fusion (RRF):

\begin{definition}[Reciprocal Rank Fusion]
Given $n$ ranked lists $L_1, \ldots, L_n$ and a constant $k$ (typically 60), the RRF score for document $d$ is:
\[
\text{RRF}(d) = \sum_{i=1}^{n} \frac{w_i}{k + \text{rank}_i(d)}
\]
where $w_i$ is the weight for list $i$ and $\text{rank}_i(d)$ is the rank of $d$ in list $i$ (or $\infty$ if absent).
\end{definition}

\begin{lstlisting}[caption={RRF implementation}]
private reciprocalRankFusion(
  results: RawResult[],
  weights: StrategyWeights
): FusedResult[] {
  const K = 60;
  const scoreMap = new Map<string, number>();
  
  // Group results by source and rank
  const bySource = new Map<SearchStrategy, RawResult[]>();
  for (const r of results) {
    const list = bySource.get(r.source) ?? [];
    list.push(r);
    bySource.set(r.source, list);
  }
  
  // Sort each list by score and apply RRF
  for (const [source, list] of bySource) {
    const weight = weights[source] ?? 1.0;
    const sorted = list.sort((a, b) => b.score - a.score);
    
    for (let rank = 0; rank < sorted.length; rank++) {
      const entityId = sorted[rank].entityId;
      const rrfScore = weight / (K + rank + 1);
      scoreMap.set(entityId, (scoreMap.get(entityId) ?? 0) + rrfScore);
    }
  }
  
  return Array.from(scoreMap.entries())
    .map(([entityId, score]) => ({ entityId, score }))
    .sort((a, b) => b.score - a.score);
}
\end{lstlisting}

\begin{algorithm}
\caption{Multi-strategy search with RRF fusion}
\label{alg:multi-search}
\begin{algorithmic}[1]
\Require Query $q$, strategies $S$, weights $W$, limit $k$
\Ensure Fused top-$k$ results
\State $parsed \gets$ \Call{ParseQuery}{$q$}
\State $allResults \gets []$
\For{each strategy $s$ in $S$}
    \State $results_s \gets$ \Call{ExecuteStrategy}{$s$, $parsed$}
    \State $allResults$.extend($results_s$)
\EndFor
\State $fused \gets$ \Call{ReciprocalRankFusion}{$allResults$, $W$}
\State $deduped \gets$ \Call{Deduplicate}{$fused$}
\State \Return $deduped[0:k]$
\end{algorithmic}
\end{algorithm}

\section{Context Assembly}

Retrieved results must be formatted for LLM consumption within token budgets.

\subsection{Token Estimation}

Token counts are estimated using character-based approximation:

\begin{lstlisting}[caption={Token estimation}]
function estimateTokens(text: string): number {
  // Approximation: ~4 characters per token
  return Math.ceil(text.length / 4);
}
\end{lstlisting}

\subsection{Context Formatting}

Results are formatted with source attribution:

\begin{lstlisting}[caption={Context assembly}]
class ContextAssembler {
  assemble(results: SearchResult[], options: AssemblyOptions): AssembledContext {
    const maxTokens = options.maxTokens ?? 4000;
    const sections: string[] = [];
    const sources: ContextSource[] = [];
    let tokenCount = 0;
    
    // Sort by relevance
    const sorted = results.sort((a, b) => b.score - a.score);
    
    for (const result of sorted) {
      const formatted = this.formatEntity(result.entity, options);
      const tokens = estimateTokens(formatted);
      
      if (tokenCount + tokens > maxTokens) {
        break;  // Token budget exhausted
      }
      
      sections.push(formatted);
      tokenCount += tokens;
      sources.push({
        entityId: result.entity.id,
        name: result.entity.name,
        type: result.entity.type,
        relevance: result.score
      });
    }
    
    return {
      context: sections.join('\n\n---\n\n'),
      sources,
      tokenCount,
      truncated: sorted.length > sources.length
    };
  }
  
  private formatEntity(entity: Entity, options: AssemblyOptions): string {
    if (options.format === 'markdown') {
      return `### ${entity.name} (${entity.type})
${entity.filePath ? `*File: ${entity.filePath}*` : ''}

${entity.summary ?? entity.content}`;
    }
    // ... other formats
  }
}
\end{lstlisting}


% ============================================================================
% CHAPTER 7: ADVANCED RETRIEVAL TECHNIQUES
% ============================================================================
\chapter{Advanced Retrieval Techniques}
\label{ch:advanced-retrieval}

\section{HyDE Query Expansion}

\Gls{hyde} addresses the \textit{vocabulary mismatch} problem---users often phrase queries differently from how information is stored.

\subsection{Motivation}

Consider the query ``how does the system handle user logins?'' The codebase might contain a function \texttt{authenticateCredentials} with no occurrence of ``login.'' Direct embedding similarity may fail to connect these semantically related terms.

HyDE generates a \textit{hypothetical answer} to the query, then uses its embedding for retrieval. The hypothetical, being generated by an LLM with broad language understanding, bridges vocabulary gaps.

\subsection{Implementation}

\begin{lstlisting}[caption={HyDE query expander}]
class HyDEQueryExpander {
  async expandQuery(context: HyDEQueryContext): Promise<HyDEResult> {
    const startTime = Date.now();
    
    // Always compute direct embedding as fallback
    const directEmbedding = await this.embeddingManager.embedText(context.query);
    
    // Check if HyDE should be used
    if (!this.shouldUseHyDE(context.query)) {
      return {
        originalQuery: context.query,
        hypotheticalAnswer: '',
        hypotheticalEmbedding: directEmbedding,
        directEmbedding,
        usedHyDE: false,
        generationTimeMs: Date.now() - startTime
      };
    }
    
    // Generate hypothetical answer
    const hypothetical = await this.hypotheticalProvider.generate(
      context.query,
      { entityTypes: context.entityTypes }
    );
    
    // Embed the hypothetical
    const hypotheticalEmbedding = await this.embeddingManager.embedText(hypothetical);
    
    return {
      originalQuery: context.query,
      hypotheticalAnswer: hypothetical,
      hypotheticalEmbedding,
      directEmbedding,
      usedHyDE: true,
      generationTimeMs: Date.now() - startTime
    };
  }
  
  shouldUseHyDE(query: string): boolean {
    const parsed = this.queryParser.parse(query);
    
    // Skip for short queries
    if (query.length < this.config.minQueryLength) return false;
    
    // Skip for specific entity mentions
    if (parsed.entityMentions.some(m => 
      m.type === 'file' || m.type === 'function')) return false;
    
    // Use for conceptual intents
    return this.config.hydeIntents.includes(parsed.intent);
  }
}
\end{lstlisting}

\subsection{Prompt Engineering}

The hypothetical generation prompt is crucial for quality:

\begin{lstlisting}[caption={Hypothetical generation prompt}]
function buildHypotheticalPrompt(query: string, entityTypes?: string[]): string {
  const typeHint = entityTypes?.length
    ? `Focus on ${entityTypes.join(', ')} entities.`
    : '';
    
  return `Given this question about a codebase, write a short hypothetical 
answer (2-3 sentences) that would be found in the actual code or documentation.
Do not make up specific function names unless they're in the question.
${typeHint}

Question: ${query}

Hypothetical answer:`;
}
\end{lstlisting}

\subsection{Selective Application}

HyDE adds latency (LLM generation + additional embedding), so it is selectively applied:

\begin{itemize}
    \item \textbf{Applied}: Conceptual queries (``how does X work?''), explanation requests, why questions
    \item \textbf{Skipped}: Specific lookups (``find `UserController`''), short queries, debugging with exact error messages
\end{itemize}

\section{Retrieval Gating}

Retrieval gating determines whether context retrieval is necessary before executing expensive search operations.

\subsection{Motivation}

Not all queries benefit from retrieved context:

\begin{itemize}
    \item \textbf{General knowledge}: ``What is a promise in JavaScript?''---answerable from training data
    \item \textbf{Simple tasks}: ``Write a function to reverse a string''---no project context needed
    \item \textbf{Clarification requests}: ``Can you explain that more?''---needs conversation history, not code search
\end{itemize}

Unnecessary retrieval wastes compute and may introduce noise into the context.

\subsection{Decision Process}

The gate uses a two-phase decision process:

\begin{enumerate}
    \item \textbf{Fast path}: Pattern-based rules for clear cases (< 1ms)
    \item \textbf{Slow path}: Model-based classification for ambiguous cases (100-500ms)
\end{enumerate}

\begin{lstlisting}[caption={Retrieval gate decision logic}]
class RetrievalGate {
  async shouldRetrieve(context: GateContext): Promise<GateDecision> {
    // Check cache
    const cached = this.getCached(context.query);
    if (cached) return cached;
    
    // Fast path: pattern-based
    const fastDecision = this.fastPathDecision(context.query);
    if (fastDecision) return fastDecision;
    
    // Slow path: model-based
    if (this.modelProvider) {
      return this.modelBasedDecision(context);
    }
    
    // Default: retrieve
    return { shouldRetrieve: true, confidence: 0.5, reason: 'default' };
  }
  
  private fastPathDecision(query: string): GateDecision | null {
    const parsed = this.queryParser.parse(query);
    
    // Always retrieve for code entity mentions
    if (parsed.entityMentions.some(m => 
      ['file', 'function', 'class'].includes(m.type))) {
      return {
        shouldRetrieve: true,
        confidence: 0.95,
        reason: 'Query mentions specific code entities',
        suggestedStrategy: 'keyword'
      };
    }
    
    // Skip for general programming questions
    if (this.isGeneralProgrammingQuestion(query)) {
      return {
        shouldRetrieve: false,
        confidence: 0.8,
        reason: 'General programming question'
      };
    }
    
    return null;  // Defer to slow path
  }
}
\end{lstlisting}

\subsection{Gating Patterns}

Table \ref{tab:gating-patterns} summarizes the fast-path patterns:

\begin{table}[htbp]
    \centering
    \caption{Retrieval gating patterns}
    \label{tab:gating-patterns}
    \begin{tabular}{lll}
        \toprule
        \textbf{Pattern} & \textbf{Decision} & \textbf{Confidence} \\
        \midrule
        Backtick entity mention & Retrieve & 0.95 \\
        ``this project/codebase'' & Retrieve & 0.9 \\
        ``our implementation'' & Retrieve & 0.85 \\
        General JS/Python question & Skip & 0.8 \\
        ``what is a [concept]'' & Skip & 0.75 \\
        Error with stack trace & Retrieve & 0.9 \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Draft-Critique Loop}

The draft-critique loop verifies LLM responses against retrieved context to detect hallucinations.

\subsection{Hallucination Problem}

LLMs may generate confident but incorrect claims about codebases:

\begin{itemize}
    \item Inventing function names or parameters
    \item Misattributing behavior to wrong modules
    \item Describing deprecated or non-existent features
    \item Conflating similar but distinct concepts
\end{itemize}

\subsection{Critique Process}

The critique loop operates iteratively:

\begin{enumerate}
    \item Generate initial draft response
    \item Extract factual claims from draft
    \item Verify each claim against retrieved context
    \item Flag unsupported or contradicted claims
    \item Optionally revise and re-critique
\end{enumerate}

\begin{lstlisting}[caption={Draft critique implementation}]
class DraftCritique {
  async critique(options: CritiqueOptions): Promise<DraftCritiqueOutput> {
    const iterations: CritiqueIteration[] = [];
    let currentDraft = options.draft;
    
    for (let i = 0; i < this.config.maxIterations; i++) {
      // Run critique
      const result = await this.runCritique(
        currentDraft,
        options.query,
        options.context
      );
      
      // Extract claims
      const claims = this.extractClaims(currentDraft, options.context);
      
      iterations.push({ iteration: i, draft: currentDraft, result, claims });
      
      // Check if passed
      if (result.passed) break;
      
      // Optionally revise
      if (options.revisionCallback) {
        currentDraft = await options.revisionCallback(currentDraft, result);
      } else {
        break;
      }
    }
    
    return this.buildOutput(iterations);
  }
  
  private extractClaims(
    draft: string, 
    context: AssembledContext
  ): ExtractedClaim[] {
    const claims: ExtractedClaim[] = [];
    const sentences = this.splitSentences(draft);
    
    for (const sentence of sentences) {
      const claimType = this.classifyClaimType(sentence);
      if (claimType === 'opinion') continue;
      
      const supported = this.verifyAgainstContext(sentence, context);
      claims.push({
        claim: sentence,
        type: claimType,
        supported,
        source: supported ? this.findSupportingSource(sentence, context) : undefined
      });
    }
    
    return claims;
  }
}
\end{lstlisting}

\subsection{Issue Classification}

Detected issues are classified by severity:

\begin{table}[htbp]
    \centering
    \caption{Critique issue types and severities}
    \label{tab:critique-issues}
    \begin{tabular}{llp{6cm}}
        \toprule
        \textbf{Type} & \textbf{Severity} & \textbf{Description} \\
        \midrule
        \texttt{hallucination} & High & Claim contradicts or has no basis in context \\
        \texttt{unsupported} & Medium & Claim cannot be verified from provided context \\
        \texttt{incomplete} & Low & Relevant information exists but was not included \\
        \texttt{outdated} & Medium & Information may be stale based on timestamps \\
        \bottomrule
    \end{tabular}
\end{table}


% ============================================================================
% CHAPTER 8: CONVERSATION MANAGEMENT
% ============================================================================
\chapter{Conversation Management}
\label{ch:conversation}

\section{Session Management}

Conversations are organized into sessions that track context and state across multiple messages.

\subsection{Session Lifecycle}

Sessions progress through defined states:

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[
        node distance=2.5cm,
        state/.style={draw, rounded corners, minimum width=2cm, minimum height=0.8cm},
        arrow/.style={->, thick}
    ]
        \node[state, fill=green!20] (active) {Active};
        \node[state, fill=yellow!20, right=of active] (archived) {Archived};
        \node[state, fill=blue!20, right=of archived] (summarized) {Summarized};
        
        \draw[arrow] (active) -- node[above] {archive()} (archived);
        \draw[arrow] (archived) -- node[above] {summarize()} (summarized);
        \draw[arrow, bend left=30] (active) to node[below] {summarize()} (summarized);
    \end{tikzpicture}
    \caption{Session state transitions}
    \label{fig:session-states}
\end{figure}

\begin{lstlisting}[caption={Session manager implementation}]
class SessionManager {
  create(name?: string): Session {
    const id = generateId();
    this.db.run(
      `INSERT INTO ${this.sessionsTable} (id, name, status, message_count)
       VALUES (?, ?, 'active', 0)`,
      [id, name ?? null]
    );
    return this.get(id)!;
  }
  
  getCurrent(): Session {
    // Return existing current session
    if (this.currentSessionId) {
      const session = this.get(this.currentSessionId);
      if (session?.status === 'active') return session;
    }
    
    // Find most recent active session
    const recent = this.db.get<SessionRow>(
      `SELECT * FROM ${this.sessionsTable}
       WHERE status = 'active'
       ORDER BY updated_at DESC LIMIT 1`
    );
    
    if (recent) {
      this.currentSessionId = recent.id;
      return this.rowToSession(recent);
    }
    
    // Create new session
    return this.create();
  }
  
  archive(id: string): Session {
    return this.update(id, { status: 'archived' });
  }
  
  markSummarized(id: string, summary: string): Session {
    return this.update(id, { status: 'summarized', summary });
  }
}
\end{lstlisting}

\subsection{Message Storage}

Messages are stored with role classification and metadata:

\begin{lstlisting}[caption={Message storage}]
interface Message {
  id: string;
  sessionId: string;
  role: 'user' | 'assistant' | 'system';
  content: string;
  metadata?: {
    model?: string;
    tokenCount?: number;
    responseTimeMs?: number;
    toolCalls?: string[];
  };
  createdAt: Date;
}

class MessageStore {
  add(
    sessionId: string,
    role: Message['role'],
    content: string,
    metadata?: Record<string, unknown>
  ): Message {
    const id = generateId();
    
    this.db.run(
      `INSERT INTO ${this.messagesTable}
       (id, session_id, role, content, metadata)
       VALUES (?, ?, ?, ?, ?)`,
      [id, sessionId, role, content, JSON.stringify(metadata ?? {})]
    );
    
    // Update session message count
    this.db.run(
      `UPDATE ${this.sessionsTable}
       SET message_count = message_count + 1, updated_at = CURRENT_TIMESTAMP
       WHERE id = ?`,
      [sessionId]
    );
    
    return this.get(id)!;
  }
}
\end{lstlisting}

\section{Conversation Summarization}

Long conversations are summarized to preserve essential information while reducing token consumption.

\subsection{Summarization Strategy}

The summarizer produces structured summaries capturing:

\begin{itemize}
    \item Main topics discussed
    \item Key decisions made
    \item Unresolved questions
    \item Important context for future reference
\end{itemize}

\begin{lstlisting}[caption={Session summarizer}]
class SessionSummarizer {
  async summarize(sessionId: string): Promise<string> {
    const messages = this.messageStore.getBySession(sessionId);
    
    if (messages.length === 0) {
      return 'Empty session with no messages.';
    }
    
    const prompt = this.buildSummarizationPrompt(messages);
    const summary = await this.provider.summarize(prompt);
    
    // Update session with summary
    this.sessionManager.markSummarized(sessionId, summary);
    
    return summary;
  }
  
  private buildSummarizationPrompt(messages: Message[]): string {
    const transcript = messages
      .map(m => `${m.role.toUpperCase()}: ${m.content}`)
      .join('\n\n');
    
    return `Summarize this conversation, capturing:
1. Main topics discussed
2. Key decisions made
3. Unresolved questions
4. Important context for future reference

CONVERSATION:
${transcript}

SUMMARY:`;
  }
}
\end{lstlisting}

\section{Decision Extraction}

The system automatically extracts decisions from conversations for future reference.

\subsection{Decision Detection}

Pattern matching identifies potential decision statements:

\begin{lstlisting}[caption={Decision detection patterns}]
const DECISION_PATTERNS = [
  /we('ll| will| should| decided| agreed| chose)/i,
  /let's (go with|use|implement|do|try)/i,
  /the (decision|plan|approach|strategy) is/i,
  /i('ll| will) (use|implement|go with|choose)/i,
  /decided to/i,
  /agreed (on|to|that)/i,
  /going (to|with)/i,
  /settled on/i
];

class DecisionExtractor {
  mightContainDecision(content: string): boolean {
    return DECISION_PATTERNS.some(p => p.test(content));
  }
}
\end{lstlisting}

\subsection{Structured Extraction}

Potential decisions are processed by an LLM to extract structured information:

\begin{lstlisting}[caption={Decision extraction}]
interface Decision {
  id: string;
  sessionId: string;
  messageId: string;
  description: string;
  context?: string;
  alternatives?: string[];
  relatedEntities: string[];
  createdAt: Date;
}

class DecisionExtractor {
  async extractFromMessage(message: Message): Promise<Decision[]> {
    if (!this.mightContainDecision(message.content)) {
      return [];
    }
    
    const prompt = `Extract any decisions from this message.
For each decision, provide:
- DECISION: (the decision itself)
- CONTEXT: (why it was made)
- ALTERNATIVES: (other options considered)

If no decisions, respond with "NO_DECISIONS".

MESSAGE:
${message.content}`;
    
    const response = await this.provider.summarize(prompt);
    return this.parseDecisions(response, message);
  }
}
\end{lstlisting}


% ============================================================================
% CHAPTER 9: AGENT PATTERNS
% ============================================================================
\chapter{Agent Patterns}
\label{ch:agent-patterns}

This chapter describes patterns for supporting long-running AI agent workflows that exceed single conversation boundaries.

\section{Checkpointing}

Checkpointing enables saving and restoring agent execution state for failure recovery and task resumption.

\subsection{Checkpoint Data Model}

\begin{lstlisting}[caption={Agent state and checkpoint interfaces}]
interface AgentState {
  query: string;           // Original task
  plan: PlanStep[];        // Execution plan
  currentStepIndex: number;
  results: StepResult[];   // Completed step results
  context: Record<string, unknown>;  // Working context
  lastError?: {
    stepIndex: number;
    message: string;
    timestamp: Date;
  };
}

interface Checkpoint {
  id: string;
  sessionId: string;
  projectId: string;
  stepNumber: number;
  createdAt: Date;
  state: AgentState;
  metadata: {
    description?: string;
    triggerType: 'auto' | 'manual' | 'error';
    durationMs: number;
    tokenUsage?: number;
  };
}
\end{lstlisting}

\subsection{Checkpoint Management}

\begin{lstlisting}[caption={Checkpoint manager}]
class CheckpointManager {
  async save(
    sessionId: string,
    state: AgentState,
    options: SaveOptions = {}
  ): Promise<Checkpoint> {
    const checkpoint: Checkpoint = {
      id: generateId('ckpt'),
      sessionId,
      projectId: this.projectId,
      stepNumber: state.currentStepIndex,
      createdAt: new Date(),
      state,
      metadata: {
        triggerType: options.triggerType ?? 'auto',
        durationMs: options.durationMs ?? 0
      }
    };
    
    this.db.run(
      `INSERT INTO ${this.prefix}_checkpoints (...)
       VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)`,
      [/* checkpoint fields */]
    );
    
    // Prune old checkpoints
    await this.pruneOldCheckpoints(sessionId);
    
    return checkpoint;
  }
  
  async loadLatest(sessionId: string): Promise<Checkpoint | null> {
    const row = this.db.get<CheckpointRow>(
      `SELECT * FROM ${this.prefix}_checkpoints
       WHERE session_id = ?
       ORDER BY step_number DESC, created_at DESC
       LIMIT 1`,
      [sessionId]
    );
    return row ? this.rowToCheckpoint(row) : null;
  }
  
  async resume(sessionId: string): Promise<AgentState | null> {
    const checkpoint = await this.loadLatest(sessionId);
    return checkpoint?.state ?? null;
  }
}
\end{lstlisting}

\subsection{Automatic Checkpointing}

Checkpoints are created automatically at key points:

\begin{itemize}
    \item After each completed step
    \item Before potentially risky operations
    \item On error (for debugging)
    \item When explicitly requested
\end{itemize}

\section{Hot/Cold Memory Tiering}

Memory tiering provides explicit control over what information remains in active context versus cold storage.

\subsection{Memory Tiers}

\begin{description}
    \item[Hot] Immediately available in context; highest token cost
    \item[Warm] Recently accessed; quickly recallable
    \item[Cold] Archived; requires explicit recall with embedding search
\end{description}

\begin{lstlisting}[caption={Memory tier configuration}]
interface MemoryConfig {
  hotTokenLimit: number;      // Max tokens in hot memory (default: 4000)
  warmAccessThreshold: number; // Accesses before promote to hot (default: 3)
  promoteThreshold: number;    // Relevance score to auto-promote (default: 0.85)
  maxColdItems: number;        // Maximum cold storage items (default: 1000)
  autoSpillEnabled: boolean;   // Auto-spill when hot is full
  autoPromoteEnabled: boolean; // Auto-promote frequently accessed
}
\end{lstlisting}

\subsection{Memory Operations}

\begin{lstlisting}[caption={Memory tier manager}]
class MemoryTierManager {
  async addToHot(
    sessionId: string,
    content: string,
    type: MemoryItemType,
    options?: AddMemoryOptions
  ): Promise<MemoryItem> {
    const tokenCount = estimateTokens(content);
    
    // Auto-spill if hot would exceed limit
    if (this.config.autoSpillEnabled) {
      const status = await this.getStatus(sessionId);
      if (status.hot.tokens + tokenCount > this.config.hotTokenLimit) {
        await this.spillToWarm(sessionId);
      }
    }
    
    // Create and store item
    const item = this.createItem(sessionId, content, type, 'hot', options);
    await this.storeItem(item);
    
    return item;
  }
  
  async spillToWarm(
    sessionId: string,
    options?: SpillOptions
  ): Promise<SpillResult> {
    // Get hot items sorted by relevance (lowest first)
    const hotItems = await this.getHot(sessionId);
    const sorted = hotItems.sort((a, b) => a.relevanceScore - b.relevanceScore);
    
    // Determine items to spill
    const toSpill = options?.itemIds
      ? sorted.filter(i => options.itemIds!.includes(i.id))
      : sorted.slice(0, options?.count ?? Math.ceil(sorted.length / 2));
    
    // Move to warm tier
    for (const item of toSpill) {
      await this.updateTier(item.id, 'warm');
    }
    
    return {
      spilledCount: toSpill.length,
      spilledIds: toSpill.map(i => i.id),
      targetTier: 'warm'
    };
  }
  
  async recall(
    sessionId: string,
    query: string,
    options?: RecallOptions
  ): Promise<RecallResult> {
    // Embed query
    const queryEmbedding = await this.embeddingProvider?.embed(query);
    
    // Search cold/warm items
    const candidates = await this.searchColdWarm(sessionId, queryEmbedding, options);
    
    // Optionally promote high-relevance items
    const promoted: string[] = [];
    if (options?.autoPromote ?? this.config.autoPromoteEnabled) {
      for (const item of candidates) {
        if (item.relevanceScore >= this.config.promoteThreshold) {
          await this.promoteToHot(item.id);
          promoted.push(item.id);
        }
      }
    }
    
    return { items: candidates, promoted };
  }
}
\end{lstlisting}

\subsection{Access Pattern Tracking}

The system tracks access patterns to inform tier management:

\begin{lstlisting}[caption={Access tracking}]
async recordAccess(itemId: string): Promise<void> {
  this.db.run(
    `UPDATE ${this.prefix}_memory_items
     SET access_count = access_count + 1,
         last_accessed_at = CURRENT_TIMESTAMP
     WHERE id = ?`,
    [itemId]
  );
  
  // Check for auto-promotion
  if (this.config.autoPromoteEnabled) {
    const item = await this.getItem(itemId);
    if (item && 
        item.tier !== 'hot' && 
        item.accessCount >= this.config.warmAccessThreshold) {
      await this.promoteToHot(itemId);
    }
  }
}
\end{lstlisting}

\section{Reflection Storage}

The reflection storage system enables agents to learn from past experiences across sessions. This is particularly valuable for long-running agent tasks where similar problems may recur.

\subsection{Reflection Model}

Reflections capture lessons learned with structured outcome tracking:

\begin{lstlisting}[caption={Reflection interface}]
interface Reflection {
  id: string;
  sessionId: string;
  content: string;           // The lesson learned
  outcome: ReflectionOutcome; // success | failure | partial | unknown
  context: string;           // Situation where lesson applies
  confidence: number;        // 0-1 confidence score
  tags: string[];           // Categorization tags
  accessCount: number;       // Usage tracking
  createdAt: Date;
}
\end{lstlisting}

\subsection{Storage and Retrieval}

The \texttt{ReflectionStore} provides embedding-based retrieval of relevant past experiences:

\begin{lstlisting}[caption={Reflection store operations}]
class ReflectionStore {
  async addReflection(input: ReflectionInput): Promise<Reflection> {
    const embedding = await this.embeddingProvider?.embed(
      `${input.content} ${input.context}`
    );

    return this.db.insert('reflections', {
      ...input,
      embedding,
      accessCount: 0
    });
  }

  async findRelevant(query: ReflectionQuery): Promise<Reflection[]> {
    const queryEmbedding = await this.embeddingProvider?.embed(query.context);

    const candidates = await this.db.query(`
      SELECT * FROM ${this.prefix}_reflections
      WHERE session_id = ? OR ? = true
      ORDER BY created_at DESC
    `, [query.sessionId, query.crossSession]);

    // Rank by embedding similarity
    return this.rankBySimilarity(candidates, queryEmbedding, query.limit);
  }

  async getSummary(sessionId?: string): Promise<ReflectionSummary> {
    const reflections = await this.getAll(sessionId);
    return {
      total: reflections.length,
      byOutcome: this.groupByOutcome(reflections),
      topTags: this.extractTopTags(reflections),
      avgConfidence: this.calculateAvgConfidence(reflections)
    };
  }
}
\end{lstlisting}

\subsection{Cross-Session Learning}

Reflections can be shared across sessions within a project, enabling pattern recognition:

\begin{itemize}
    \item Successful approaches are surfaced for similar problems
    \item Failed attempts inform what to avoid
    \item Confidence scores evolve based on outcome consistency
    \item Tags enable filtering by problem domain
\end{itemize}

\section{Proactive Context}

The proactive context system pushes relevant information to the agent based on detected triggers rather than waiting for explicit queries.

\subsection{Subscription Model}

Agents subscribe to context updates through defined patterns:

\begin{lstlisting}[caption={Context subscription interface}]
interface ContextSubscription {
  id: string;
  sessionId: string;
  pattern: WatchPattern;          // What to watch
  triggerType: ProactiveTriggerType; // When to trigger
  callback?: (suggestion: ContextSuggestion) => void;
  status: 'active' | 'paused' | 'completed';
}

interface WatchPattern {
  type: WatchPatternType;  // file_pattern | entity_type | keyword
  value: string;           // Glob pattern, entity type, or keyword
  scope?: string;          // Optional scope limitation
}

type ProactiveTriggerType =
  | 'file_change'          // File modified/created
  | 'cursor_proximity'     // Cursor near relevant code
  | 'context_mention'      // Entity mentioned in conversation
  | 'schedule';            // Time-based
\end{lstlisting}

\subsection{Suggestion Generation}

When a trigger fires, the system generates contextual suggestions:

\begin{lstlisting}[caption={Proactive context provider}]
class ProactiveContextProvider {
  async subscribe(input: SubscriptionInput): Promise<ContextSubscription> {
    const subscription = await this.store.create(input);

    // Register with appropriate watcher
    if (input.triggerType === 'file_change') {
      this.fileWatcher.addPattern(input.pattern.value, (event) => {
        this.handleTrigger(subscription, event);
      });
    }

    return subscription;
  }

  private async handleTrigger(
    subscription: ContextSubscription,
    event: TriggerEvent
  ): Promise<void> {
    // Generate suggestion based on trigger context
    const suggestion = await this.generateSuggestion(subscription, event);

    if (suggestion.relevanceScore >= this.config.minRelevance) {
      await this.deliverSuggestion(subscription, suggestion);
    }
  }

  async getSuggestions(query: ProactiveQuery): Promise<ContextSuggestion[]> {
    return this.store.getPendingSuggestions(query.sessionId, {
      status: query.includeDelivered ? undefined : 'pending',
      limit: query.limit
    });
  }
}
\end{lstlisting}

\subsection{Integration with File Watching}

The proactive system integrates with the file watching infrastructure (Chapter 7) to detect relevant changes:

\begin{itemize}
    \item File modifications trigger re-indexing and suggestion generation
    \item Subscribed patterns are matched against changed file paths
    \item Related entities are automatically included in suggestions
    \item Usage statistics track which suggestions are acted upon
\end{itemize}


% ============================================================================
% CHAPTER 10: SYSTEM INTEGRATION
% ============================================================================
\chapter{System Integration}
\label{ch:integration}

\section{Model Context Protocol}

The primary integration mechanism is the Model Context Protocol (MCP), an open standard for connecting AI assistants to external data sources.

\subsection{MCP Architecture}

\begin{lstlisting}[caption={MCP server implementation}]
class CtxSysMcpServer {
  private server: Server;
  private db: DatabaseConnection;
  private toolRegistry: ToolRegistry;
  
  constructor(config: McpServerConfig) {
    this.server = new Server({
      name: config.name ?? 'ctx-sys',
      version: config.version ?? '0.1.0'
    }, {
      capabilities: {
        tools: {}
      }
    });
    
    this.setupHandlers();
    this.registerTools();
  }
  
  private setupHandlers(): void {
    this.server.setRequestHandler(ListToolsRequestSchema, async () => ({
      tools: this.toolRegistry.list()
    }));
    
    this.server.setRequestHandler(CallToolRequestSchema, async (request) => {
      const tool = this.toolRegistry.get(request.params.name);
      if (!tool) throw new Error(`Unknown tool: ${request.params.name}`);
      
      const result = await tool.execute(request.params.arguments);
      return { content: [{ type: 'text', text: JSON.stringify(result) }] };
    });
  }
  
  async start(): Promise<void> {
    const transport = new StdioServerTransport();
    await this.server.connect(transport);
  }
}
\end{lstlisting}

\subsection{Tool Interface}

MCP tools expose ctx-sys functionality:

\begin{lstlisting}[caption={Tool registration}]
interface Tool {
  name: string;
  description: string;
  inputSchema: JSONSchema;
  execute(args: unknown): Promise<unknown>;
}

// Example: context_query tool
const contextQueryTool: Tool = {
  name: 'context_query',
  description: 'Search for relevant context in the codebase',
  inputSchema: {
    type: 'object',
    properties: {
      query: { type: 'string', description: 'Search query' },
      max_tokens: { type: 'number', description: 'Maximum tokens in response' },
      include_sources: { type: 'boolean', description: 'Include source attribution' }
    },
    required: ['query']
  },
  async execute(args) {
    const { query, max_tokens, include_sources } = args as QueryArgs;
    const results = await multiSearch.search(query);
    const context = contextAssembler.assemble(results, { 
      maxTokens: max_tokens,
      includeSources: include_sources 
    });
    return context;
  }
};
\end{lstlisting}

\subsection{Available Tools}

Table \ref{tab:mcp-tools} summarizes the MCP tools exposed by ctx-sys:

\begin{table}[htbp]
    \centering
    \caption{MCP tools}
    \label{tab:mcp-tools}
    \begin{tabular}{lp{7cm}}
        \toprule
        \textbf{Tool} & \textbf{Description} \\
        \midrule
        \texttt{context\_query} & Search for relevant context \\
        \texttt{index\_codebase} & Index a codebase \\
        \texttt{index\_document} & Index a single document \\
        \texttt{store\_message} & Store a conversation message \\
        \texttt{get\_history} & Retrieve conversation history \\
        \texttt{add\_entity} & Add a custom entity \\
        \texttt{link\_entities} & Create entity relationship \\
        \texttt{query\_graph} & Query the entity graph \\
        \texttt{checkpoint\_save} & Save agent checkpoint \\
        \texttt{checkpoint\_load} & Load agent checkpoint \\
        \texttt{memory\_spill} & Spill hot memory to cold \\
        \texttt{memory\_recall} & Recall from cold storage \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Command Line Interface}

The CLI provides management and debugging capabilities:

\begin{lstlisting}[caption={CLI command structure}]
ctx-sys
  init [directory]     Initialize project configuration
    --name <name>      Project name
    --force            Overwrite existing config
    --global           Initialize global config
    
  index <path>         Index a codebase
    --depth <level>    full|signatures|selective
    --summarize        Generate AI summaries
    --languages <list> Limit to specific languages
    
  search <query>       Search the index
    --limit <n>        Maximum results
    --format <fmt>     Output format (json|text|markdown)
    
  watch                Watch for file changes
    --debounce <ms>    Debounce interval
    
  serve                Start MCP server
    --db <path>        Database path
    --name <name>      Server name
    
  config               Manage configuration
    get <key>          Get config value
    set <key> <value>  Set config value
    
  status               Show project status
\end{lstlisting}

\section{Configuration System}

Configuration is managed at global and per-project levels:

\begin{lstlisting}[caption={Configuration schema}]
// Global configuration (~/.ctx-sys/config.yaml)
interface GlobalConfig {
  database: {
    path: string;  // Default database location
  };
  providers: {
    ollama?: { base_url: string };
    openai?: { api_key: string };
    anthropic?: { api_key: string };
  };
  defaults: {
    summarization_provider: string;
    embedding_provider: string;
  };
}

// Project configuration (.ctx-sys.yaml)
interface ProjectConfig {
  project: {
    name: string;
    description?: string;
  };
  indexing: {
    include: string[];
    exclude: string[];
    languages: string[];
  };
  embeddings: {
    provider: string;
    model: string;
  };
  retrieval: {
    default_max_tokens: number;
    strategies: string[];
  };
}
\end{lstlisting}

\subsection{Configuration Resolution}

Configuration values are resolved with precedence:

\begin{enumerate}
    \item Environment variables (highest priority)
    \item Project configuration file
    \item Global configuration file
    \item Built-in defaults (lowest priority)
\end{enumerate}


% ============================================================================
% CHAPTER 11: EVALUATION
% ============================================================================
\chapter{Evaluation}
\label{ch:evaluation}

% TODO: Add comprehensive evaluation with real benchmarks
\textbf{Note}: This chapter presents preliminary evaluation. Comprehensive benchmarks are planned for future work.

\section{Evaluation Methodology}

\subsection{Test Coverage}

The implementation includes comprehensive unit and integration testing:

\begin{itemize}
    \item \textbf{Test Count}: 1271 passing tests across 9 implementation phases
    \item \textbf{Code Coverage}: Approximately 85\% line coverage
    \item \textbf{Test Categories}:
    \begin{itemize}
        \item Unit tests for individual components
        \item Integration tests for component interactions
        \item End-to-end tests for CLI and MCP interface
    \end{itemize}
\end{itemize}

\subsection{Codebase Metrics}

\begin{table}[htbp]
    \centering
    \caption{Implementation metrics}
    \label{tab:implementation-metrics}
    \begin{tabular}{lr}
        \toprule
        \textbf{Metric} & \textbf{Value} \\
        \midrule
        Source lines of code & $\sim$20,000 \\
        Test lines of code & $\sim$16,000 \\
        Number of modules & 52 \\
        External dependencies & 10 \\
        Passing tests & 1271 \\
        Implementation phases & 9 \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Retrieval Quality}

% TODO: Add quantitative retrieval benchmarks

\subsection{Qualitative Assessment}

Preliminary testing on sample codebases demonstrates:

\begin{itemize}
    \item \textbf{Keyword queries}: High precision for exact symbol lookup
    \item \textbf{Conceptual queries}: Good recall with semantic search
    \item \textbf{Dependency queries}: Effective graph traversal for import chains
\end{itemize}

\subsection{Strategy Comparison}

% TODO: Add quantitative comparison with baseline systems

Informal comparison suggests:
\begin{itemize}
    \item Multi-strategy fusion outperforms single-strategy approaches
    \item HyDE provides measurable improvement for vocabulary-mismatched queries
    \item Retrieval gating reduces unnecessary computation by approximately 30\%
\end{itemize}

\section{Performance Characteristics}

\subsection{Indexing Performance}

% TODO: Add actual benchmarks

Preliminary measurements on medium-sized codebases (10,000-50,000 lines):
\begin{itemize}
    \item AST parsing: $\sim$100 files/second
    \item Embedding generation (local Ollama): $\sim$10 entities/second
    \item Embedding generation (OpenAI API): $\sim$50 entities/second (with batching)
\end{itemize}

\subsection{Query Performance}

\begin{itemize}
    \item Query parsing: < 1ms
    \item Vector similarity search: 10-50ms (depends on corpus size)
    \item Graph traversal: 5-20ms for depth 2
    \item Full retrieval pipeline: 50-200ms (without HyDE)
    \item With HyDE: +500-2000ms (LLM generation)
\end{itemize}

\section{Limitations}

Current limitations include:

\begin{enumerate}
    \item \textbf{Vector Search Scalability}: In-memory similarity computation limits corpus size. Production deployment should use sqlite-vec.
    
    \item \textbf{Summarization Latency}: LLM-based summarization is slow; should be performed asynchronously.
    
    \item \textbf{Language Support}: AST parsing limited to languages with available tree-sitter grammars.
    
    \item \textbf{Evaluation Depth}: Comprehensive benchmarks against baseline systems not yet completed.
\end{enumerate}


% ============================================================================
% CHAPTER 12: CONCLUSION
% ============================================================================
\chapter{Conclusion}
\label{ch:conclusion}

\section{Summary}

This thesis presented ctx-sys, an intelligent context management system for AI-assisted software development. The system addresses the fundamental problem of context limitations in LLM-based coding assistants through a comprehensive architecture integrating:

\begin{itemize}
    \item A unified entity model supporting heterogeneous information types
    \item Multi-language AST parsing for structural code understanding
    \item Hybrid retrieval combining vector similarity, graph traversal, and keyword search
    \item Advanced techniques including HyDE query expansion and retrieval gating
    \item Verification mechanisms through draft-critique loops
    \item Agent-oriented memory management with checkpointing and tiering
\end{itemize}

The implementation demonstrates that practical context management for AI coding assistants is achievable with reasonable complexity, enabling significantly improved context relevance compared to naive approaches.

\section{Contributions Revisited}

The key contributions of this work include:

\begin{enumerate}
    \item A practical architecture for context-aware code retrieval that balances multiple strategies for comprehensive coverage
    
    \item Integration of established techniques (RAG, tree-sitter AST parsing, dense embeddings) into a cohesive system
    
    \item Implementation of advanced patterns (HyDE, retrieval gating, draft-critique) adapted for code-specific use cases
    
    \item A complete, tested reference implementation with 1271 passing tests and MCP integration for practical deployment
\end{enumerate}

\section{Future Work}

Several directions remain for future investigation:

\subsection{Completed Features (Phases 1-9)}

The implementation is feature-complete across all planned phases:

\textbf{Phases 1-7: Core Infrastructure}
\begin{itemize}
    \item Database infrastructure and project management
    \item Entity storage and embedding pipeline
    \item MCP server implementation
    \item AST parsing and code summarization
    \item Relationship extraction and graph storage
    \item Conversation management and decision extraction
    \item Document intelligence and requirement extraction
    \item Multi-strategy search and context assembly
    \item HyDE, retrieval gating, and draft-critique
    \item Configuration system and CLI
    \item File watching with incremental updates
\end{itemize}

\textbf{Phase 8: Agent Patterns}
\begin{itemize}
    \item Agent checkpointing with save/restore and auto-pruning
    \item Hot/cold memory tiering with explicit spill/recall APIs
    \item Reflection storage for lessons learned
    \item Proactive context with subscriptions and suggestions
\end{itemize}

\textbf{Phase 9: Integrations \& Analytics}
\begin{itemize}
    \item Token analytics with query logging and ROI dashboards
    \item Git hooks for automatic indexing with impact analysis
    \item Support documentation in MDX format
    \item Product website scaffold with Next.js
    \item NPM distribution configuration
\end{itemize}

\subsection{Future Enhancements}

Several directions remain for future work:

\begin{enumerate}
    \item \textbf{VS Code Extension UI}: While MCP tools are complete, a native VS Code extension with sidebar panel and hover information would improve developer experience

    \item \textbf{Team Knowledge Base}: Cross-team sharing of decisions and context repositories

    \item \textbf{Adaptive Strategy Selection}: Learning optimal strategy weights based on query characteristics and feedback

    \item \textbf{Code-Specific Embeddings}: Training or fine-tuning embedding models specifically for code retrieval

    \item \textbf{Multi-Repository Context}: Methods for retrieving context across related repositories

    \item \textbf{Production Deployment}: sqlite-vec integration for scalable vector search, cloud deployment options
\end{enumerate}

\section{Closing Remarks}

As AI coding assistants become integral to software development workflows, effective context management will be increasingly critical. The ``smart librarian'' paradigm embodied by ctx-sys---knowing where information exists rather than hoarding everything---provides a scalable foundation for context-aware AI assistance.

This thesis represents both a practical contribution (a deployable system) and a reference architecture for future work in this rapidly evolving space.


% ============================================================================
% BIBLIOGRAPHY
% ============================================================================
\chapter*{Bibliography}
\addcontentsline{toc}{chapter}{Bibliography}

% TODO: Replace with actual bibliography file
\begin{thebibliography}{99}

\bibitem{lewis2020rag}
Lewis, P., et al. (2020).
\textit{Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.}
NeurIPS 2020.

\bibitem{karpukhin2020dpr}
Karpukhin, V., et al. (2020).
\textit{Dense Passage Retrieval for Open-Domain Question Answering.}
EMNLP 2020.

\bibitem{edge2024graphrag}
Edge, D., et al. (2024).
\textit{From Local to Global: A Graph RAG Approach to Query-Focused Summarization.}
Microsoft Research.

\bibitem{gao2022hyde}
Gao, L., et al. (2022).
\textit{Precise Zero-Shot Dense Retrieval without Relevance Labels.}
arXiv:2212.10496.

\bibitem{treesitter}
Brunsfeld, M. (2018).
\textit{Tree-sitter: An incremental parsing system for programming tools.}
https://tree-sitter.github.io/tree-sitter/

\bibitem{feng2020codebert}
Feng, Z., et al. (2020).
\textit{CodeBERT: A Pre-Trained Model for Programming and Natural Languages.}
EMNLP 2020.

\bibitem{wang2021codet5}
Wang, Y., et al. (2021).
\textit{CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation.}
EMNLP 2021.

\bibitem{alon2019code2vec}
Alon, U., et al. (2019).
\textit{code2vec: Learning Distributed Representations of Code.}
POPL 2019.

\bibitem{xu2021conversation}
Xu, Y., et al. (2021).
\textit{Beyond Goldfish Memory: Long-Term Open-Domain Conversation.}
ACL 2022.

\bibitem{packer2023memgpt}
Packer, C., et al. (2023).
\textit{MemGPT: Towards LLMs as Operating Systems.}
arXiv:2310.08560.

\bibitem{wang2022selfconsistency}
Wang, X., et al. (2022).
\textit{Self-Consistency Improves Chain of Thought Reasoning in Language Models.}
ICLR 2023.

\end{thebibliography}


% ============================================================================
% APPENDICES
% ============================================================================
\begin{appendices}

\chapter{Database Schema Reference}
\label{app:schema}

Complete SQL schema for ctx-sys database:

\begin{lstlisting}[style=sql,caption={Complete database schema}]
-- See Chapter 4 for detailed schema documentation

-- Global Tables
CREATE TABLE projects (...);
CREATE TABLE embedding_models (...);
CREATE TABLE config (...);
CREATE TABLE shared_entities (...);
CREATE TABLE cross_project_links (...);

-- Per-Project Tables (prefixed)
CREATE TABLE {prefix}_entities (...);
CREATE TABLE {prefix}_vectors (...);
CREATE TABLE {prefix}_relationships (...);
CREATE TABLE {prefix}_sessions (...);
CREATE TABLE {prefix}_messages (...);
CREATE TABLE {prefix}_decisions (...);
CREATE TABLE {prefix}_checkpoints (...);
CREATE TABLE {prefix}_memory_items (...);
CREATE TABLE {prefix}_feedback (...);
\end{lstlisting}

\chapter{Configuration Reference}
\label{app:config}

\section{Global Configuration}

\begin{lstlisting}[style=yaml,caption={Example global configuration}]
# ~/.ctx-sys/config.yaml

database:
  path: ~/.ctx-sys/ctx-sys.db

providers:
  ollama:
    base_url: http://localhost:11434
  openai:
    api_key: ${OPENAI_API_KEY}
  anthropic:
    api_key: ${ANTHROPIC_API_KEY}

defaults:
  summarization_provider: ollama
  summarization_model: qwen2.5-coder
  embedding_provider: ollama
  embedding_model: nomic-embed-text

cli:
  output_format: text
  color: true
\end{lstlisting}

\section{Project Configuration}

\begin{lstlisting}[style=yaml,caption={Example project configuration}]
# .ctx-sys.yaml

project:
  name: my-project
  description: A sample project

indexing:
  include:
    - src/**/*.ts
    - docs/**/*.md
  exclude:
    - node_modules
    - dist
    - "**/*.test.ts"
  languages:
    - typescript
    - markdown

summarization:
  enabled: true
  provider: ollama

embeddings:
  provider: ollama
  model: nomic-embed-text

sessions:
  retention: 30
  auto_summarize: true

retrieval:
  default_max_tokens: 4000
  strategies:
    - semantic
    - keyword
    - graph
  hyde:
    enabled: true
  gating:
    enabled: true
\end{lstlisting}

\chapter{MCP Tool Reference}
\label{app:mcp-tools}

% TODO: Add complete MCP tool documentation with request/response schemas

Complete documentation for all MCP tools including input schemas and example responses.

\end{appendices}

\printbibliography[heading=bibintoc,title={References}]

\end{document}
