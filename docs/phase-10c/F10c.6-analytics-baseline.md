# F10c.6 Analytics Baseline: Realistic Savings Comparison

**Phase**: 10c - Retrieval Quality Improvements
**Priority**: Medium
**Dependencies**: None

## Problem

The token savings metric compares against dumping the entire codebase into context (`tokensEstimatedFull`), which is unrealistic. No developer or AI tool actually does this. The real comparison should be against typical alternatives: `grep + read` for targeted searches, or "read the whole file" for file-level context. The current 86% savings figure is misleading.

### Observed Behavior

```
analytics_get_stats()
→ tokensSaved: 691348, savingsPercent: 86.4%
→ Compared against: tokensEstimatedFull: 100000 (heuristic)
→ Reality: A grep+read workflow would retrieve ~5000-15000 tokens
→ Actual savings vs grep+read: ~50-70% (still good, but honest)
```

### Root Cause

In `src/analytics/query-logger.ts` (line 317), `getFullContextEstimate` falls back to a hardcoded `100000` tokens when no measured estimate exists. Even the measured estimate from `FullContextEstimator` compares against the entire project — all files concatenated.

## Implementation Plan

### Step 1: Define Multiple Comparison Baselines

**File**: `src/analytics/types.ts`

```typescript
export interface ComparisonBaseline {
  /** Full codebase dump (current) */
  fullContext: number;
  /** Targeted grep+read: avg 3-5 file reads per query */
  grepRead: number;
  /** Single file read: avg file size */
  singleFile: number;
  /** Summary-only: entity summaries without content */
  summaryOnly: number;
}

export interface RealisticStats extends UsageStats {
  /** Savings vs different baselines */
  savingsVsGrep: number;       // % saved vs grep+read
  savingsVsFile: number;       // % saved vs single file read
  savingsVsFull: number;       // % saved vs full dump (current)
  /** Quality-adjusted savings (savings * relevance) */
  qualityAdjustedSavings: number;
}
```

### Step 2: Measure Realistic Baselines

**File**: `src/analytics/full-context-estimator.ts`

```typescript
async measureBaselines(
  projectId: string,
  projectPath: string
): Promise<ComparisonBaseline> {
  const fullEstimate = await this.measureProject(projectId, projectPath);

  // Grep+read baseline: average file size * typical files per query
  const avgFileTokens = fullEstimate.totalTokens / Math.max(fullEstimate.totalFiles, 1);
  const typicalFilesPerQuery = 4; // Based on common grep workflows
  const grepReadBaseline = avgFileTokens * typicalFilesPerQuery;

  return {
    fullContext: fullEstimate.totalTokens,
    grepRead: Math.round(grepReadBaseline),
    singleFile: Math.round(avgFileTokens),
    summaryOnly: Math.round(fullEstimate.totalTokens * 0.05)
  };
}
```

### Step 3: Update Dashboard to Show Multiple Baselines

**File**: `src/analytics/dashboard.ts`

```typescript
interface DashboardSummary {
  totalQueries: number;
  tokensSaved: string;      // vs full context (existing)
  tokensSavedVsGrep: string; // vs grep+read (new)
  costSaved: string;
  savingsPercent: number;    // vs full (existing)
  savingsPercentVsGrep: number; // vs grep+read (new)
  avgRelevance: number;
  qualityScore: string;      // relevance * savings combined
}
```

### Step 4: Add Quality-Adjusted Savings

Savings mean nothing if the retrieved content isn't relevant. Weight savings by relevance:

```typescript
function qualityAdjustedSavings(
  tokensSaved: number,
  tokensRetrieved: number,
  relevance: number
): number {
  // If relevance is low, the "savings" aren't real savings
  // because the user would need to do additional retrieval
  const effectiveSavings = tokensSaved * relevance;
  const totalContext = tokensSaved + tokensRetrieved;
  return totalContext > 0 ? (effectiveSavings / totalContext) * 100 : 0;
}
```

### Step 5: Track Retrieval Completeness

**File**: `src/analytics/query-logger.ts`

Log whether the user needed to do follow-up queries (indicating incomplete retrieval):

```typescript
interface QueryLog {
  // ... existing fields ...
  /** Whether this query was a follow-up to refine previous results */
  isFollowUp?: boolean;
  /** Related query IDs (for tracking refinement chains) */
  relatedQueries?: string[];
}
```

## Testing

```typescript
describe('Realistic analytics baselines', () => {
  it('should calculate grep+read baseline', async () => {
    const baselines = await estimator.measureBaselines(projectId, projectPath);
    expect(baselines.grepRead).toBeGreaterThan(0);
    expect(baselines.grepRead).toBeLessThan(baselines.fullContext);
  });

  it('should show savings vs grep is lower than vs full', async () => {
    const stats = await logger.getRealisticStats(projectId, 'week');
    expect(stats.savingsVsGrep).toBeLessThan(stats.savingsVsFull);
  });

  it('should quality-adjust savings by relevance', async () => {
    const qa = qualityAdjustedSavings(90000, 10000, 0.5);
    // 50% relevance should halve the effective savings
    expect(qa).toBeLessThan(45);
  });

  it('should penalize low-relevance "savings"', async () => {
    const highQ = qualityAdjustedSavings(90000, 10000, 0.9);
    const lowQ = qualityAdjustedSavings(90000, 10000, 0.1);
    expect(highQ).toBeGreaterThan(lowQ * 5);
  });
});
```

## Success Criteria

- Dashboard shows savings vs grep+read baseline (not just full context)
- Quality-adjusted savings factor in relevance scores
- Savings figures are honest and defensible
- Multiple baseline comparisons available in analytics API
- Follow-up query tracking identifies incomplete retrievals
