# F10g.2 Score Normalization

**Phase**: 10g - Retrieval Foundations
**Priority**: High
**Dependencies**: None

## Problem

Relevance scores displayed to the user are not on a meaningful scale. A "69% relevance" doesn't mean "69% relevant" — it's an RRF fusion score plus heuristic boosts that happen to sum to 0.69. The scores can exceed 1.0 (requiring a cap), go negative (from reranker penalties), and don't compare meaningfully across queries.

### Score Pipeline (Current)

```
RRF Fusion         →  ~0.01-0.02 (1/(60+rank))
+ Reranker Boosts  →  +0.1 to +2.5 (name match, type, summary, connections)
= Display Score    →  0.0 to 2.5+ (capped at 1.0 by F10f.2)
```

The reranker adds absolute boosts that dwarf the RRF scores:
- Exact name match: +2.0
- Name contains query term: +0.5 per term
- Summary relevance: up to +0.3
- Entity type: -0.1 to +0.15
- Short content penalty: -0.3
- Connection boost: +0.2

For a query like "GraphTraversal", the entity `GraphTraversal` gets: RRF 0.016 + name match 2.0 + type boost 0.15 = 2.166, capped to 1.0 (shown as 100%). This makes the score meaningless — it's either "name matched" (100%) or "didn't match" (much lower).

### What Scores Should Mean

| Score | Meaning |
|-------|---------|
| 0.8-1.0 | Exact or near-exact match for the query |
| 0.5-0.8 | Strongly relevant — directly related code/docs |
| 0.2-0.5 | Moderately relevant — related but not central |
| 0.0-0.2 | Weakly relevant — tangential connection |

## Implementation

### Step 1: Switch reranker to multiplicative scoring

**File**: `src/retrieval/heuristic-reranker.ts`

Instead of adding absolute boosts to the RRF score, use multiplicative factors that scale the existing score:

```typescript
async rerank(query: string, candidates: SearchResult[]): Promise<RerankerResult[]> {
  const queryTerms = this.extractTerms(query);
  const queryLower = query.toLowerCase();

  const results = candidates.map(candidate => {
    let multiplier = 1.0;
    const entity = candidate.entity;

    // Exact name match — strong signal
    if (entity.name.toLowerCase() === queryLower) {
      multiplier *= 3.0;
    }

    // Name contains query terms
    const nameLower = entity.name.toLowerCase();
    const nameHits = queryTerms.filter(t => nameLower.includes(t)).length;
    if (queryTerms.length > 0 && nameHits > 0) {
      multiplier *= 1.0 + (nameHits / queryTerms.length) * 1.5;
    }

    // Summary relevance
    if (entity.summary) {
      const summaryLower = entity.summary.toLowerCase();
      const termHits = queryTerms.filter(t => summaryLower.includes(t)).length;
      if (queryTerms.length > 0 && termHits > 0) {
        multiplier *= 1.0 + (termHits / queryTerms.length) * 0.5;
      }
    }

    // Penalize stubs
    if (entity.content && entity.content.length < 50) {
      multiplier *= 0.5;
    }

    // Entity type weight
    const typeMultipliers: Record<string, number> = {
      'class': 1.3, 'function': 1.2, 'method': 1.2,
      'interface': 1.1, 'type': 1.1,
      'document': 1.2, 'section': 1.1,
      'file': 0.7, 'module': 0.8,
    };
    multiplier *= typeMultipliers[entity.type] || 1.0;

    return {
      entityId: entity.id,
      originalScore: candidate.score,
      rerankedScore: candidate.score * multiplier,
      source: candidate.source
    };
  });

  // ... sort and return
}
```

### Step 2: Normalize final scores to [0, 1]

**File**: `src/retrieval/heuristic-reranker.ts`

After multiplicative scoring, normalize to [0, 1] using the maximum score in the result set:

```typescript
// After computing all rerankedScores:
const maxScore = Math.max(...results.map(r => r.rerankedScore), 0.001);
for (const r of results) {
  r.rerankedScore = r.rerankedScore / maxScore;
}
```

This ensures the best result is always 1.0 (or close to it) and other results are proportional. When no results match well, the absolute scores stay low because the RRF inputs are low.

### Step 3: Adjust confidence and minRelevance thresholds

With normalized scores:
- `minRelevance` default should stay at 0.1 (bottom 10% of best result's score)
- Confidence calculation remains weighted top-5 (already sensible with normalized scores)
- Remove the `Math.min(1.0, raw)` cap in `calculateConfidence()` since scores are now naturally bounded

## Expected Result

```
# Before (absolute boosts):
class  GraphTraversal    100%  ← capped, meaningless
class  RelationshipStore  72%
file   traversal.ts       72%  ← same as a class?

# After (multiplicative + normalized):
class  GraphTraversal    100%  ← best match (normalized)
class  RelationshipStore  65%  ← proportionally ranked
file   traversal.ts       38%  ← properly penalized
```

## Success Criteria

- Scores are always in [0, 1] without needing a cap
- Best result for a good query is near 1.0
- Scores are proportional — a result half as relevant shows ~0.5x the score
- Confidence metric works naturally without capping
- Entity type differences are visible in scores (classes rank higher than file stubs)
