# F10.12: Advanced Query Pipeline — Decomposition + Re-ranking

## Goal

Improve retrieval quality for complex queries by decomposing multi-part questions into sub-queries, and by adding a re-ranking pass that uses the LLM to score result relevance after initial retrieval.

## Current Limitation

### No Query Decomposition

```typescript
// Complex query treated as one monolithic search:
"How does the auth system validate tokens and what happens when they expire?"

// Current: single LIKE/vector search for the whole string
// Should be: decomposed into sub-queries:
//   1. "auth system token validation"
//   2. "token expiration handling"
// Results merged and deduplicated
```

The query parser (`src/retrieval/query-parser.ts`) detects intent and extracts entity mentions, but `generateSearchQueries()` just produces variations of the same query — never breaks it into sub-questions.

### No Re-ranking

```typescript
// Current: RRF score fusion with hardcoded weights
const weights = { keyword: 0.6, semantic: 1.0, graph: 0.8 };
// No verification that top results actually answer the query
// No cross-encoder or LLM-based relevance scoring
```

## Solution

### Query Decomposition

Add a decomposition step to the query parser that breaks complex queries into sub-queries:

```typescript
interface DecomposedQuery {
  original: string;
  subQueries: Array<{
    query: string;
    intent: QueryIntent;
    focus: string;  // What this sub-query is trying to find
  }>;
  mergeStrategy: 'union' | 'intersection' | 'sequential';
}

class QueryDecomposer {
  // Rule-based decomposition (no LLM needed)
  decompose(query: string): DecomposedQuery;
}
```

Decomposition rules (pattern-based, no LLM required):

- Split on "and", "also", "as well as" → union sub-queries
- Split on "then", "after", "when" → sequential sub-queries
- Detect multi-entity queries → separate sub-query per entity
- Keep single-topic queries intact (no decomposition)

### LLM Re-ranking

After initial retrieval returns top-K candidates, use the local LLM to score relevance:

```typescript
interface RerankedResult {
  entityId: string;
  originalScore: number;
  rerankedScore: number;
  reasoning?: string;
}

class LLMReranker {
  constructor(options?: { baseUrl?: string; model?: string });

  // Re-rank top-K results against the original query
  async rerank(
    query: string,
    candidates: Array<{ entityId: string; name: string; content?: string; score: number }>,
    topK?: number  // Only re-rank top N (default: 20)
  ): Promise<RerankedResult[]>;
}
```

The LLM prompt asks: "Given this query and these code/document snippets, rate each snippet's relevance from 0-10." This is much cheaper than a cross-encoder model and can use the existing qwen3:0.6b.

## Implementation Plan

### Step 1: Query decomposition

- Create `src/retrieval/query-decomposer.ts`
- Pattern-based splitting (conjunctions, multi-entity detection)
- Integrate into `MultiStrategySearch` — run sub-queries, merge results
- Deduplication across sub-query results

### Step 2: LLM re-ranking

- Create `src/retrieval/llm-reranker.ts`
- Prompt design: batch candidates into a single prompt, ask for relevance scores
- Handle `<think>` tag stripping, JSON response parsing
- Only re-rank top-20 candidates (keep it fast)
- Make re-ranking opt-in (flag in search options)

### Step 3: Integration

- Update `MultiStrategySearch` to optionally decompose + re-rank
- Update `context_query` MCP tool to support `rerank: true` option
- Update CLI `search` command to support `--rerank` flag

## Verification

- "How does auth work and how is logging configured?" returns results for both topics
- Re-ranked results are more relevant than RRF-only results
- Re-ranking adds <2s latency for top-20 candidates
- Single-topic queries are not unnecessarily decomposed
