# F10.6: LLM-Generated Summaries

## Goal

Generate intelligent AI-powered summaries for code entities (functions, classes, modules) during indexing. Summaries provide semantic understanding beyond what AST parsing alone can offer, enabling more accurate retrieval and better context for AI assistants.

## Current Limitation

```typescript
// Current: Entities store mechanical descriptions
{
  name: "processPayment",
  type: "function",
  summary: "function with 3 parameters",  // Not helpful!
  content: "..."
}

// What we need: Semantic understanding
{
  name: "processPayment",
  type: "function",
  summary: "Processes a credit card payment by validating the card,
            checking for fraud, and charging through Stripe. Returns
            a PaymentResult with transaction ID or error details.",
  content: "..."
}
```

## Solution Overview

Integrate LLM-based summarization into the indexing pipeline:

1. **Provider Abstraction** - Support both Ollama (local) and cloud models (OpenAI, Anthropic)
2. **Batch Processing** - Summarize multiple entities efficiently
3. **Incremental Updates** - Only re-summarize changed entities
4. **Quality Prompts** - Specialized prompts for different entity types
5. **Fallback Handling** - Graceful degradation when LLM unavailable

## Architecture

### Provider Interface

```
┌─────────────────────────────────────────────────────────────┐
│                  Summarization Manager                       │
├─────────────────────────────────────────────────────────────┤
│  ┌─────────────────┐  ┌─────────────────┐                   │
│  │  Provider       │  │  Prompt         │                   │
│  │  Selector       │──│  Templates      │                   │
│  └────────┬────────┘  └─────────────────┘                   │
│           │                                                  │
│  ┌────────┴────────────────────────────────┐                │
│  │         Provider Interface               │                │
│  ├──────────────┬──────────────┬───────────┤                │
│  │    Ollama    │    OpenAI    │  Anthropic │                │
│  │    (local)   │   (cloud)    │  (cloud)   │                │
│  └──────────────┴──────────────┴───────────┘                │
└─────────────────────────────────────────────────────────────┘
```

### Summarization Flow

```
Entity Batch
    │
    ├── Check content hash (skip if unchanged)
    │
    ├── Select provider (Ollama → OpenAI → Anthropic → fallback)
    │
    ├── Build prompt (entity-type specific)
    │
    ├── Call LLM (with timeout & retry)
    │
    └── Store summary + hash
```

## Implementation

### 1. Summarization Provider Interface

```typescript
// src/summarization/provider.ts

export interface SummarizationProvider {
  /** Provider identifier */
  readonly id: string;

  /** Model being used */
  readonly model: string;

  /** Check if provider is available */
  isAvailable(): Promise<boolean>;

  /** Generate summary for content */
  summarize(content: string, options: SummarizeOptions): Promise<string>;

  /** Batch summarization for efficiency */
  summarizeBatch(items: SummarizeItem[]): Promise<string[]>;
}

export interface SummarizeOptions {
  /** Type of code entity */
  entityType: EntityType;

  /** Entity name for context */
  name: string;

  /** File path for context */
  filePath?: string;

  /** Maximum tokens for response */
  maxTokens?: number;

  /** Temperature for creativity (0-1) */
  temperature?: number;
}

export interface SummarizeItem {
  id: string;
  content: string;
  options: SummarizeOptions;
}
```

### 2. Ollama Provider (Local)

```typescript
// src/summarization/providers/ollama.ts

import { SummarizationProvider, SummarizeOptions, SummarizeItem } from '../provider';

export class OllamaSummarizationProvider implements SummarizationProvider {
  readonly id = 'ollama';
  readonly model: string;

  private baseUrl: string;
  private available: boolean | null = null;

  constructor(options: {
    baseUrl?: string;
    model?: string;
  } = {}) {
    this.baseUrl = options.baseUrl || 'http://localhost:11434';
    this.model = options.model || 'qwen2.5-coder:7b';
  }

  async isAvailable(): Promise<boolean> {
    if (this.available !== null) return this.available;

    try {
      const response = await fetch(`${this.baseUrl}/api/tags`);
      if (!response.ok) {
        this.available = false;
        return false;
      }

      const data = await response.json();
      const models = data.models?.map((m: any) => m.name) || [];

      // Check if our model is available
      this.available = models.some((m: string) =>
        m === this.model || m.startsWith(this.model.split(':')[0])
      );

      return this.available;
    } catch {
      this.available = false;
      return false;
    }
  }

  async summarize(content: string, options: SummarizeOptions): Promise<string> {
    const prompt = this.buildPrompt(content, options);

    const response = await fetch(`${this.baseUrl}/api/generate`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: this.model,
        prompt,
        stream: false,
        options: {
          temperature: options.temperature ?? 0.3,
          num_predict: options.maxTokens ?? 150
        }
      })
    });

    if (!response.ok) {
      throw new Error(`Ollama error: ${response.statusText}`);
    }

    const data = await response.json();
    return data.response.trim();
  }

  async summarizeBatch(items: SummarizeItem[]): Promise<string[]> {
    // Ollama doesn't have native batch API, process sequentially
    // but with concurrency limit
    const results: string[] = [];
    const concurrency = 3;

    for (let i = 0; i < items.length; i += concurrency) {
      const batch = items.slice(i, i + concurrency);
      const batchResults = await Promise.all(
        batch.map(item => this.summarize(item.content, item.options))
      );
      results.push(...batchResults);
    }

    return results;
  }

  private buildPrompt(content: string, options: SummarizeOptions): string {
    const typePrompts: Record<string, string> = {
      function: `Summarize this ${options.name} function in 1-2 sentences.
Focus on: what it does, key parameters, return value, and any side effects.`,

      method: `Summarize this ${options.name} method in 1-2 sentences.
Focus on: its purpose within the class, parameters, return value.`,

      class: `Summarize this ${options.name} class in 2-3 sentences.
Focus on: its responsibility, key methods, and relationships.`,

      interface: `Summarize this ${options.name} interface in 1-2 sentences.
Focus on: what it defines and its purpose in the codebase.`,

      module: `Summarize this module in 2-3 sentences.
Focus on: what functionality it provides and its main exports.`,

      file: `Summarize this file in 2-3 sentences.
Focus on: its purpose, main exports, and how it fits in the codebase.`
    };

    const typePrompt = typePrompts[options.entityType] ||
      `Summarize this ${options.entityType} in 1-2 sentences.`;

    return `${typePrompt}

Be concise and technical. Don't start with "This function..." - just describe what it does.

Code:
\`\`\`
${content}
\`\`\`

Summary:`;
  }
}
```

### 3. OpenAI Provider (Cloud)

```typescript
// src/summarization/providers/openai.ts

import { SummarizationProvider, SummarizeOptions, SummarizeItem } from '../provider';

export class OpenAISummarizationProvider implements SummarizationProvider {
  readonly id = 'openai';
  readonly model: string;

  private apiKey: string;
  private baseUrl: string;

  constructor(options: {
    apiKey?: string;
    model?: string;
    baseUrl?: string;
  } = {}) {
    this.apiKey = options.apiKey || process.env.OPENAI_API_KEY || '';
    this.model = options.model || 'gpt-4o-mini';
    this.baseUrl = options.baseUrl || 'https://api.openai.com/v1';
  }

  async isAvailable(): Promise<boolean> {
    return !!this.apiKey;
  }

  async summarize(content: string, options: SummarizeOptions): Promise<string> {
    const response = await fetch(`${this.baseUrl}/chat/completions`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${this.apiKey}`
      },
      body: JSON.stringify({
        model: this.model,
        messages: [
          {
            role: 'system',
            content: this.getSystemPrompt(options.entityType)
          },
          {
            role: 'user',
            content: this.buildUserPrompt(content, options)
          }
        ],
        temperature: options.temperature ?? 0.3,
        max_tokens: options.maxTokens ?? 150
      })
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(`OpenAI error: ${error.error?.message || response.statusText}`);
    }

    const data = await response.json();
    return data.choices[0].message.content.trim();
  }

  async summarizeBatch(items: SummarizeItem[]): Promise<string[]> {
    // OpenAI supports concurrent requests, use Promise.all with limit
    const concurrency = 10;
    const results: string[] = [];

    for (let i = 0; i < items.length; i += concurrency) {
      const batch = items.slice(i, i + concurrency);
      const batchResults = await Promise.all(
        batch.map(item => this.summarize(item.content, item.options))
      );
      results.push(...batchResults);
    }

    return results;
  }

  private getSystemPrompt(entityType: string): string {
    return `You are a code documentation expert. Generate concise, technical summaries
of code entities. Focus on what the code does, not how it's implemented.
Be direct - don't start with phrases like "This function..." or "This class...".
Keep summaries to 1-3 sentences.`;
  }

  private buildUserPrompt(content: string, options: SummarizeOptions): string {
    return `Summarize this ${options.entityType} named "${options.name}":

\`\`\`
${content}
\`\`\``;
  }
}
```

### 4. Anthropic Provider (Cloud)

```typescript
// src/summarization/providers/anthropic.ts

import { SummarizationProvider, SummarizeOptions, SummarizeItem } from '../provider';

export class AnthropicSummarizationProvider implements SummarizationProvider {
  readonly id = 'anthropic';
  readonly model: string;

  private apiKey: string;

  constructor(options: {
    apiKey?: string;
    model?: string;
  } = {}) {
    this.apiKey = options.apiKey || process.env.ANTHROPIC_API_KEY || '';
    this.model = options.model || 'claude-3-haiku-20240307';
  }

  async isAvailable(): Promise<boolean> {
    return !!this.apiKey;
  }

  async summarize(content: string, options: SummarizeOptions): Promise<string> {
    const response = await fetch('https://api.anthropic.com/v1/messages', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'x-api-key': this.apiKey,
        'anthropic-version': '2023-06-01'
      },
      body: JSON.stringify({
        model: this.model,
        max_tokens: options.maxTokens ?? 150,
        messages: [
          {
            role: 'user',
            content: this.buildPrompt(content, options)
          }
        ]
      })
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(`Anthropic error: ${error.error?.message || response.statusText}`);
    }

    const data = await response.json();
    return data.content[0].text.trim();
  }

  async summarizeBatch(items: SummarizeItem[]): Promise<string[]> {
    const concurrency = 5;
    const results: string[] = [];

    for (let i = 0; i < items.length; i += concurrency) {
      const batch = items.slice(i, i + concurrency);
      const batchResults = await Promise.all(
        batch.map(item => this.summarize(item.content, item.options))
      );
      results.push(...batchResults);
    }

    return results;
  }

  private buildPrompt(content: string, options: SummarizeOptions): string {
    return `Summarize this ${options.entityType} named "${options.name}" in 1-3 concise sentences.
Focus on what it does, not implementation details. Be direct.

\`\`\`
${content}
\`\`\``;
  }
}
```

### 5. Summarization Manager

```typescript
// src/summarization/manager.ts

import { DatabaseConnection } from '../db/connection';
import { SummarizationProvider, SummarizeItem } from './provider';
import { OllamaSummarizationProvider } from './providers/ollama';
import { OpenAISummarizationProvider } from './providers/openai';
import { AnthropicSummarizationProvider } from './providers/anthropic';
import { hashContent } from '../utils/hash';

export interface SummarizationConfig {
  /** Provider preference order */
  providers?: ('ollama' | 'openai' | 'anthropic')[];

  /** Ollama settings */
  ollama?: {
    baseUrl?: string;
    model?: string;
  };

  /** OpenAI settings */
  openai?: {
    apiKey?: string;
    model?: string;
  };

  /** Anthropic settings */
  anthropic?: {
    apiKey?: string;
    model?: string;
  };

  /** Batch size for processing */
  batchSize?: number;

  /** Max retries on failure */
  maxRetries?: number;
}

export class SummarizationManager {
  private providers: Map<string, SummarizationProvider> = new Map();
  private preferenceOrder: string[];
  private activeProvider: SummarizationProvider | null = null;
  private batchSize: number;
  private maxRetries: number;

  constructor(
    private db: DatabaseConnection,
    private entityTable: string,
    config: SummarizationConfig = {}
  ) {
    this.preferenceOrder = config.providers || ['ollama', 'openai', 'anthropic'];
    this.batchSize = config.batchSize || 10;
    this.maxRetries = config.maxRetries || 3;

    // Initialize providers
    this.providers.set('ollama', new OllamaSummarizationProvider(config.ollama));
    this.providers.set('openai', new OpenAISummarizationProvider(config.openai));
    this.providers.set('anthropic', new AnthropicSummarizationProvider(config.anthropic));
  }

  /**
   * Get the first available provider based on preference order.
   */
  async getProvider(): Promise<SummarizationProvider | null> {
    if (this.activeProvider) return this.activeProvider;

    for (const providerId of this.preferenceOrder) {
      const provider = this.providers.get(providerId);
      if (provider && await provider.isAvailable()) {
        this.activeProvider = provider;
        return provider;
      }
    }

    return null;
  }

  /**
   * Check which providers are available.
   */
  async getAvailableProviders(): Promise<string[]> {
    const available: string[] = [];

    for (const [id, provider] of this.providers) {
      if (await provider.isAvailable()) {
        available.push(id);
      }
    }

    return available;
  }

  /**
   * Summarize entities, skipping those already summarized with same content.
   */
  async summarizeEntities(
    entities: EntityForSummary[],
    options?: {
      force?: boolean;
      onProgress?: (completed: number, total: number, skipped: number) => void;
    }
  ): Promise<SummarizeResult> {
    const provider = await this.getProvider();

    if (!provider) {
      return {
        summarized: 0,
        skipped: entities.length,
        failed: 0,
        provider: null,
        error: 'No summarization provider available'
      };
    }

    // Determine which entities need summarization
    const toSummarize: EntityForSummary[] = [];
    let skipped = 0;

    for (const entity of entities) {
      const contentHash = hashContent(entity.content);

      if (!options?.force) {
        // Check if we already have a summary with this hash
        const existing = this.db.get<{ summary_hash: string }>(
          `SELECT summary_hash FROM ${this.entityTable} WHERE id = ?`,
          [entity.id]
        );

        if (existing?.summary_hash === contentHash) {
          skipped++;
          continue;
        }
      }

      toSummarize.push({ ...entity, contentHash });
    }

    if (toSummarize.length === 0) {
      return {
        summarized: 0,
        skipped,
        failed: 0,
        provider: provider.id
      };
    }

    // Process in batches
    let summarized = 0;
    let failed = 0;

    for (let i = 0; i < toSummarize.length; i += this.batchSize) {
      const batch = toSummarize.slice(i, i + this.batchSize);

      const items: SummarizeItem[] = batch.map(entity => ({
        id: entity.id,
        content: entity.content,
        options: {
          entityType: entity.type,
          name: entity.name,
          filePath: entity.filePath
        }
      }));

      try {
        const summaries = await this.summarizeBatchWithRetry(provider, items);

        // Store summaries
        for (let j = 0; j < batch.length; j++) {
          const entity = batch[j];
          const summary = summaries[j];

          if (summary) {
            this.db.run(
              `UPDATE ${this.entityTable}
               SET summary = ?, summary_hash = ?, summarized_at = CURRENT_TIMESTAMP
               WHERE id = ?`,
              [summary, entity.contentHash, entity.id]
            );
            summarized++;
          } else {
            failed++;
          }
        }
      } catch (error) {
        console.error(`Batch summarization failed:`, error);
        failed += batch.length;
      }

      options?.onProgress?.(summarized, toSummarize.length, skipped);
    }

    return {
      summarized,
      skipped,
      failed,
      provider: provider.id
    };
  }

  /**
   * Summarize a batch with retry logic.
   */
  private async summarizeBatchWithRetry(
    provider: SummarizationProvider,
    items: SummarizeItem[]
  ): Promise<string[]> {
    let lastError: Error | null = null;

    for (let attempt = 0; attempt < this.maxRetries; attempt++) {
      try {
        return await provider.summarizeBatch(items);
      } catch (error) {
        lastError = error as Error;
        // Exponential backoff
        await new Promise(r => setTimeout(r, Math.pow(2, attempt) * 1000));
      }
    }

    throw lastError;
  }

  /**
   * Get summarization statistics.
   */
  getStats(): SummarizationStats {
    const total = this.db.get<{ count: number }>(
      `SELECT COUNT(*) as count FROM ${this.entityTable}`
    );

    const withSummary = this.db.get<{ count: number }>(
      `SELECT COUNT(*) as count FROM ${this.entityTable} WHERE summary IS NOT NULL`
    );

    const stale = this.db.get<{ count: number }>(
      `SELECT COUNT(*) as count FROM ${this.entityTable}
       WHERE summary IS NOT NULL AND summary_hash IS NULL`
    );

    return {
      totalEntities: total?.count || 0,
      entitiesWithSummary: withSummary?.count || 0,
      stalesSummaries: stale?.count || 0,
      coveragePercent: total?.count
        ? Math.round((withSummary?.count || 0) / total.count * 100)
        : 0
    };
  }
}

interface EntityForSummary {
  id: string;
  name: string;
  type: string;
  content: string;
  filePath?: string;
  contentHash?: string;
}

interface SummarizeResult {
  summarized: number;
  skipped: number;
  failed: number;
  provider: string | null;
  error?: string;
}

interface SummarizationStats {
  totalEntities: number;
  entitiesWithSummary: number;
  stalesSummaries: number;
  coveragePercent: number;
}
```

### 6. Integration with Indexer

```typescript
// src/indexer/indexer.ts (updated)

import { SummarizationManager } from '../summarization/manager';

export class Indexer {
  private summarizer: SummarizationManager;

  async indexFile(
    filePath: string,
    options?: { summarize?: boolean }
  ): Promise<IndexResult> {
    // Parse and extract entities
    const parseResult = await this.parser.parse(filePath);
    const entities = await this.storeEntities(parseResult);

    // Generate summaries if requested
    if (options?.summarize) {
      const entitiesForSummary = entities.map(e => ({
        id: e.id,
        name: e.name,
        type: e.type,
        content: e.content,
        filePath: e.filePath
      }));

      await this.summarizer.summarizeEntities(entitiesForSummary);
    }

    return { entities: entities.length };
  }
}
```

### 7. Database Schema Update

```sql
-- Add summary tracking columns
ALTER TABLE {prefix}_entities ADD COLUMN summary_hash TEXT;
ALTER TABLE {prefix}_entities ADD COLUMN summarized_at TEXT;

-- Index for finding stale summaries
CREATE INDEX idx_entities_summary_hash
ON {prefix}_entities (summary_hash);
```

## Configuration

```yaml
# .ctx-sys/config.yaml
summarization:
  enabled: true

  # Provider preference (first available wins)
  providers:
    - ollama    # Try local first
    - openai    # Fall back to cloud
    - anthropic

  ollama:
    base_url: http://localhost:11434
    model: qwen2.5-coder:7b  # Good for code understanding

  openai:
    # api_key: ${OPENAI_API_KEY}  # From environment
    model: gpt-4o-mini

  anthropic:
    # api_key: ${ANTHROPIC_API_KEY}
    model: claude-3-haiku-20240307

  # Processing settings
  batch_size: 10
  max_retries: 3
```

## CLI Commands

```bash
# Index with summarization
ctx-sys index . --summarize

# Force re-summarize all entities
ctx-sys index . --summarize --force-summarize

# Check summarization status
ctx-sys summary-status

# Summarize entities that are missing summaries
ctx-sys summarize --missing-only

# Use specific provider
ctx-sys summarize --provider openai
```

## MCP Tool Integration

```typescript
// Expose summarization controls via MCP

tools.push({
  name: 'summarize_entities',
  description: 'Generate LLM summaries for indexed entities',
  inputSchema: {
    type: 'object',
    properties: {
      project: { type: 'string' },
      force: { type: 'boolean', description: 'Re-summarize even if unchanged' },
      provider: { type: 'string', enum: ['ollama', 'openai', 'anthropic'] }
    }
  },
  handler: async (args) => {
    const result = await summarizer.summarizeEntities(entities, {
      force: args.force,
      provider: args.provider
    });
    return result;
  }
});

tools.push({
  name: 'get_summarization_status',
  description: 'Get summarization coverage statistics',
  inputSchema: {
    type: 'object',
    properties: {
      project: { type: 'string' }
    }
  },
  handler: async (args) => {
    return summarizer.getStats();
  }
});
```

## Testing

```typescript
describe('LLM Summarization', () => {
  describe('Provider Selection', () => {
    it('prefers Ollama when available', async () => {
      mockOllamaAvailable(true);
      const provider = await manager.getProvider();
      expect(provider?.id).toBe('ollama');
    });

    it('falls back to OpenAI when Ollama unavailable', async () => {
      mockOllamaAvailable(false);
      mockOpenAIAvailable(true);
      const provider = await manager.getProvider();
      expect(provider?.id).toBe('openai');
    });
  });

  describe('Incremental Summarization', () => {
    it('skips unchanged entities', async () => {
      await manager.summarizeEntities(entities);
      const result = await manager.summarizeEntities(entities);
      expect(result.skipped).toBe(entities.length);
      expect(result.summarized).toBe(0);
    });

    it('re-summarizes changed entities', async () => {
      await manager.summarizeEntities(entities);
      entities[0].content = 'modified content';
      const result = await manager.summarizeEntities(entities);
      expect(result.summarized).toBe(1);
    });
  });

  describe('Batch Processing', () => {
    it('processes in batches', async () => {
      const manyEntities = Array(25).fill(null).map((_, i) => ({
        id: `entity-${i}`,
        name: `func${i}`,
        type: 'function',
        content: `function func${i}() {}`
      }));

      const result = await manager.summarizeEntities(manyEntities);
      expect(result.summarized).toBe(25);
    });
  });
});
```

## Success Metrics

| Metric | Target |
|--------|--------|
| Ollama availability detection | <100ms |
| Summary generation per entity | <2s (Ollama), <1s (cloud) |
| Skip rate for unchanged entities | 100% |
| Fallback to cloud on Ollama failure | Automatic |
| Summary quality (human eval) | >80% rated helpful |

## Dependencies

- F10.1: Code Content Storage (need actual code to summarize)
- F10.4: Incremental Embedding (shares content hashing logic)

## Risks & Mitigations

| Risk | Mitigation |
|------|------------|
| LLM unavailable | Graceful fallback, continue without summaries |
| Rate limiting | Exponential backoff, respect rate limits |
| High API costs | Incremental updates, local Ollama preferred |
| Inconsistent quality | Specialized prompts per entity type, temperature tuning |
| Slow on large codebases | Batch processing, parallel requests |

## Next Steps

1. Implement provider interface and Ollama provider
2. Add OpenAI and Anthropic providers
3. Implement SummarizationManager with batching
4. Add content hash tracking for incremental updates
5. Integrate with indexer
6. Add CLI commands and MCP tools
