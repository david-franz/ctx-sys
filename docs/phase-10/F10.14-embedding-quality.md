# F10.14: Embedding Quality — Chunking + Longer Context

## Goal

Improve embedding quality by handling long entities properly instead of silently truncating them. Long code files and document sections should be split into overlapping chunks, each independently embedded, so no information is lost.

## Current Limitation

```typescript
// Ollama embedding truncation: silent data loss
const MODEL_MAX_CHARS: Record<string, number> = {
  'nomic-embed-text': 4000,    // ~80 lines of code
  'mxbai-embed-large': 2000,   // ~40 lines of code
  'all-minilm': 1000,          // ~20 lines of code
};

// A 300-line class gets truncated to the first 80 lines
// The embedding only represents the constructor and first few methods
// Methods at the bottom are invisible to semantic search
const truncated = text.length > maxChars ? text.slice(0, maxChars) : text;
```

This means:

- Long functions/classes are only partially embedded
- Document sections with lots of content lose their tail
- Search quality degrades for content that appears late in an entity

## Solution

### Overlapping Chunk Strategy

For entities that exceed the model's context limit, split into overlapping chunks:

```typescript
interface ChunkOptions {
  maxChars: number;       // Per-chunk size (model-dependent)
  overlapChars: number;   // Overlap between chunks (default: 200)
  minChunkChars: number;  // Don't create tiny trailing chunks (default: 100)
}

interface EntityChunk {
  entityId: string;
  chunkIndex: number;
  totalChunks: number;
  content: string;
  startOffset: number;
  endOffset: number;
}

function chunkEntity(content: string, options: ChunkOptions): EntityChunk[];
```

### Smart Splitting

Don't split mid-line or mid-word. Prefer splitting at:

1. Double newlines (paragraph/function boundaries)
2. Single newlines (line boundaries)
3. Sentence endings (`. `)
4. Word boundaries (spaces)

### Multi-Vector Storage

Each chunk gets its own embedding vector. On search, any chunk matching returns the full entity:

```typescript
// Storage: one entity → multiple vectors
// entity "MyClass" (500 lines) → 3 chunks → 3 embeddings

// Search: query matches chunk 2 → return full entity with chunk 2's score
// If multiple chunks match, use the highest score
```

### Embedding-Aware Summary

For chunked entities, the entity summary (from LLM summarization) serves as an additional "overview" embedding. This captures the high-level meaning while chunks capture details.

## Implementation Plan

### Step 1: Create chunking utility

- Create `src/embeddings/chunker.ts`
- Implement `chunkEntity()` with smart boundary detection
- Configurable chunk size, overlap, and minimum size
- Unit tests for edge cases (short content, exact boundary, etc.)

### Step 2: Update EmbeddingManager for multi-vector

- Update `embedEntity()` to chunk content if it exceeds model limit
- Store multiple vectors per entity (with chunk index metadata)
- Update vector schema: add `chunk_index` column to vectors table
- Migration for existing single-vector entries (chunk_index = 0)

### Step 3: Update similarity search

- `findSimilar()` returns entity-level results (not chunk-level)
- If multiple chunks of same entity match, use highest score
- Deduplicate by entity ID, keep best chunk score

### Step 4: Summary embedding

- If entity has an LLM summary, embed the summary as chunk_index = -1
- Summary embedding captures high-level meaning
- Combine summary score with best chunk score (weighted average)

## Verification

- A 300-line class gets 3-4 chunk embeddings
- Searching for a method defined on line 250 finds the class
- Short entities (under limit) still get single embedding (no regression)
- Total vector count increases but search quality improves measurably
