# F10.3: Scalable Indexing

## Goal

Enable indexing of large codebases (10k+ files, 100k+ symbols) without running out of memory or crashing. Currently, attempting to index and embed large projects causes OOM errors.

## Current Limitation

The current implementation has several bottlenecks:

1. **All-at-once embedding**: Loads all entities into memory, then tries to embed them all
2. **No streaming**: File parsing happens sequentially but results accumulate in memory
3. **No progress persistence**: If indexing fails mid-way, everything must restart
4. **Unbounded batch sizes**: No limits on how many items are processed at once

Example failure on large codebase:
```
Indexing codebase...
Processing 2847 files...
Generating embeddings for 74123 entities...
Killed  # OOM
```

## Solution Overview

### 1. Streaming Architecture

Process files in batches, persisting results incrementally:

```typescript
interface IndexingState {
  totalFiles: number;
  processedFiles: number;
  currentBatch: number;
  failedFiles: string[];
  lastProcessedPath: string;
  checkpointAt: Date;
}
```

### 2. Configurable Batch Sizes

```yaml
# .ctx-sys/config.yaml
indexing:
  file_batch_size: 100      # Files per batch
  entity_batch_size: 500    # Entities per embedding batch
  max_file_size_kb: 500     # Skip files larger than this
  max_entities_per_file: 100 # Limit symbols extracted per file
  checkpoint_interval: 50   # Save state every N files
```

### 3. Incremental Embedding

Separate indexing from embedding so each can fail/resume independently.

## Implementation

### 1. Streaming File Processor

```typescript
// src/indexer/streaming-processor.ts

export class StreamingFileProcessor {
  private state: IndexingState;
  private stateFile: string;

  constructor(
    private projectPath: string,
    private options: IndexingOptions
  ) {
    this.stateFile = path.join(projectPath, '.ctx-sys', 'indexing-state.json');
  }

  /**
   * Process files in batches with checkpointing.
   */
  async *processFiles(): AsyncGenerator<FileResult[], void, unknown> {
    const files = await this.discoverFiles();
    const batchSize = this.options.fileBatchSize || 100;

    // Resume from checkpoint if available
    const startIndex = await this.loadCheckpoint();

    for (let i = startIndex; i < files.length; i += batchSize) {
      const batch = files.slice(i, i + batchSize);
      const results: FileResult[] = [];

      for (const file of batch) {
        try {
          const result = await this.processFile(file);
          if (result) {
            results.push(result);
          }
        } catch (error) {
          this.state.failedFiles.push(file);
          console.error(`Failed to process ${file}: ${error}`);
        }
      }

      // Yield results for persistence
      yield results;

      // Checkpoint progress
      this.state.processedFiles = i + batch.length;
      this.state.lastProcessedPath = batch[batch.length - 1];
      await this.saveCheckpoint();

      // Allow GC between batches
      if (global.gc) global.gc();
    }
  }

  private async loadCheckpoint(): Promise<number> {
    try {
      const data = await fs.readFile(this.stateFile, 'utf-8');
      this.state = JSON.parse(data);
      return this.state.processedFiles;
    } catch {
      this.state = {
        totalFiles: 0,
        processedFiles: 0,
        currentBatch: 0,
        failedFiles: [],
        lastProcessedPath: '',
        checkpointAt: new Date()
      };
      return 0;
    }
  }

  private async saveCheckpoint(): Promise<void> {
    this.state.checkpointAt = new Date();
    await fs.writeFile(
      this.stateFile,
      JSON.stringify(this.state, null, 2)
    );
  }

  /**
   * Process a single file with size limits.
   */
  private async processFile(filePath: string): Promise<FileResult | null> {
    const stats = await fs.stat(filePath);

    // Skip large files
    if (stats.size > (this.options.maxFileSizeKb || 500) * 1024) {
      console.log(`Skipping large file: ${filePath} (${Math.round(stats.size / 1024)}KB)`);
      return null;
    }

    const content = await fs.readFile(filePath, 'utf-8');
    const parseResult = await this.parser.parse(filePath, content);

    // Limit symbols per file
    const maxSymbols = this.options.maxEntitiesPerFile || 100;
    if (parseResult.symbols.length > maxSymbols) {
      parseResult.symbols = parseResult.symbols.slice(0, maxSymbols);
      console.log(`Truncated ${filePath} to ${maxSymbols} symbols`);
    }

    return {
      filePath,
      parseResult,
      fileSize: stats.size
    };
  }
}
```

### 2. Batched Entity Storage

```typescript
// src/indexer/batch-store.ts

export class BatchEntityStore {
  private pendingEntities: CreateEntityInput[] = [];
  private batchSize: number;

  constructor(
    private entityStore: EntityStore,
    options?: { batchSize?: number }
  ) {
    this.batchSize = options?.batchSize || 500;
  }

  /**
   * Queue an entity for batch insertion.
   */
  async queue(entity: CreateEntityInput): Promise<void> {
    this.pendingEntities.push(entity);

    if (this.pendingEntities.length >= this.batchSize) {
      await this.flush();
    }
  }

  /**
   * Flush all pending entities to the database.
   */
  async flush(): Promise<number> {
    if (this.pendingEntities.length === 0) return 0;

    const count = this.pendingEntities.length;

    // Use bulk insert for efficiency
    await this.entityStore.createBatch(this.pendingEntities);

    this.pendingEntities = [];
    return count;
  }
}
```

### 3. Streaming Embedding Generator

```typescript
// src/embeddings/streaming-embedder.ts

export class StreamingEmbedder {
  private queue: EmbeddingTask[] = [];
  private processing = false;

  constructor(
    private embeddingManager: EmbeddingManager,
    private options: {
      batchSize?: number;
      maxConcurrent?: number;
      onProgress?: (completed: number, total: number) => void;
    } = {}
  ) {}

  /**
   * Queue entities for embedding without blocking.
   */
  async queueForEmbedding(entities: Entity[]): Promise<void> {
    for (const entity of entities) {
      this.queue.push({
        entityId: entity.id,
        content: this.buildEmbeddingContent(entity)
      });
    }

    // Start processing if not already running
    if (!this.processing) {
      this.processQueue();
    }
  }

  /**
   * Process the embedding queue in batches.
   */
  private async processQueue(): Promise<void> {
    this.processing = true;
    const batchSize = this.options.batchSize || 50;
    let completed = 0;
    const total = this.queue.length;

    while (this.queue.length > 0) {
      const batch = this.queue.splice(0, batchSize);

      try {
        await this.embeddingManager.embedBatch(
          batch.map(t => ({ id: t.entityId, content: t.content }))
        );
        completed += batch.length;
        this.options.onProgress?.(completed, total);
      } catch (error) {
        console.error(`Embedding batch failed: ${error}`);
        // Re-queue failed items at lower priority
        this.queue.push(...batch);
      }

      // Yield to event loop
      await new Promise(resolve => setImmediate(resolve));
    }

    this.processing = false;
  }

  private buildEmbeddingContent(entity: Entity): string {
    const parts: string[] = [`${entity.type}: ${entity.name}`];

    if (entity.summary) {
      parts.push(entity.summary);
    }

    if (entity.content) {
      // Limit content for embedding
      const preview = entity.content.split('\n').slice(0, 50).join('\n');
      parts.push(preview);
    }

    return parts.join('\n\n');
  }
}
```

### 4. Updated CLI with Progress

```typescript
// src/cli/index-cmd.ts

async function indexWithProgress(
  projectPath: string,
  options: IndexOptions
): Promise<void> {
  const processor = new StreamingFileProcessor(projectPath, options);
  const batchStore = new BatchEntityStore(entityStore, { batchSize: 500 });

  const progressBar = new ProgressBar(
    'Indexing [:bar] :current/:total files :percent :etas',
    { total: 0, width: 40 }
  );

  let totalEntities = 0;

  // Process files in streaming batches
  for await (const results of processor.processFiles()) {
    progressBar.total = processor.state.totalFiles;
    progressBar.tick(results.length);

    // Store entities from this batch
    for (const result of results) {
      const summary = await summarizer.summarizeFile(result.parseResult);

      for (const symbol of summary.symbols) {
        await batchStore.queue({
          type: symbol.type as EntityType,
          name: symbol.name,
          qualifiedName: symbol.qualifiedName,
          content: symbol.description,
          metadata: { filePath: result.filePath, ...symbol.metadata }
        });
        totalEntities++;
      }
    }

    // Flush periodically
    if (totalEntities % 1000 === 0) {
      await batchStore.flush();
    }
  }

  // Final flush
  await batchStore.flush();
  console.log(`Indexed ${totalEntities} entities from ${processor.state.processedFiles} files`);

  // Optional: Generate embeddings
  if (options.embed) {
    await embedWithProgress(entityStore, embeddingManager, options);
  }
}

async function embedWithProgress(
  entityStore: EntityStore,
  embeddingManager: EmbeddingManager,
  options: IndexOptions
): Promise<void> {
  // Get entities without embeddings
  const entities = await entityStore.getWithoutEmbeddings();

  const progressBar = new ProgressBar(
    'Embedding [:bar] :current/:total :percent :etas',
    { total: entities.length, width: 40 }
  );

  const embedder = new StreamingEmbedder(embeddingManager, {
    batchSize: options.embedBatchSize || 50,
    onProgress: (completed) => progressBar.update(completed / entities.length)
  });

  await embedder.queueForEmbedding(entities);
}
```

### 5. Memory-Efficient File Discovery

```typescript
// src/indexer/file-discovery.ts

export async function* discoverFilesStreaming(
  rootPath: string,
  options: DiscoveryOptions
): AsyncGenerator<string, void, unknown> {
  const ignorePatterns = options.ignore || [
    'node_modules', '.git', 'dist', 'build', '.next', 'coverage'
  ];

  async function* walkDir(dir: string): AsyncGenerator<string> {
    const entries = await fs.readdir(dir, { withFileTypes: true });

    for (const entry of entries) {
      const fullPath = path.join(dir, entry.name);

      // Skip ignored directories
      if (entry.isDirectory()) {
        if (ignorePatterns.some(p => minimatch(entry.name, p))) {
          continue;
        }
        yield* walkDir(fullPath);
      } else if (entry.isFile()) {
        // Check extension
        if (options.extensions?.length) {
          const ext = path.extname(entry.name);
          if (!options.extensions.includes(ext)) continue;
        }
        yield fullPath;
      }
    }
  }

  yield* walkDir(rootPath);
}
```

## CLI Commands

```bash
# Index with default settings (resumable)
ctx-sys index . --streaming

# Resume interrupted indexing
ctx-sys index . --resume

# Index with custom batch sizes
ctx-sys index . --file-batch 50 --entity-batch 200

# Skip embedding (just parse/store)
ctx-sys index . --no-embed

# Embed separately (after indexing)
ctx-sys embed --batch-size 100

# Check indexing status
ctx-sys index-status

# Clear incomplete indexing state
ctx-sys index --clear-state
```

## Configuration

```yaml
# .ctx-sys/config.yaml
indexing:
  # File processing
  file_batch_size: 100
  max_file_size_kb: 500
  checkpoint_interval: 50

  # Entity extraction
  max_entities_per_file: 100
  include_private: false

  # Embedding
  embed_batch_size: 50
  embed_on_index: true

  # Ignore patterns
  ignore:
    - node_modules
    - .git
    - dist
    - build
    - "*.min.js"
    - "*.bundle.js"
```

## Testing

```typescript
describe('Scalable Indexing', () => {
  it('indexes large codebase without OOM', async () => {
    // Create test codebase with 1000 files
    const testDir = await createLargeTestCodebase(1000);

    const processor = new StreamingFileProcessor(testDir, {
      fileBatchSize: 50
    });

    let fileCount = 0;
    for await (const batch of processor.processFiles()) {
      fileCount += batch.length;
    }

    expect(fileCount).toBe(1000);
    expect(process.memoryUsage().heapUsed).toBeLessThan(500 * 1024 * 1024);
  });

  it('resumes from checkpoint after interruption', async () => {
    const testDir = await createLargeTestCodebase(100);

    // Process first 50 files
    const processor1 = new StreamingFileProcessor(testDir, {
      fileBatchSize: 25
    });

    let processed = 0;
    for await (const batch of processor1.processFiles()) {
      processed += batch.length;
      if (processed >= 50) break; // Simulate interruption
    }

    // Resume from checkpoint
    const processor2 = new StreamingFileProcessor(testDir, {
      fileBatchSize: 25
    });

    let resumed = 0;
    for await (const batch of processor2.processFiles()) {
      resumed += batch.length;
    }

    expect(resumed).toBe(50); // Only remaining files
  });

  it('handles embedding failures gracefully', async () => {
    const entities = createTestEntities(100);
    const failingProvider = new FailingEmbeddingProvider(0.1); // 10% failure rate

    const embedder = new StreamingEmbedder(
      new EmbeddingManager(db, 'test', failingProvider),
      { batchSize: 10 }
    );

    await embedder.queueForEmbedding(entities);

    // All entities should eventually be embedded (with retries)
    const stats = await embeddingManager.getStats();
    expect(stats.count).toBe(100);
  });
});
```

## Success Metrics

| Metric | Target |
|--------|--------|
| Max memory usage | <1GB for 100k entity codebase |
| Recovery from crash | Resume within 1 batch of interruption |
| Indexing throughput | >100 files/second |
| Embedding throughput | >50 entities/second |

## Dependencies

- F10.1: Code Content Storage (for entity content)

## Risks & Mitigations

| Risk | Mitigation |
|------|------------|
| Checkpoint file corruption | JSON validation, backup before overwrite |
| Stale checkpoint from old run | Timestamp validation, --clear-state option |
| Database locks during batch insert | Use WAL mode, retry with backoff |

## Next Steps

1. Implement StreamingFileProcessor with checkpointing
2. Add batch insert to EntityStore
3. Update CLI with progress bars and --streaming flag
4. Add resume capability
5. Test with real large codebases (React, VS Code, etc.)
