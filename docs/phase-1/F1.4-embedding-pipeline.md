# F1.4 Embedding Pipeline

**Phase**: 1 - Foundation
**Priority**: Critical
**Dependencies**: F1.1 Database Schema, F1.3 Entity Storage

## Goal

Generate and store vector embeddings with provider abstraction, supporting both local (Ollama) and cloud (OpenAI) providers.

## Overview

Embeddings are the foundation of semantic search in ctx-sys. This module provides:
- Abstract provider interface for different embedding models
- Efficient batch processing
- Vector storage in sqlite-vec
- Similarity search operations
- Model-agnostic storage (embeddings tagged with model ID)

## Data Model

```typescript
interface EmbeddingProvider {
  readonly name: string;
  readonly modelId: string;
  readonly dimensions: number;

  embed(text: string): Promise<number[]>;
  embedBatch(texts: string[], options?: BatchOptions): Promise<number[][]>;
  isAvailable(): Promise<boolean>;
}

interface BatchOptions {
  batchSize?: number;          // Max items per API call
  concurrency?: number;        // Parallel API calls
  onProgress?: (completed: number, total: number) => void;
}

interface StoredEmbedding {
  id: string;
  entityId: string;
  modelId: string;
  embedding: number[];
  createdAt: Date;
}

interface SimilarityResult {
  entityId: string;
  score: number;                // Cosine similarity (0-1)
  distance: number;             // L2 distance
}
```

## Supported Models

| Provider | Model | Dimensions | Notes |
|----------|-------|------------|-------|
| Ollama | nomic-embed-text | 768 | Good general-purpose, fast |
| Ollama | mxbai-embed-large | 1024 | Higher quality |
| OpenAI | text-embedding-3-small | 1536 | Cloud fallback |
| OpenAI | text-embedding-3-large | 3072 | Highest quality |

## Implementation

### File: `src/embeddings/provider.ts`

```typescript
export interface EmbeddingProvider {
  readonly name: string;
  readonly modelId: string;
  readonly dimensions: number;

  embed(text: string): Promise<number[]>;
  embedBatch(texts: string[], options?: BatchOptions): Promise<number[][]>;
  isAvailable(): Promise<boolean>;
}

export interface BatchOptions {
  batchSize?: number;
  concurrency?: number;
  onProgress?: (completed: number, total: number) => void;
}
```

### File: `src/embeddings/ollama.ts`

```typescript
import { EmbeddingProvider, BatchOptions } from './provider';

interface OllamaConfig {
  baseUrl: string;
  model: string;
}

const MODEL_DIMENSIONS: Record<string, number> = {
  'nomic-embed-text': 768,
  'mxbai-embed-large': 1024,
  'all-minilm': 384
};

export class OllamaEmbeddingProvider implements EmbeddingProvider {
  readonly name = 'ollama';
  readonly modelId: string;
  readonly dimensions: number;

  constructor(private config: OllamaConfig) {
    this.modelId = `ollama:${config.model}`;
    this.dimensions = MODEL_DIMENSIONS[config.model] || 768;
  }

  async embed(text: string): Promise<number[]> {
    const response = await fetch(`${this.config.baseUrl}/api/embeddings`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: this.config.model,
        prompt: text
      })
    });

    if (!response.ok) {
      throw new Error(`Ollama embedding failed: ${response.statusText}`);
    }

    const data = await response.json();
    return data.embedding;
  }

  async embedBatch(texts: string[], options?: BatchOptions): Promise<number[][]> {
    const batchSize = options?.batchSize || 10;
    const concurrency = options?.concurrency || 3;
    const results: number[][] = [];
    let completed = 0;

    // Process in batches with concurrency limit
    for (let i = 0; i < texts.length; i += batchSize * concurrency) {
      const batchPromises: Promise<number[][]>[] = [];

      for (let j = 0; j < concurrency && i + j * batchSize < texts.length; j++) {
        const start = i + j * batchSize;
        const end = Math.min(start + batchSize, texts.length);
        const batch = texts.slice(start, end);

        batchPromises.push(
          Promise.all(batch.map(text => this.embed(text)))
        );
      }

      const batchResults = await Promise.all(batchPromises);
      for (const batch of batchResults) {
        results.push(...batch);
        completed += batch.length;
        options?.onProgress?.(completed, texts.length);
      }
    }

    return results;
  }

  async isAvailable(): Promise<boolean> {
    try {
      const response = await fetch(`${this.config.baseUrl}/api/tags`);
      if (!response.ok) return false;

      const data = await response.json();
      return data.models?.some((m: { name: string }) =>
        m.name.startsWith(this.config.model)
      );
    } catch {
      return false;
    }
  }
}
```

### File: `src/embeddings/openai.ts`

```typescript
import { EmbeddingProvider, BatchOptions } from './provider';

interface OpenAIConfig {
  apiKey: string;
  model: string;
}

const MODEL_DIMENSIONS: Record<string, number> = {
  'text-embedding-3-small': 1536,
  'text-embedding-3-large': 3072,
  'text-embedding-ada-002': 1536
};

export class OpenAIEmbeddingProvider implements EmbeddingProvider {
  readonly name = 'openai';
  readonly modelId: string;
  readonly dimensions: number;

  constructor(private config: OpenAIConfig) {
    this.modelId = `openai:${config.model}`;
    this.dimensions = MODEL_DIMENSIONS[config.model] || 1536;
  }

  async embed(text: string): Promise<number[]> {
    const response = await fetch('https://api.openai.com/v1/embeddings', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${this.config.apiKey}`
      },
      body: JSON.stringify({
        model: this.config.model,
        input: text
      })
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(`OpenAI embedding failed: ${error.error?.message}`);
    }

    const data = await response.json();
    return data.data[0].embedding;
  }

  async embedBatch(texts: string[], options?: BatchOptions): Promise<number[][]> {
    // OpenAI supports batch input natively (up to 2048 items)
    const batchSize = Math.min(options?.batchSize || 100, 2048);
    const results: number[][] = [];
    let completed = 0;

    for (let i = 0; i < texts.length; i += batchSize) {
      const batch = texts.slice(i, i + batchSize);

      const response = await fetch('https://api.openai.com/v1/embeddings', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.config.apiKey}`
        },
        body: JSON.stringify({
          model: this.config.model,
          input: batch
        })
      });

      if (!response.ok) {
        const error = await response.json();
        throw new Error(`OpenAI embedding failed: ${error.error?.message}`);
      }

      const data = await response.json();
      const embeddings = data.data
        .sort((a: { index: number }, b: { index: number }) => a.index - b.index)
        .map((d: { embedding: number[] }) => d.embedding);

      results.push(...embeddings);
      completed += embeddings.length;
      options?.onProgress?.(completed, texts.length);
    }

    return results;
  }

  async isAvailable(): Promise<boolean> {
    try {
      const response = await fetch('https://api.openai.com/v1/models', {
        headers: { 'Authorization': `Bearer ${this.config.apiKey}` }
      });
      return response.ok;
    } catch {
      return false;
    }
  }
}
```

### File: `src/embeddings/manager.ts`

```typescript
import { DatabaseConnection } from '../db/connection';
import { EmbeddingProvider } from './provider';
import { generateId } from '../utils/id';

export class EmbeddingManager {
  private vectorTable: string;

  constructor(
    private db: DatabaseConnection,
    private projectId: string,
    private provider: EmbeddingProvider
  ) {
    this.vectorTable = `${projectId}_vectors`;
    this.ensureModelRegistered();
  }

  private ensureModelRegistered(): void {
    const existing = this.db.get(
      `SELECT id FROM embedding_models WHERE id = ?`,
      [this.provider.modelId]
    );

    if (!existing) {
      this.db.run(
        `INSERT INTO embedding_models (id, name, provider, dimensions)
         VALUES (?, ?, ?, ?)`,
        [
          this.provider.modelId,
          this.provider.modelId,
          this.provider.name,
          this.provider.dimensions
        ]
      );
    }
  }

  async embed(entityId: string, content: string): Promise<void> {
    const embedding = await this.provider.embed(content);
    await this.store(entityId, embedding);
  }

  async embedBatch(
    entities: Array<{ id: string; content: string }>,
    options?: { onProgress?: (completed: number, total: number) => void }
  ): Promise<void> {
    const contents = entities.map(e => e.content);
    const embeddings = await this.provider.embedBatch(contents, {
      onProgress: options?.onProgress
    });

    // Store all embeddings in a transaction
    this.db.transaction(() => {
      for (let i = 0; i < entities.length; i++) {
        this.store(entities[i].id, embeddings[i]);
      }
    });
  }

  private store(entityId: string, embedding: number[]): void {
    const id = generateId();

    // Delete existing embedding for this entity/model
    this.db.run(
      `DELETE FROM ${this.vectorTable} WHERE entity_id = ? AND model_id = ?`,
      [entityId, this.provider.modelId]
    );

    // Insert new embedding
    // sqlite-vec expects embeddings as a JSON array
    this.db.run(
      `INSERT INTO ${this.vectorTable} (id, entity_id, model_id, embedding)
       VALUES (?, ?, ?, ?)`,
      [id, entityId, this.provider.modelId, JSON.stringify(embedding)]
    );
  }

  async findSimilar(
    query: string,
    options?: { limit?: number; threshold?: number; entityTypes?: string[] }
  ): Promise<SimilarityResult[]> {
    const queryEmbedding = await this.provider.embed(query);
    return this.findSimilarByVector(queryEmbedding, options);
  }

  findSimilarByVector(
    embedding: number[],
    options?: { limit?: number; threshold?: number; entityTypes?: string[] }
  ): SimilarityResult[] {
    const limit = options?.limit || 10;
    const threshold = options?.threshold || 0.0;

    // sqlite-vec similarity search
    // Using cosine distance (1 - cosine_similarity)
    let sql = `
      SELECT
        v.entity_id,
        vec_distance_cosine(v.embedding, ?) as distance
      FROM ${this.vectorTable} v
      WHERE v.model_id = ?
    `;
    const params: unknown[] = [JSON.stringify(embedding), this.provider.modelId];

    if (options?.entityTypes?.length) {
      const placeholders = options.entityTypes.map(() => '?').join(', ');
      sql += `
        AND v.entity_id IN (
          SELECT id FROM ${this.projectId}_entities WHERE type IN (${placeholders})
        )
      `;
      params.push(...options.entityTypes);
    }

    sql += `
      ORDER BY distance ASC
      LIMIT ?
    `;
    params.push(limit);

    const rows = this.db.all<{ entity_id: string; distance: number }>(sql, params);

    return rows
      .map(row => ({
        entityId: row.entity_id,
        distance: row.distance,
        score: 1 - row.distance  // Convert distance to similarity
      }))
      .filter(r => r.score >= threshold);
  }

  async deleteForEntity(entityId: string): Promise<void> {
    this.db.run(
      `DELETE FROM ${this.vectorTable} WHERE entity_id = ?`,
      [entityId]
    );
  }

  async hasEmbedding(entityId: string): Promise<boolean> {
    const row = this.db.get<{ count: number }>(
      `SELECT COUNT(*) as count FROM ${this.vectorTable}
       WHERE entity_id = ? AND model_id = ?`,
      [entityId, this.provider.modelId]
    );
    return (row?.count || 0) > 0;
  }

  async getStats(): Promise<{ count: number; modelId: string; dimensions: number }> {
    const row = this.db.get<{ count: number }>(
      `SELECT COUNT(*) as count FROM ${this.vectorTable} WHERE model_id = ?`,
      [this.provider.modelId]
    );
    return {
      count: row?.count || 0,
      modelId: this.provider.modelId,
      dimensions: this.provider.dimensions
    };
  }
}
```

### File: `src/embeddings/factory.ts`

```typescript
import { EmbeddingProvider } from './provider';
import { OllamaEmbeddingProvider } from './ollama';
import { OpenAIEmbeddingProvider } from './openai';

interface ProviderConfig {
  provider: 'ollama' | 'openai';
  model: string;
  baseUrl?: string;   // For Ollama
  apiKey?: string;    // For OpenAI
}

export class EmbeddingProviderFactory {
  static create(config: ProviderConfig): EmbeddingProvider {
    switch (config.provider) {
      case 'ollama':
        return new OllamaEmbeddingProvider({
          baseUrl: config.baseUrl || 'http://localhost:11434',
          model: config.model
        });
      case 'openai':
        if (!config.apiKey) {
          throw new Error('OpenAI API key required');
        }
        return new OpenAIEmbeddingProvider({
          apiKey: config.apiKey,
          model: config.model
        });
      default:
        throw new Error(`Unknown provider: ${config.provider}`);
    }
  }

  static async createWithFallback(
    primary: ProviderConfig,
    fallback: ProviderConfig
  ): Promise<EmbeddingProvider> {
    const primaryProvider = this.create(primary);
    if (await primaryProvider.isAvailable()) {
      return primaryProvider;
    }

    console.warn(`Primary embedding provider unavailable, using fallback`);
    return this.create(fallback);
  }
}
```

## Tasks

- [ ] Implement EmbeddingProvider interface
- [ ] Implement OllamaEmbeddingProvider
- [ ] Implement OpenAIEmbeddingProvider
- [ ] Implement EmbeddingManager
- [ ] Implement EmbeddingProviderFactory
- [ ] Add batching with progress callbacks
- [ ] Test sqlite-vec vector operations
- [ ] Add retry logic for API failures
- [ ] Write unit tests
- [ ] Write integration tests with real providers

## Testing

```typescript
describe('OllamaEmbeddingProvider', () => {
  it('should generate embedding for text');
  it('should batch embed multiple texts');
  it('should report availability correctly');
  it('should handle API errors gracefully');
});

describe('OpenAIEmbeddingProvider', () => {
  it('should generate embedding for text');
  it('should use native batch API');
  it('should handle rate limiting');
});

describe('EmbeddingManager', () => {
  it('should store embedding for entity');
  it('should find similar entities by query');
  it('should filter by entity type');
  it('should respect similarity threshold');
  it('should delete embeddings for entity');
});

describe('EmbeddingProviderFactory', () => {
  it('should create Ollama provider');
  it('should create OpenAI provider');
  it('should fallback when primary unavailable');
});
```

## Notes

- sqlite-vec uses L2 distance by default; we convert to cosine similarity for scoring
- Embeddings are stored with model ID to support model migrations
- Batch processing uses concurrency limits to avoid overwhelming local Ollama
- OpenAI has native batch support (up to 2048 items per request)
- Consider caching embeddings for frequently queried terms
